{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, we'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 79, 42, 47, 52, 62, 57, 67, 35, 45, 45, 45, 66, 42, 47, 47, 13,\n",
       "       67, 38, 42, 49, 80,  0, 80, 62, 26, 67, 42, 57, 62, 67, 42,  0,  0,\n",
       "       67, 42,  0, 80, 75, 62, 15, 67, 62, 46, 62, 57, 13, 67, 65, 74, 79,\n",
       "       42, 47, 47, 13, 67, 38, 42, 49, 80,  0, 13, 67, 80, 26, 67, 65, 74,\n",
       "       79, 42, 47, 47, 13, 67, 80, 74, 67, 80, 52, 26, 67, 10,  4, 74, 45,\n",
       "        4, 42, 13, 33, 45, 45, 78, 46, 62, 57, 13, 52, 79, 80, 74], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "Here is where we'll make our mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "We have our text encoded as integers as one long array in `encoded`. Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass `encoded` into this function and get our batch generator.\n",
    "\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the number of batches we can make from some array `arr`, you divide the length of `arr` by the batch size. Once you know the number of batches and the batch size, you can get the total number of characters to keep.\n",
    "\n",
    "After that, we need to split `arr` into $N$ sequences. You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (`n_seqs` below), let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$ where $K$ is the number of batches.\n",
    "\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the array. For each subsequent batch, the window moves over by `n_steps`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. You'll usually see the first input character used as the last target character, so something like this:\n",
    "```python\n",
    "y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "```\n",
    "where `x` is the input batch and `y` is the target batch.\n",
    "\n",
    "The way I like to do this window is use `range` to take steps of size `n_steps` from $0$ to `arr.shape[1]`, the total number of steps in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `n_steps` wide.\n",
    "\n",
    "> **Exercise:** Write the code for creating batches in the function below. The exercises in this notebook _will not be easy_. I've provided a notebook with solutions alongside this notebook. If you get stuck, checkout the solutions. The most important thing is that you don't copy and paste the code into here, **type out the solution code yourself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    characters_per_batch = n_steps * n_seqs\n",
    "    n_batches = len(arr) // characters_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 3 79 42 47 52 62 57 67 35 45]\n",
      " [67 42 49 67 74 10 52 67 43 10]\n",
      " [46 80 74 33 45 45  2 44 62 26]\n",
      " [74 67 25 65 57 80 74 43 67 79]\n",
      " [67 80 52 67 80 26 29 67 26 80]\n",
      " [67 12 52 67  4 42 26 45 10 74]\n",
      " [79 62 74 67 34 10 49 62 67 38]\n",
      " [15 67 18 65 52 67 74 10  4 67]\n",
      " [52 67 80 26 74 71 52 33 67 20]\n",
      " [67 26 42 80 25 67 52 10 67 79]]\n",
      "\n",
      "y\n",
      " [[79 42 47 52 62 57 67 35 45 45]\n",
      " [42 49 67 74 10 52 67 43 10 80]\n",
      " [80 74 33 45 45  2 44 62 26 29]\n",
      " [67 25 65 57 80 74 43 67 79 80]\n",
      " [80 52 67 80 26 29 67 26 80 57]\n",
      " [12 52 67  4 42 26 45 10 74  0]\n",
      " [62 74 67 34 10 49 62 67 38 10]\n",
      " [67 18 65 52 67 74 10  4 67 26]\n",
      " [67 80 26 74 71 52 33 67 20 79]\n",
      " [26 42 80 25 67 52 10 67 79 62]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented `get_batches` correctly, the above output should look something like \n",
    "```\n",
    "x\n",
    " [[55 63 69 22  6 76 45  5 16 35]\n",
    " [ 5 69  1  5 12 52  6  5 56 52]\n",
    " [48 29 12 61 35 35  8 64 76 78]\n",
    " [12  5 24 39 45 29 12 56  5 63]\n",
    " [ 5 29  6  5 29 78 28  5 78 29]\n",
    " [ 5 13  6  5 36 69 78 35 52 12]\n",
    " [63 76 12  5 18 52  1 76  5 58]\n",
    " [34  5 73 39  6  5 12 52 36  5]\n",
    " [ 6  5 29 78 12 79  6 61  5 59]\n",
    " [ 5 78 69 29 24  5  6 52  5 63]]\n",
    "\n",
    "y\n",
    " [[63 69 22  6 76 45  5 16 35 35]\n",
    " [69  1  5 12 52  6  5 56 52 29]\n",
    " [29 12 61 35 35  8 64 76 78 28]\n",
    " [ 5 24 39 45 29 12 56  5 63 29]\n",
    " [29  6  5 29 78 28  5 78 29 45]\n",
    " [13  6  5 36 69 78 35 52 12 43]\n",
    " [76 12  5 18 52  1 76  5 58 52]\n",
    " [ 5 73 39  6  5 12 52 36  5 78]\n",
    " [ 5 29 78 12 79  6 61  5 59 63]\n",
    " [78 69 29 24  5  6 52  5 63 76]]\n",
    " ```\n",
    " although the exact numbers will be different. Check to make sure the data is shifted over one step for `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is where you'll build the network. We'll break it up into parts so it's easier to reason about each bit. Then we can connect them up into the whole network.\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called `keep_prob`. This will be a scalar, that is a 0-D tensor. To make a scalar, you create a placeholder without giving it a size.\n",
    "\n",
    "> **Exercise:** Create the input placeholders in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell\n",
    "\n",
    "Here we will create the LSTM cell we'll use in the hidden layer. We'll use this cell as a building block for the RNN. So we aren't actually defining the RNN here, just the type of cell we'll use in the hidden layer.\n",
    "\n",
    "We first create a basic LSTM cell with\n",
    "\n",
    "```python\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "where `num_units` is the number of units in the hidden layers in the cell. Then we can add dropout by wrapping it with \n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "```\n",
    "You pass in a cell and it will automatically add dropout to the inputs or outputs. Finally, we can stack up the LSTM cells into layers with [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell). With this, you pass in a list of cells and it will send the output of one cell into the next cell. Previously with TensorFlow 1.0, you could do this\n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "```\n",
    "\n",
    "This might look a little weird if you know Python well because this will create a list of the same `cell` object. However, TensorFlow 1.0 will create different weight matrices for all `cell` objects. But, starting with TensorFlow 1.1 you actually need to create new cell objects in the list. To get it to work in TensorFlow 1.1, it should look like\n",
    "\n",
    "```python\n",
    "def build_cell(num_units, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    return drop\n",
    "    \n",
    "tf.contrib.rnn.MultiRNNCell([build_cell(num_units, keep_prob) for _ in range(num_layers)])\n",
    "```\n",
    "\n",
    "Even though this is actually multiple LSTM cells stacked on each other, you can treat the multiple layers as one cell.\n",
    "\n",
    "We also need to create an initial cell state of all zeros. This can be done like so\n",
    "\n",
    "```python\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "```\n",
    "\n",
    "Below, we implement the `build_lstm` function to create these LSTM cells and the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size, reuse=tf.get_variable_scope().reuse)\n",
    "\n",
    "        # Add dropout to the cell outputs\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Output\n",
    "\n",
    "Here we'll create the output layer. We need to connect the output of the RNN cells to a full connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character, so we want this layer to have size $C$, the number of classes/characters we have in our text.\n",
    "\n",
    "If our input has batch size $N$, number of steps $M$, and the hidden layer has $L$ hidden units, then the output is a 3D tensor with size $N \\times M \\times L$. The output of each LSTM cell has size $L$, we have $M$ of them, one for each sequence step, and we have $N$ sequences. So the total size is $N \\times M \\times L$. \n",
    "\n",
    "We are using the same fully connected layer, the same weights, for each of the outputs. Then, to make things easier, we should reshape the outputs into a 2D tensor with shape $(M * N) \\times L$. That is, one row for each sequence and step, where the values of each row are the output from the LSTM cells. We get the LSTM output as a list, `lstm_output`. First we need to concatenate this whole list into one array with [`tf.concat`](https://www.tensorflow.org/api_docs/python/tf/concat). Then, reshape it (with `tf.reshape`) to size $(M * N) \\times L$.\n",
    "\n",
    "One we have the outputs reshaped, we can do the matrix multiplication with the weights. We need to wrap the weight and bias variables in a variable scope with `tf.variable_scope(scope_name)` because there are weights being created in the LSTM cells. TensorFlow will throw an error if the weights created here have the same names as the weights created in the LSTM cells, which they will be default. To avoid this, we wrap the variables in a variable scope so we can give them unique names.\n",
    "\n",
    "> **Exercise:** Implement the output layer in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w)  + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss\n",
    "\n",
    "Next up is the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. First we need to one-hot encode the targets, we're getting them as encoded characters. Then, reshape the one-hot targets so it's a 2D tensor with size $(M*N) \\times C$ where $C$ is the number of classes/characters we have. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with $C$ units. So our logits will also have size $(M*N) \\times C$.\n",
    "\n",
    "Then we run the logits and targets through `tf.nn.softmax_cross_entropy_with_logits` and find the mean to get the loss.\n",
    "\n",
    ">**Exercise:** Implement the loss calculation in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_sum(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Here we build the optimizer. Normal RNNs have have issues gradients exploding and disappearing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n",
    "\n",
    "Now we can put all the pieces together and build a class for the network. To actually run data through the LSTM cells, we will use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn). This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as `final_state` so we can pass it to the first LSTM cell in the the next mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state we get from `build_lstm`, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN. \n",
    "\n",
    "> **Exercise:** Use the functions you've implemented previously and `tf.nn.dynamic_rnn` to build the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state) \n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss =  build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here are the hyperparameters for the network.\n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to copy it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for training\n",
    "\n",
    "This is typical training code, passing inputs and targets into the network, then running the optimizer. Here we also get back the final LSTM state for the mini-batch. Then, we pass that state back into the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}.ckpt`\n",
    "\n",
    "> **Exercise:** Set the hyperparameters above to train the network. Watch the training loss, it should be consistently dropping. Also, I highly advise running this on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i3800_l512.ckpt\n",
      "Epoch: 1/20...  Training Step: 1...  Training loss: 63686.7617...  7.5540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 55972.2383...  6.9525 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 50085.1055...  5.9451 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 45041.9141...  6.2856 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 41910.2344...  6.3952 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 39633.0547...  5.7793 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 37484.5938...  6.2382 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 36355.4414...  6.4187 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 34911.4766...  5.6866 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 34285.9727...  6.3157 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 33830.7734...  6.7229 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 33428.7344...  6.3264 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 32901.6328...  8.7444 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 32962.4219...  6.1848 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 32433.9297...  6.3115 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 32173.2773...  7.5961 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 31806.6055...  6.4378 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 31888.9297...  7.5879 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 31512.7773...  6.2835 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 30710.2363...  6.3109 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 30740.8867...  7.5322 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 30389.4766...  6.2817 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 30020.9102...  7.4309 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 29739.3125...  6.3423 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 29406.1465...  6.3112 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 29265.4297...  7.5546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 29052.2930...  6.3399 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 28588.5938...  7.5878 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 28483.4180...  6.3120 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 28229.3359...  6.3950 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 28353.8867...  7.5827 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 27672.8574...  6.2358 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 27319.8965...  7.3840 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 27324.1875...  6.4613 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 26869.2891...  6.4802 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 27042.4863...  8.6831 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 26596.6074...  6.4085 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 26227.5820...  7.4045 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 25989.3145...  6.3636 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 25887.5078...  6.3358 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 25656.5586...  7.4336 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 25568.8652...  6.4811 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 25391.6250...  7.8164 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 25388.7168...  6.2782 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 25090.5762...  6.3129 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 24679.5664...  7.5265 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 25004.1484...  6.1744 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 24786.1953...  7.5254 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 24480.7305...  6.3049 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 24865.2031...  6.2363 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 24221.7520...  7.5211 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 24454.5391...  6.4681 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 23992.9004...  7.5695 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 23886.0273...  6.3126 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 23873.8984...  6.3156 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 23814.1016...  7.5660 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 23699.7578...  6.3677 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 23485.9062...  7.5650 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 23389.9707...  6.2874 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 23525.5898...  6.2593 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 23240.5547...  7.7325 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 23386.1094...  6.3222 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 23334.3809...  7.5966 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 23061.1621...  6.2696 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 22831.8594...  6.2874 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 23282.0664...  7.6174 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 23030.8203...  6.2872 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 22360.3438...  9.1970 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 22463.9219...  6.9526 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 22659.3184...  6.7366 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 22669.5742...  7.4941 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 22713.3945...  6.9232 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 22447.3926...  7.5188 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 22355.6699...  6.3590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 22290.1094...  6.9351 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 22735.2988...  6.8514 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 22057.1367...  6.3897 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 22185.6816...  7.5911 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 21843.2246...  6.3495 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 21914.8945...  6.9924 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 21727.7520...  6.8510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 22172.6016...  6.3068 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 21684.2695...  7.6547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 21707.3711...  6.2047 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 21191.7910...  6.6376 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 21491.6289...  7.1314 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 21615.6504...  6.4545 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 21368.9023...  7.4141 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 21218.0234...  7.1382 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 21497.5371...  8.1305 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 21153.5430...  6.9315 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 21362.4121...  6.3805 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 21143.1719...  7.5335 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 21037.1133...  6.8866 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 21010.4160...  7.6395 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 21126.8906...  6.4145 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 21123.6230...  7.4697 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 20855.8438...  8.7196 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 20967.9062...  7.2100 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 20749.8379...  8.1981 sec/batch\n",
      "Epoch: 1/20...  Training Step: 101...  Training loss: 21063.8105...  7.4834 sec/batch\n",
      "Epoch: 1/20...  Training Step: 102...  Training loss: 20727.1758...  10.1935 sec/batch\n",
      "Epoch: 1/20...  Training Step: 103...  Training loss: 20680.0234...  8.7642 sec/batch\n",
      "Epoch: 1/20...  Training Step: 104...  Training loss: 20681.3730...  13.8100 sec/batch\n",
      "Epoch: 1/20...  Training Step: 105...  Training loss: 20645.8711...  8.9980 sec/batch\n",
      "Epoch: 1/20...  Training Step: 106...  Training loss: 20815.5293...  7.8509 sec/batch\n",
      "Epoch: 1/20...  Training Step: 107...  Training loss: 20521.1855...  6.3622 sec/batch\n",
      "Epoch: 1/20...  Training Step: 108...  Training loss: 20674.0645...  7.5940 sec/batch\n",
      "Epoch: 1/20...  Training Step: 109...  Training loss: 20686.1797...  6.4033 sec/batch\n",
      "Epoch: 1/20...  Training Step: 110...  Training loss: 20470.0723...  7.3321 sec/batch\n",
      "Epoch: 1/20...  Training Step: 111...  Training loss: 20603.0781...  6.3841 sec/batch\n",
      "Epoch: 1/20...  Training Step: 112...  Training loss: 20280.7344...  6.3147 sec/batch\n",
      "Epoch: 1/20...  Training Step: 113...  Training loss: 20353.3730...  8.2864 sec/batch\n",
      "Epoch: 1/20...  Training Step: 114...  Training loss: 20223.4531...  7.5232 sec/batch\n",
      "Epoch: 1/20...  Training Step: 115...  Training loss: 20219.6289...  7.9375 sec/batch\n",
      "Epoch: 1/20...  Training Step: 116...  Training loss: 19921.4727...  7.2584 sec/batch\n",
      "Epoch: 1/20...  Training Step: 117...  Training loss: 20249.1719...  8.8417 sec/batch\n",
      "Epoch: 1/20...  Training Step: 118...  Training loss: 20193.0527...  7.5327 sec/batch\n",
      "Epoch: 1/20...  Training Step: 119...  Training loss: 20251.8125...  6.4871 sec/batch\n",
      "Epoch: 1/20...  Training Step: 120...  Training loss: 20237.1367...  8.2909 sec/batch\n",
      "Epoch: 1/20...  Training Step: 121...  Training loss: 20408.8398...  7.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 122...  Training loss: 19883.3203...  11.4417 sec/batch\n",
      "Epoch: 1/20...  Training Step: 123...  Training loss: 20078.2695...  6.9216 sec/batch\n",
      "Epoch: 1/20...  Training Step: 124...  Training loss: 20294.0020...  9.2275 sec/batch\n",
      "Epoch: 1/20...  Training Step: 125...  Training loss: 20018.7168...  6.3131 sec/batch\n",
      "Epoch: 1/20...  Training Step: 126...  Training loss: 19600.7891...  8.2330 sec/batch\n",
      "Epoch: 1/20...  Training Step: 127...  Training loss: 20121.3281...  9.2801 sec/batch\n",
      "Epoch: 1/20...  Training Step: 128...  Training loss: 20057.3652...  6.9915 sec/batch\n",
      "Epoch: 1/20...  Training Step: 129...  Training loss: 19801.5586...  7.7488 sec/batch\n",
      "Epoch: 1/20...  Training Step: 130...  Training loss: 19932.1230...  6.3450 sec/batch\n",
      "Epoch: 1/20...  Training Step: 131...  Training loss: 19707.0371...  7.6067 sec/batch\n",
      "Epoch: 1/20...  Training Step: 132...  Training loss: 19597.9473...  6.3676 sec/batch\n",
      "Epoch: 1/20...  Training Step: 133...  Training loss: 19954.5449...  6.3928 sec/batch\n",
      "Epoch: 1/20...  Training Step: 134...  Training loss: 19922.7598...  7.6735 sec/batch\n",
      "Epoch: 1/20...  Training Step: 135...  Training loss: 19660.8750...  6.3341 sec/batch\n",
      "Epoch: 1/20...  Training Step: 136...  Training loss: 19833.0078...  7.5437 sec/batch\n",
      "Epoch: 1/20...  Training Step: 137...  Training loss: 19766.6719...  6.2633 sec/batch\n",
      "Epoch: 1/20...  Training Step: 138...  Training loss: 19556.9570...  6.4726 sec/batch\n",
      "Epoch: 1/20...  Training Step: 139...  Training loss: 19962.6211...  7.5495 sec/batch\n",
      "Epoch: 1/20...  Training Step: 140...  Training loss: 19695.2363...  6.3573 sec/batch\n",
      "Epoch: 1/20...  Training Step: 141...  Training loss: 19960.0938...  7.5659 sec/batch\n",
      "Epoch: 1/20...  Training Step: 142...  Training loss: 19550.9766...  6.3115 sec/batch\n",
      "Epoch: 1/20...  Training Step: 143...  Training loss: 19620.6680...  6.4142 sec/batch\n",
      "Epoch: 1/20...  Training Step: 144...  Training loss: 19474.3164...  7.6740 sec/batch\n",
      "Epoch: 1/20...  Training Step: 145...  Training loss: 19300.1465...  6.3420 sec/batch\n",
      "Epoch: 1/20...  Training Step: 146...  Training loss: 19659.5195...  7.6246 sec/batch\n",
      "Epoch: 1/20...  Training Step: 147...  Training loss: 19495.1133...  6.4114 sec/batch\n",
      "Epoch: 1/20...  Training Step: 148...  Training loss: 19777.8906...  6.3028 sec/batch\n",
      "Epoch: 1/20...  Training Step: 149...  Training loss: 19406.7656...  7.7247 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 19242.3984...  6.3277 sec/batch\n",
      "Epoch: 1/20...  Training Step: 151...  Training loss: 19270.4961...  7.6975 sec/batch\n",
      "Epoch: 1/20...  Training Step: 152...  Training loss: 19899.2578...  6.2523 sec/batch\n",
      "Epoch: 1/20...  Training Step: 153...  Training loss: 19452.0312...  6.3155 sec/batch\n",
      "Epoch: 1/20...  Training Step: 154...  Training loss: 19398.2812...  7.6069 sec/batch\n",
      "Epoch: 1/20...  Training Step: 155...  Training loss: 19271.1211...  6.8052 sec/batch\n",
      "Epoch: 1/20...  Training Step: 156...  Training loss: 19232.5156...  8.7782 sec/batch\n",
      "Epoch: 1/20...  Training Step: 157...  Training loss: 19364.7109...  7.4124 sec/batch\n",
      "Epoch: 1/20...  Training Step: 158...  Training loss: 19258.7910...  7.7196 sec/batch\n",
      "Epoch: 1/20...  Training Step: 159...  Training loss: 18783.9688...  6.2799 sec/batch\n",
      "Epoch: 1/20...  Training Step: 160...  Training loss: 19355.5781...  6.2869 sec/batch\n",
      "Epoch: 1/20...  Training Step: 161...  Training loss: 19472.6797...  7.4868 sec/batch\n",
      "Epoch: 1/20...  Training Step: 162...  Training loss: 19012.0137...  7.3110 sec/batch\n",
      "Epoch: 1/20...  Training Step: 163...  Training loss: 19126.3848...  9.4802 sec/batch\n",
      "Epoch: 1/20...  Training Step: 164...  Training loss: 19146.3086...  6.4601 sec/batch\n",
      "Epoch: 1/20...  Training Step: 165...  Training loss: 19151.8203...  7.0265 sec/batch\n",
      "Epoch: 1/20...  Training Step: 166...  Training loss: 19003.8809...  6.9980 sec/batch\n",
      "Epoch: 1/20...  Training Step: 167...  Training loss: 19104.0176...  6.4750 sec/batch\n",
      "Epoch: 1/20...  Training Step: 168...  Training loss: 19428.1133...  7.8464 sec/batch\n",
      "Epoch: 1/20...  Training Step: 169...  Training loss: 18952.7070...  6.3191 sec/batch\n",
      "Epoch: 1/20...  Training Step: 170...  Training loss: 18889.5469...  7.3285 sec/batch\n",
      "Epoch: 1/20...  Training Step: 171...  Training loss: 18939.1250...  6.5834 sec/batch\n",
      "Epoch: 1/20...  Training Step: 172...  Training loss: 19177.6953...  6.5285 sec/batch\n",
      "Epoch: 1/20...  Training Step: 173...  Training loss: 19397.7578...  7.7211 sec/batch\n",
      "Epoch: 1/20...  Training Step: 174...  Training loss: 19382.8340...  7.8154 sec/batch\n",
      "Epoch: 1/20...  Training Step: 175...  Training loss: 19321.0859...  7.7319 sec/batch\n",
      "Epoch: 1/20...  Training Step: 176...  Training loss: 18878.4141...  9.6819 sec/batch\n",
      "Epoch: 1/20...  Training Step: 177...  Training loss: 18702.1543...  9.0828 sec/batch\n",
      "Epoch: 1/20...  Training Step: 178...  Training loss: 18990.2090...  6.3999 sec/batch\n",
      "Epoch: 1/20...  Training Step: 179...  Training loss: 18549.8711...  7.4060 sec/batch\n",
      "Epoch: 1/20...  Training Step: 180...  Training loss: 18490.8613...  6.8821 sec/batch\n",
      "Epoch: 1/20...  Training Step: 181...  Training loss: 18660.4648...  6.4374 sec/batch\n",
      "Epoch: 1/20...  Training Step: 182...  Training loss: 18819.0645...  8.0221 sec/batch\n",
      "Epoch: 1/20...  Training Step: 183...  Training loss: 18722.7344...  6.3715 sec/batch\n",
      "Epoch: 1/20...  Training Step: 184...  Training loss: 18863.3477...  7.7982 sec/batch\n",
      "Epoch: 1/20...  Training Step: 185...  Training loss: 18900.2363...  6.5062 sec/batch\n",
      "Epoch: 1/20...  Training Step: 186...  Training loss: 18507.3086...  6.4785 sec/batch\n",
      "Epoch: 1/20...  Training Step: 187...  Training loss: 18708.3652...  7.4717 sec/batch\n",
      "Epoch: 1/20...  Training Step: 188...  Training loss: 18397.1914...  6.3485 sec/batch\n",
      "Epoch: 1/20...  Training Step: 189...  Training loss: 18630.7656...  8.0516 sec/batch\n",
      "Epoch: 1/20...  Training Step: 190...  Training loss: 18654.3613...  6.2292 sec/batch\n",
      "Epoch: 1/20...  Training Step: 191...  Training loss: 18489.6191...  6.2883 sec/batch\n",
      "Epoch: 1/20...  Training Step: 192...  Training loss: 18141.5020...  7.5394 sec/batch\n",
      "Epoch: 1/20...  Training Step: 193...  Training loss: 18587.7656...  6.4005 sec/batch\n",
      "Epoch: 1/20...  Training Step: 194...  Training loss: 18378.3574...  7.8907 sec/batch\n",
      "Epoch: 1/20...  Training Step: 195...  Training loss: 18271.1523...  8.1829 sec/batch\n",
      "Epoch: 1/20...  Training Step: 196...  Training loss: 18505.0664...  9.4969 sec/batch\n",
      "Epoch: 1/20...  Training Step: 197...  Training loss: 18397.6523...  8.4107 sec/batch\n",
      "Epoch: 1/20...  Training Step: 198...  Training loss: 18233.7070...  7.2508 sec/batch\n",
      "Epoch: 2/20...  Training Step: 199...  Training loss: 19492.2246...  6.6962 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 18362.1094...  6.2488 sec/batch\n",
      "Epoch: 2/20...  Training Step: 201...  Training loss: 18221.6680...  8.0882 sec/batch\n",
      "Epoch: 2/20...  Training Step: 202...  Training loss: 18338.9727...  6.2559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 203...  Training loss: 18387.4824...  7.6986 sec/batch\n",
      "Epoch: 2/20...  Training Step: 204...  Training loss: 17971.3047...  6.2383 sec/batch\n",
      "Epoch: 2/20...  Training Step: 205...  Training loss: 18284.8164...  6.3139 sec/batch\n",
      "Epoch: 2/20...  Training Step: 206...  Training loss: 18251.3711...  7.8000 sec/batch\n",
      "Epoch: 2/20...  Training Step: 207...  Training loss: 18539.7715...  7.3291 sec/batch\n",
      "Epoch: 2/20...  Training Step: 208...  Training loss: 18231.6719...  7.9487 sec/batch\n",
      "Epoch: 2/20...  Training Step: 209...  Training loss: 17908.4922...  6.7406 sec/batch\n",
      "Epoch: 2/20...  Training Step: 210...  Training loss: 18053.2695...  7.4585 sec/batch\n",
      "Epoch: 2/20...  Training Step: 211...  Training loss: 18084.5664...  7.3060 sec/batch\n",
      "Epoch: 2/20...  Training Step: 212...  Training loss: 18676.4141...  6.4894 sec/batch\n",
      "Epoch: 2/20...  Training Step: 213...  Training loss: 18149.0391...  8.1563 sec/batch\n",
      "Epoch: 2/20...  Training Step: 214...  Training loss: 18062.2383...  6.9518 sec/batch\n",
      "Epoch: 2/20...  Training Step: 215...  Training loss: 18125.1738...  7.7101 sec/batch\n",
      "Epoch: 2/20...  Training Step: 216...  Training loss: 18464.5859...  6.8636 sec/batch\n",
      "Epoch: 2/20...  Training Step: 217...  Training loss: 18130.5312...  8.8628 sec/batch\n",
      "Epoch: 2/20...  Training Step: 218...  Training loss: 18263.1914...  8.1929 sec/batch\n",
      "Epoch: 2/20...  Training Step: 219...  Training loss: 17864.9062...  8.0191 sec/batch\n",
      "Epoch: 2/20...  Training Step: 220...  Training loss: 18306.1914...  8.3249 sec/batch\n",
      "Epoch: 2/20...  Training Step: 221...  Training loss: 18030.8457...  10.0414 sec/batch\n",
      "Epoch: 2/20...  Training Step: 222...  Training loss: 17940.5020...  8.0605 sec/batch\n",
      "Epoch: 2/20...  Training Step: 223...  Training loss: 17975.4453...  7.3406 sec/batch\n",
      "Epoch: 2/20...  Training Step: 224...  Training loss: 17863.3164...  7.5056 sec/batch\n",
      "Epoch: 2/20...  Training Step: 225...  Training loss: 17794.5391...  6.2797 sec/batch\n",
      "Epoch: 2/20...  Training Step: 226...  Training loss: 18126.5215...  7.4948 sec/batch\n",
      "Epoch: 2/20...  Training Step: 227...  Training loss: 18223.5391...  6.1765 sec/batch\n",
      "Epoch: 2/20...  Training Step: 228...  Training loss: 18133.6875...  6.1544 sec/batch\n",
      "Epoch: 2/20...  Training Step: 229...  Training loss: 17921.1367...  7.5334 sec/batch\n",
      "Epoch: 2/20...  Training Step: 230...  Training loss: 17730.4141...  6.4245 sec/batch\n",
      "Epoch: 2/20...  Training Step: 231...  Training loss: 18015.0508...  7.6416 sec/batch\n",
      "Epoch: 2/20...  Training Step: 232...  Training loss: 18151.0664...  6.2063 sec/batch\n",
      "Epoch: 2/20...  Training Step: 233...  Training loss: 17730.3965...  6.2679 sec/batch\n",
      "Epoch: 2/20...  Training Step: 234...  Training loss: 17967.9434...  7.5161 sec/batch\n",
      "Epoch: 2/20...  Training Step: 235...  Training loss: 17738.5156...  6.2444 sec/batch\n",
      "Epoch: 2/20...  Training Step: 236...  Training loss: 17530.2051...  7.4668 sec/batch\n",
      "Epoch: 2/20...  Training Step: 237...  Training loss: 17500.7402...  7.1037 sec/batch\n",
      "Epoch: 2/20...  Training Step: 238...  Training loss: 17551.4062...  8.2149 sec/batch\n",
      "Epoch: 2/20...  Training Step: 239...  Training loss: 17604.3438...  7.7418 sec/batch\n",
      "Epoch: 2/20...  Training Step: 240...  Training loss: 17826.7695...  7.5536 sec/batch\n",
      "Epoch: 2/20...  Training Step: 241...  Training loss: 17591.5195...  8.8934 sec/batch\n",
      "Epoch: 2/20...  Training Step: 242...  Training loss: 17520.1406...  7.4733 sec/batch\n",
      "Epoch: 2/20...  Training Step: 243...  Training loss: 17671.5898...  7.7241 sec/batch\n",
      "Epoch: 2/20...  Training Step: 244...  Training loss: 17307.0508...  6.3730 sec/batch\n",
      "Epoch: 2/20...  Training Step: 245...  Training loss: 17643.6914...  7.7463 sec/batch\n",
      "Epoch: 2/20...  Training Step: 246...  Training loss: 17561.3203...  6.3193 sec/batch\n",
      "Epoch: 2/20...  Training Step: 247...  Training loss: 17448.2500...  6.3225 sec/batch\n",
      "Epoch: 2/20...  Training Step: 248...  Training loss: 18006.7812...  7.6229 sec/batch\n",
      "Epoch: 2/20...  Training Step: 249...  Training loss: 17427.8555...  7.7991 sec/batch\n",
      "Epoch: 2/20...  Training Step: 250...  Training loss: 18109.5156...  7.8178 sec/batch\n",
      "Epoch: 2/20...  Training Step: 251...  Training loss: 17599.0957...  6.4757 sec/batch\n",
      "Epoch: 2/20...  Training Step: 252...  Training loss: 17655.3867...  9.0984 sec/batch\n",
      "Epoch: 2/20...  Training Step: 253...  Training loss: 17511.5762...  8.6389 sec/batch\n",
      "Epoch: 2/20...  Training Step: 254...  Training loss: 17624.9355...  10.1314 sec/batch\n",
      "Epoch: 2/20...  Training Step: 255...  Training loss: 17813.6641...  7.0159 sec/batch\n",
      "Epoch: 2/20...  Training Step: 256...  Training loss: 17357.0117...  7.6108 sec/batch\n",
      "Epoch: 2/20...  Training Step: 257...  Training loss: 17376.1367...  9.4407 sec/batch\n",
      "Epoch: 2/20...  Training Step: 258...  Training loss: 17859.6758...  6.4008 sec/batch\n",
      "Epoch: 2/20...  Training Step: 259...  Training loss: 17620.8555...  7.5134 sec/batch\n",
      "Epoch: 2/20...  Training Step: 260...  Training loss: 18039.7910...  6.2584 sec/batch\n",
      "Epoch: 2/20...  Training Step: 261...  Training loss: 17824.7676...  7.6964 sec/batch\n",
      "Epoch: 2/20...  Training Step: 262...  Training loss: 17720.1895...  6.1535 sec/batch\n",
      "Epoch: 2/20...  Training Step: 263...  Training loss: 17423.3203...  6.3356 sec/batch\n",
      "Epoch: 2/20...  Training Step: 264...  Training loss: 17876.0742...  7.4602 sec/batch\n",
      "Epoch: 2/20...  Training Step: 265...  Training loss: 17694.8516...  6.3551 sec/batch\n",
      "Epoch: 2/20...  Training Step: 266...  Training loss: 17311.7441...  9.6718 sec/batch\n",
      "Epoch: 2/20...  Training Step: 267...  Training loss: 17396.8242...  8.5403 sec/batch\n",
      "Epoch: 2/20...  Training Step: 268...  Training loss: 17428.6055...  7.5544 sec/batch\n",
      "Epoch: 2/20...  Training Step: 269...  Training loss: 17922.3125...  6.4318 sec/batch\n",
      "Epoch: 2/20...  Training Step: 270...  Training loss: 17591.6133...  6.1925 sec/batch\n",
      "Epoch: 2/20...  Training Step: 271...  Training loss: 17742.0820...  7.3571 sec/batch\n",
      "Epoch: 2/20...  Training Step: 272...  Training loss: 17294.4961...  7.0003 sec/batch\n",
      "Epoch: 2/20...  Training Step: 273...  Training loss: 17406.0859...  11.9707 sec/batch\n",
      "Epoch: 2/20...  Training Step: 274...  Training loss: 17613.5586...  7.3621 sec/batch\n",
      "Epoch: 2/20...  Training Step: 275...  Training loss: 17270.9980...  8.3957 sec/batch\n",
      "Epoch: 2/20...  Training Step: 276...  Training loss: 17365.1641...  6.1343 sec/batch\n",
      "Epoch: 2/20...  Training Step: 277...  Training loss: 17058.2930...  6.3295 sec/batch\n",
      "Epoch: 2/20...  Training Step: 278...  Training loss: 17328.6367...  7.6258 sec/batch\n",
      "Epoch: 2/20...  Training Step: 279...  Training loss: 16899.4902...  6.2109 sec/batch\n",
      "Epoch: 2/20...  Training Step: 280...  Training loss: 17526.0527...  7.4553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 281...  Training loss: 17113.2715...  6.1378 sec/batch\n",
      "Epoch: 2/20...  Training Step: 282...  Training loss: 17341.2969...  6.4035 sec/batch\n",
      "Epoch: 2/20...  Training Step: 283...  Training loss: 16971.6055...  8.1108 sec/batch\n",
      "Epoch: 2/20...  Training Step: 284...  Training loss: 17031.7539...  8.0782 sec/batch\n",
      "Epoch: 2/20...  Training Step: 285...  Training loss: 17120.1523...  7.3884 sec/batch\n",
      "Epoch: 2/20...  Training Step: 286...  Training loss: 17016.7148...  6.4983 sec/batch\n",
      "Epoch: 2/20...  Training Step: 287...  Training loss: 16866.5430...  7.1614 sec/batch\n",
      "Epoch: 2/20...  Training Step: 288...  Training loss: 17343.4453...  7.1393 sec/batch\n",
      "Epoch: 2/20...  Training Step: 289...  Training loss: 17039.6797...  6.2033 sec/batch\n",
      "Epoch: 2/20...  Training Step: 290...  Training loss: 17152.7676...  7.4686 sec/batch\n",
      "Epoch: 2/20...  Training Step: 291...  Training loss: 16965.6602...  6.1684 sec/batch\n",
      "Epoch: 2/20...  Training Step: 292...  Training loss: 16904.3262...  6.1887 sec/batch\n",
      "Epoch: 2/20...  Training Step: 293...  Training loss: 16997.4375...  7.3901 sec/batch\n",
      "Epoch: 2/20...  Training Step: 294...  Training loss: 17224.5410...  6.1126 sec/batch\n",
      "Epoch: 2/20...  Training Step: 295...  Training loss: 17099.8125...  7.3941 sec/batch\n",
      "Epoch: 2/20...  Training Step: 296...  Training loss: 16844.6523...  6.2058 sec/batch\n",
      "Epoch: 2/20...  Training Step: 297...  Training loss: 16992.0742...  6.3483 sec/batch\n",
      "Epoch: 2/20...  Training Step: 298...  Training loss: 16664.5859...  7.5855 sec/batch\n",
      "Epoch: 2/20...  Training Step: 299...  Training loss: 17139.2930...  6.3216 sec/batch\n",
      "Epoch: 2/20...  Training Step: 300...  Training loss: 16868.5137...  7.5271 sec/batch\n",
      "Epoch: 2/20...  Training Step: 301...  Training loss: 16909.9688...  6.9276 sec/batch\n",
      "Epoch: 2/20...  Training Step: 302...  Training loss: 16932.8242...  9.2216 sec/batch\n",
      "Epoch: 2/20...  Training Step: 303...  Training loss: 16870.7305...  7.4636 sec/batch\n",
      "Epoch: 2/20...  Training Step: 304...  Training loss: 17005.6484...  8.7733 sec/batch\n",
      "Epoch: 2/20...  Training Step: 305...  Training loss: 17039.2031...  8.1336 sec/batch\n",
      "Epoch: 2/20...  Training Step: 306...  Training loss: 16994.7500...  6.3532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 307...  Training loss: 17145.2344...  7.5827 sec/batch\n",
      "Epoch: 2/20...  Training Step: 308...  Training loss: 17110.2930...  6.6159 sec/batch\n",
      "Epoch: 2/20...  Training Step: 309...  Training loss: 16821.2402...  8.1507 sec/batch\n",
      "Epoch: 2/20...  Training Step: 310...  Training loss: 16898.9844...  6.4261 sec/batch\n",
      "Epoch: 2/20...  Training Step: 311...  Training loss: 16927.9883...  6.0212 sec/batch\n",
      "Epoch: 2/20...  Training Step: 312...  Training loss: 16887.3965...  6.7590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 313...  Training loss: 16656.3105...  5.7113 sec/batch\n",
      "Epoch: 2/20...  Training Step: 314...  Training loss: 16647.8516...  5.6898 sec/batch\n",
      "Epoch: 2/20...  Training Step: 315...  Training loss: 16991.9453...  6.8117 sec/batch\n",
      "Epoch: 2/20...  Training Step: 316...  Training loss: 16882.9629...  5.7084 sec/batch\n",
      "Epoch: 2/20...  Training Step: 317...  Training loss: 16929.9121...  6.7505 sec/batch\n",
      "Epoch: 2/20...  Training Step: 318...  Training loss: 16835.0664...  5.9195 sec/batch\n",
      "Epoch: 2/20...  Training Step: 319...  Training loss: 17090.3184...  5.7220 sec/batch\n",
      "Epoch: 2/20...  Training Step: 320...  Training loss: 16576.5449...  6.8881 sec/batch\n",
      "Epoch: 2/20...  Training Step: 321...  Training loss: 16513.5117...  5.8176 sec/batch\n",
      "Epoch: 2/20...  Training Step: 322...  Training loss: 17040.7695...  5.7441 sec/batch\n",
      "Epoch: 2/20...  Training Step: 323...  Training loss: 16816.7441...  6.8429 sec/batch\n",
      "Epoch: 2/20...  Training Step: 324...  Training loss: 16520.5000...  5.7451 sec/batch\n",
      "Epoch: 2/20...  Training Step: 325...  Training loss: 16964.5039...  7.2351 sec/batch\n",
      "Epoch: 2/20...  Training Step: 326...  Training loss: 17054.7461...  6.8157 sec/batch\n",
      "Epoch: 2/20...  Training Step: 327...  Training loss: 16686.1270...  6.3298 sec/batch\n",
      "Epoch: 2/20...  Training Step: 328...  Training loss: 16736.9004...  7.5979 sec/batch\n",
      "Epoch: 2/20...  Training Step: 329...  Training loss: 16580.3828...  6.3556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 330...  Training loss: 16625.8906...  6.8627 sec/batch\n",
      "Epoch: 2/20...  Training Step: 331...  Training loss: 17102.6719...  7.0079 sec/batch\n",
      "Epoch: 2/20...  Training Step: 332...  Training loss: 16942.1230...  7.1475 sec/batch\n",
      "Epoch: 2/20...  Training Step: 333...  Training loss: 16849.9414...  7.8403 sec/batch\n",
      "Epoch: 2/20...  Training Step: 334...  Training loss: 16788.7207...  6.3323 sec/batch\n",
      "Epoch: 2/20...  Training Step: 335...  Training loss: 16889.9004...  8.1984 sec/batch\n",
      "Epoch: 2/20...  Training Step: 336...  Training loss: 16797.5078...  7.7888 sec/batch\n",
      "Epoch: 2/20...  Training Step: 337...  Training loss: 16967.4043...  6.7041 sec/batch\n",
      "Epoch: 2/20...  Training Step: 338...  Training loss: 16742.9766...  7.4050 sec/batch\n",
      "Epoch: 2/20...  Training Step: 339...  Training loss: 17212.8828...  6.3356 sec/batch\n",
      "Epoch: 2/20...  Training Step: 340...  Training loss: 16826.9023...  7.7407 sec/batch\n",
      "Epoch: 2/20...  Training Step: 341...  Training loss: 16798.5371...  6.3178 sec/batch\n",
      "Epoch: 2/20...  Training Step: 342...  Training loss: 16826.9062...  6.6167 sec/batch\n",
      "Epoch: 2/20...  Training Step: 343...  Training loss: 16513.4883...  7.4646 sec/batch\n",
      "Epoch: 2/20...  Training Step: 344...  Training loss: 16869.6445...  6.6863 sec/batch\n",
      "Epoch: 2/20...  Training Step: 345...  Training loss: 16757.7305...  7.6773 sec/batch\n",
      "Epoch: 2/20...  Training Step: 346...  Training loss: 16962.3633...  6.4038 sec/batch\n",
      "Epoch: 2/20...  Training Step: 347...  Training loss: 16748.4434...  7.9071 sec/batch\n",
      "Epoch: 2/20...  Training Step: 348...  Training loss: 16683.1152...  6.4190 sec/batch\n",
      "Epoch: 2/20...  Training Step: 349...  Training loss: 16371.4121...  6.3578 sec/batch\n",
      "Epoch: 2/20...  Training Step: 350...  Training loss: 16744.0430...  7.5814 sec/batch\n",
      "Epoch: 2/20...  Training Step: 351...  Training loss: 16865.9570...  6.3772 sec/batch\n",
      "Epoch: 2/20...  Training Step: 352...  Training loss: 16705.5625...  7.5629 sec/batch\n",
      "Epoch: 2/20...  Training Step: 353...  Training loss: 16594.7266...  6.5189 sec/batch\n",
      "Epoch: 2/20...  Training Step: 354...  Training loss: 16674.5957...  6.3321 sec/batch\n",
      "Epoch: 2/20...  Training Step: 355...  Training loss: 16803.6992...  7.6656 sec/batch\n",
      "Epoch: 2/20...  Training Step: 356...  Training loss: 16644.1250...  6.2215 sec/batch\n",
      "Epoch: 2/20...  Training Step: 357...  Training loss: 16354.5850...  7.2927 sec/batch\n",
      "Epoch: 2/20...  Training Step: 358...  Training loss: 16770.9082...  6.5654 sec/batch\n",
      "Epoch: 2/20...  Training Step: 359...  Training loss: 16958.2930...  6.8972 sec/batch\n",
      "Epoch: 2/20...  Training Step: 360...  Training loss: 16659.4727...  8.6914 sec/batch\n",
      "Epoch: 2/20...  Training Step: 361...  Training loss: 16589.8066...  6.9660 sec/batch\n",
      "Epoch: 2/20...  Training Step: 362...  Training loss: 16637.6035...  7.5229 sec/batch\n",
      "Epoch: 2/20...  Training Step: 363...  Training loss: 16541.9590...  6.6524 sec/batch\n",
      "Epoch: 2/20...  Training Step: 364...  Training loss: 16565.3984...  8.4511 sec/batch\n",
      "Epoch: 2/20...  Training Step: 365...  Training loss: 16730.3066...  7.0622 sec/batch\n",
      "Epoch: 2/20...  Training Step: 366...  Training loss: 17187.0547...  6.3210 sec/batch\n",
      "Epoch: 2/20...  Training Step: 367...  Training loss: 16494.7129...  7.7947 sec/batch\n",
      "Epoch: 2/20...  Training Step: 368...  Training loss: 16472.4238...  7.6415 sec/batch\n",
      "Epoch: 2/20...  Training Step: 369...  Training loss: 16502.1055...  8.4293 sec/batch\n",
      "Epoch: 2/20...  Training Step: 370...  Training loss: 16511.3086...  6.4081 sec/batch\n",
      "Epoch: 2/20...  Training Step: 371...  Training loss: 16915.6621...  6.6677 sec/batch\n",
      "Epoch: 2/20...  Training Step: 372...  Training loss: 16812.2051...  8.2912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 373...  Training loss: 16815.7930...  7.4269 sec/batch\n",
      "Epoch: 2/20...  Training Step: 374...  Training loss: 16493.2070...  7.5986 sec/batch\n",
      "Epoch: 2/20...  Training Step: 375...  Training loss: 16423.0586...  6.1714 sec/batch\n",
      "Epoch: 2/20...  Training Step: 376...  Training loss: 16668.5859...  8.7091 sec/batch\n",
      "Epoch: 2/20...  Training Step: 377...  Training loss: 16204.9375...  7.3440 sec/batch\n",
      "Epoch: 2/20...  Training Step: 378...  Training loss: 16215.9385...  6.9007 sec/batch\n",
      "Epoch: 2/20...  Training Step: 379...  Training loss: 16231.4521...  7.0793 sec/batch\n",
      "Epoch: 2/20...  Training Step: 380...  Training loss: 16451.3867...  7.6070 sec/batch\n",
      "Epoch: 2/20...  Training Step: 381...  Training loss: 16438.0879...  8.4473 sec/batch\n",
      "Epoch: 2/20...  Training Step: 382...  Training loss: 16573.8652...  7.3157 sec/batch\n",
      "Epoch: 2/20...  Training Step: 383...  Training loss: 16570.5859...  7.8721 sec/batch\n",
      "Epoch: 2/20...  Training Step: 384...  Training loss: 16246.0312...  6.6341 sec/batch\n",
      "Epoch: 2/20...  Training Step: 385...  Training loss: 16600.1562...  7.6077 sec/batch\n",
      "Epoch: 2/20...  Training Step: 386...  Training loss: 16273.0371...  6.7331 sec/batch\n",
      "Epoch: 2/20...  Training Step: 387...  Training loss: 16315.4756...  6.2830 sec/batch\n",
      "Epoch: 2/20...  Training Step: 388...  Training loss: 16464.8574...  7.7800 sec/batch\n",
      "Epoch: 2/20...  Training Step: 389...  Training loss: 16266.3887...  6.2365 sec/batch\n",
      "Epoch: 2/20...  Training Step: 390...  Training loss: 16161.4648...  7.2729 sec/batch\n",
      "Epoch: 2/20...  Training Step: 391...  Training loss: 16384.0918...  6.4805 sec/batch\n",
      "Epoch: 2/20...  Training Step: 392...  Training loss: 16181.6309...  6.3366 sec/batch\n",
      "Epoch: 2/20...  Training Step: 393...  Training loss: 16182.7510...  7.5639 sec/batch\n",
      "Epoch: 2/20...  Training Step: 394...  Training loss: 16359.9775...  6.8047 sec/batch\n",
      "Epoch: 2/20...  Training Step: 395...  Training loss: 16321.1611...  9.5498 sec/batch\n",
      "Epoch: 2/20...  Training Step: 396...  Training loss: 16251.3838...  6.3810 sec/batch\n",
      "Epoch: 3/20...  Training Step: 397...  Training loss: 17200.8184...  6.8189 sec/batch\n",
      "Epoch: 3/20...  Training Step: 398...  Training loss: 16443.8281...  7.2110 sec/batch\n",
      "Epoch: 3/20...  Training Step: 399...  Training loss: 16178.5293...  6.4099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 400...  Training loss: 16374.6992...  7.6475 sec/batch\n",
      "Epoch: 3/20...  Training Step: 401...  Training loss: 16171.3428...  6.3789 sec/batch\n",
      "Epoch: 3/20...  Training Step: 402...  Training loss: 15971.6982...  7.4062 sec/batch\n",
      "Epoch: 3/20...  Training Step: 403...  Training loss: 16341.0488...  6.6789 sec/batch\n",
      "Epoch: 3/20...  Training Step: 404...  Training loss: 16243.6328...  6.2890 sec/batch\n",
      "Epoch: 3/20...  Training Step: 405...  Training loss: 16494.3281...  9.1452 sec/batch\n",
      "Epoch: 3/20...  Training Step: 406...  Training loss: 16244.5938...  6.4298 sec/batch\n",
      "Epoch: 3/20...  Training Step: 407...  Training loss: 16070.2402...  7.3836 sec/batch\n",
      "Epoch: 3/20...  Training Step: 408...  Training loss: 16094.4971...  6.3067 sec/batch\n",
      "Epoch: 3/20...  Training Step: 409...  Training loss: 16137.5068...  6.2160 sec/batch\n",
      "Epoch: 3/20...  Training Step: 410...  Training loss: 16504.0586...  7.4273 sec/batch\n",
      "Epoch: 3/20...  Training Step: 411...  Training loss: 16140.4473...  6.2234 sec/batch\n",
      "Epoch: 3/20...  Training Step: 412...  Training loss: 16058.4395...  8.1142 sec/batch\n",
      "Epoch: 3/20...  Training Step: 413...  Training loss: 16281.0840...  7.3152 sec/batch\n",
      "Epoch: 3/20...  Training Step: 414...  Training loss: 16504.8711...  6.7092 sec/batch\n",
      "Epoch: 3/20...  Training Step: 415...  Training loss: 16354.6797...  6.3480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 416...  Training loss: 16447.5039...  5.7956 sec/batch\n",
      "Epoch: 3/20...  Training Step: 417...  Training loss: 16059.3057...  7.1416 sec/batch\n",
      "Epoch: 3/20...  Training Step: 418...  Training loss: 16378.1357...  6.0717 sec/batch\n",
      "Epoch: 3/20...  Training Step: 419...  Training loss: 16008.3438...  6.4021 sec/batch\n",
      "Epoch: 3/20...  Training Step: 420...  Training loss: 16319.4756...  6.8530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 421...  Training loss: 16201.8037...  5.7742 sec/batch\n",
      "Epoch: 3/20...  Training Step: 422...  Training loss: 15727.1572...  6.0680 sec/batch\n",
      "Epoch: 3/20...  Training Step: 423...  Training loss: 16068.4072...  6.6919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 424...  Training loss: 16441.1484...  8.7047 sec/batch\n",
      "Epoch: 3/20...  Training Step: 425...  Training loss: 16333.7158...  9.8583 sec/batch\n",
      "Epoch: 3/20...  Training Step: 426...  Training loss: 16400.2031...  6.0634 sec/batch\n",
      "Epoch: 3/20...  Training Step: 427...  Training loss: 16168.1758...  7.5126 sec/batch\n",
      "Epoch: 3/20...  Training Step: 428...  Training loss: 15957.8672...  6.6931 sec/batch\n",
      "Epoch: 3/20...  Training Step: 429...  Training loss: 16220.4023...  6.4799 sec/batch\n",
      "Epoch: 3/20...  Training Step: 430...  Training loss: 16307.2002...  7.6176 sec/batch\n",
      "Epoch: 3/20...  Training Step: 431...  Training loss: 15972.7637...  6.4435 sec/batch\n",
      "Epoch: 3/20...  Training Step: 432...  Training loss: 16216.2129...  7.6276 sec/batch\n",
      "Epoch: 3/20...  Training Step: 433...  Training loss: 15918.9023...  6.4155 sec/batch\n",
      "Epoch: 3/20...  Training Step: 434...  Training loss: 15738.7598...  6.4522 sec/batch\n",
      "Epoch: 3/20...  Training Step: 435...  Training loss: 15595.3545...  7.6886 sec/batch\n",
      "Epoch: 3/20...  Training Step: 436...  Training loss: 15858.2695...  6.3254 sec/batch\n",
      "Epoch: 3/20...  Training Step: 437...  Training loss: 15858.5078...  7.5185 sec/batch\n",
      "Epoch: 3/20...  Training Step: 438...  Training loss: 16199.8066...  6.3699 sec/batch\n",
      "Epoch: 3/20...  Training Step: 439...  Training loss: 15789.3691...  6.3808 sec/batch\n",
      "Epoch: 3/20...  Training Step: 440...  Training loss: 15863.0645...  7.7667 sec/batch\n",
      "Epoch: 3/20...  Training Step: 441...  Training loss: 16025.6084...  6.2850 sec/batch\n",
      "Epoch: 3/20...  Training Step: 442...  Training loss: 15619.3574...  7.6409 sec/batch\n",
      "Epoch: 3/20...  Training Step: 443...  Training loss: 15930.6885...  6.3777 sec/batch\n",
      "Epoch: 3/20...  Training Step: 444...  Training loss: 15795.6943...  6.4969 sec/batch\n",
      "Epoch: 3/20...  Training Step: 445...  Training loss: 15863.3340...  7.5746 sec/batch\n",
      "Epoch: 3/20...  Training Step: 446...  Training loss: 16312.4531...  6.4352 sec/batch\n",
      "Epoch: 3/20...  Training Step: 447...  Training loss: 15717.0684...  7.6937 sec/batch\n",
      "Epoch: 3/20...  Training Step: 448...  Training loss: 16410.0898...  6.4068 sec/batch\n",
      "Epoch: 3/20...  Training Step: 449...  Training loss: 16117.1094...  6.3608 sec/batch\n",
      "Epoch: 3/20...  Training Step: 450...  Training loss: 16127.2275...  7.5427 sec/batch\n",
      "Epoch: 3/20...  Training Step: 451...  Training loss: 15904.9854...  6.3122 sec/batch\n",
      "Epoch: 3/20...  Training Step: 452...  Training loss: 15964.5830...  7.5528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 453...  Training loss: 16190.1475...  6.3637 sec/batch\n",
      "Epoch: 3/20...  Training Step: 454...  Training loss: 15845.1719...  6.4487 sec/batch\n",
      "Epoch: 3/20...  Training Step: 455...  Training loss: 15850.3740...  7.6049 sec/batch\n",
      "Epoch: 3/20...  Training Step: 456...  Training loss: 16271.7920...  6.3139 sec/batch\n",
      "Epoch: 3/20...  Training Step: 457...  Training loss: 16034.9668...  7.7344 sec/batch\n",
      "Epoch: 3/20...  Training Step: 458...  Training loss: 16561.1289...  6.5697 sec/batch\n",
      "Epoch: 3/20...  Training Step: 459...  Training loss: 16320.3477...  6.3785 sec/batch\n",
      "Epoch: 3/20...  Training Step: 460...  Training loss: 16082.1045...  7.5563 sec/batch\n",
      "Epoch: 3/20...  Training Step: 461...  Training loss: 15807.2188...  6.3430 sec/batch\n",
      "Epoch: 3/20...  Training Step: 462...  Training loss: 16068.4795...  7.5778 sec/batch\n",
      "Epoch: 3/20...  Training Step: 463...  Training loss: 16081.8994...  6.3300 sec/batch\n",
      "Epoch: 3/20...  Training Step: 464...  Training loss: 15789.0781...  6.4106 sec/batch\n",
      "Epoch: 3/20...  Training Step: 465...  Training loss: 15856.8555...  7.6178 sec/batch\n",
      "Epoch: 3/20...  Training Step: 466...  Training loss: 15822.9082...  6.3179 sec/batch\n",
      "Epoch: 3/20...  Training Step: 467...  Training loss: 16293.9375...  7.4812 sec/batch\n",
      "Epoch: 3/20...  Training Step: 468...  Training loss: 16020.6777...  6.3493 sec/batch\n",
      "Epoch: 3/20...  Training Step: 469...  Training loss: 16239.1484...  6.3143 sec/batch\n",
      "Epoch: 3/20...  Training Step: 470...  Training loss: 15857.1689...  8.0822 sec/batch\n",
      "Epoch: 3/20...  Training Step: 471...  Training loss: 15787.2988...  6.3158 sec/batch\n",
      "Epoch: 3/20...  Training Step: 472...  Training loss: 16216.8125...  7.5702 sec/batch\n",
      "Epoch: 3/20...  Training Step: 473...  Training loss: 15810.1650...  6.3499 sec/batch\n",
      "Epoch: 3/20...  Training Step: 474...  Training loss: 15893.4746...  6.3317 sec/batch\n",
      "Epoch: 3/20...  Training Step: 475...  Training loss: 15526.3398...  8.0385 sec/batch\n",
      "Epoch: 3/20...  Training Step: 476...  Training loss: 15891.7734...  6.2382 sec/batch\n",
      "Epoch: 3/20...  Training Step: 477...  Training loss: 15430.7734...  7.5298 sec/batch\n",
      "Epoch: 3/20...  Training Step: 478...  Training loss: 15932.0850...  6.7735 sec/batch\n",
      "Epoch: 3/20...  Training Step: 479...  Training loss: 15605.8252...  6.8245 sec/batch\n",
      "Epoch: 3/20...  Training Step: 480...  Training loss: 15871.6152...  7.1875 sec/batch\n",
      "Epoch: 3/20...  Training Step: 481...  Training loss: 15555.6934...  6.3801 sec/batch\n",
      "Epoch: 3/20...  Training Step: 482...  Training loss: 15711.9785...  7.4660 sec/batch\n",
      "Epoch: 3/20...  Training Step: 483...  Training loss: 15625.3213...  6.2974 sec/batch\n",
      "Epoch: 3/20...  Training Step: 484...  Training loss: 15555.2832...  6.3410 sec/batch\n",
      "Epoch: 3/20...  Training Step: 485...  Training loss: 15416.0859...  7.4312 sec/batch\n",
      "Epoch: 3/20...  Training Step: 486...  Training loss: 15985.0596...  6.2285 sec/batch\n",
      "Epoch: 3/20...  Training Step: 487...  Training loss: 15614.2012...  7.5689 sec/batch\n",
      "Epoch: 3/20...  Training Step: 488...  Training loss: 15599.5576...  6.5224 sec/batch\n",
      "Epoch: 3/20...  Training Step: 489...  Training loss: 15580.0420...  6.1790 sec/batch\n",
      "Epoch: 3/20...  Training Step: 490...  Training loss: 15591.4395...  7.5305 sec/batch\n",
      "Epoch: 3/20...  Training Step: 491...  Training loss: 15605.9971...  6.2050 sec/batch\n",
      "Epoch: 3/20...  Training Step: 492...  Training loss: 15995.1162...  7.6544 sec/batch\n",
      "Epoch: 3/20...  Training Step: 493...  Training loss: 15785.8945...  6.2761 sec/batch\n",
      "Epoch: 3/20...  Training Step: 494...  Training loss: 15573.8457...  6.2520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 495...  Training loss: 15615.9121...  7.5550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 496...  Training loss: 15357.2363...  6.3710 sec/batch\n",
      "Epoch: 3/20...  Training Step: 497...  Training loss: 15854.2646...  7.4767 sec/batch\n",
      "Epoch: 3/20...  Training Step: 498...  Training loss: 15517.2148...  6.3585 sec/batch\n",
      "Epoch: 3/20...  Training Step: 499...  Training loss: 15599.6377...  6.4160 sec/batch\n",
      "Epoch: 3/20...  Training Step: 500...  Training loss: 15656.6201...  7.6524 sec/batch\n",
      "Epoch: 3/20...  Training Step: 501...  Training loss: 15494.7988...  6.4978 sec/batch\n",
      "Epoch: 3/20...  Training Step: 502...  Training loss: 15623.3740...  7.5044 sec/batch\n",
      "Epoch: 3/20...  Training Step: 503...  Training loss: 15715.3564...  6.3243 sec/batch\n",
      "Epoch: 3/20...  Training Step: 504...  Training loss: 15620.8262...  6.2406 sec/batch\n",
      "Epoch: 3/20...  Training Step: 505...  Training loss: 15636.5176...  7.6249 sec/batch\n",
      "Epoch: 3/20...  Training Step: 506...  Training loss: 15902.3848...  6.2191 sec/batch\n",
      "Epoch: 3/20...  Training Step: 507...  Training loss: 15432.5469...  7.5137 sec/batch\n",
      "Epoch: 3/20...  Training Step: 508...  Training loss: 15668.9668...  6.2880 sec/batch\n",
      "Epoch: 3/20...  Training Step: 509...  Training loss: 15625.8057...  6.2765 sec/batch\n",
      "Epoch: 3/20...  Training Step: 510...  Training loss: 15602.9980...  7.5028 sec/batch\n",
      "Epoch: 3/20...  Training Step: 511...  Training loss: 15483.7988...  6.5335 sec/batch\n",
      "Epoch: 3/20...  Training Step: 512...  Training loss: 15273.0156...  8.2121 sec/batch\n",
      "Epoch: 3/20...  Training Step: 513...  Training loss: 15764.0059...  7.0624 sec/batch\n",
      "Epoch: 3/20...  Training Step: 514...  Training loss: 15606.6641...  7.2015 sec/batch\n",
      "Epoch: 3/20...  Training Step: 515...  Training loss: 15600.9102...  8.6537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 516...  Training loss: 15502.3320...  6.7099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 517...  Training loss: 15732.5547...  7.9469 sec/batch\n",
      "Epoch: 3/20...  Training Step: 518...  Training loss: 15286.7852...  6.5215 sec/batch\n",
      "Epoch: 3/20...  Training Step: 519...  Training loss: 15177.3047...  8.4229 sec/batch\n",
      "Epoch: 3/20...  Training Step: 520...  Training loss: 15751.9932...  6.4341 sec/batch\n",
      "Epoch: 3/20...  Training Step: 521...  Training loss: 15635.2432...  6.5743 sec/batch\n",
      "Epoch: 3/20...  Training Step: 522...  Training loss: 15214.8115...  7.3517 sec/batch\n",
      "Epoch: 3/20...  Training Step: 523...  Training loss: 15752.0713...  6.3757 sec/batch\n",
      "Epoch: 3/20...  Training Step: 524...  Training loss: 15766.1318...  7.6605 sec/batch\n",
      "Epoch: 3/20...  Training Step: 525...  Training loss: 15515.4180...  7.5513 sec/batch\n",
      "Epoch: 3/20...  Training Step: 526...  Training loss: 15406.1875...  8.5148 sec/batch\n",
      "Epoch: 3/20...  Training Step: 527...  Training loss: 15214.1133...  6.6027 sec/batch\n",
      "Epoch: 3/20...  Training Step: 528...  Training loss: 15296.0059...  6.6981 sec/batch\n",
      "Epoch: 3/20...  Training Step: 529...  Training loss: 15780.3359...  8.1362 sec/batch\n",
      "Epoch: 3/20...  Training Step: 530...  Training loss: 15642.9189...  6.4886 sec/batch\n",
      "Epoch: 3/20...  Training Step: 531...  Training loss: 15569.6484...  7.7623 sec/batch\n",
      "Epoch: 3/20...  Training Step: 532...  Training loss: 15632.9229...  6.5216 sec/batch\n",
      "Epoch: 3/20...  Training Step: 533...  Training loss: 15873.0781...  6.9930 sec/batch\n",
      "Epoch: 3/20...  Training Step: 534...  Training loss: 15714.8340...  6.9650 sec/batch\n",
      "Epoch: 3/20...  Training Step: 535...  Training loss: 15700.9102...  7.1762 sec/batch\n",
      "Epoch: 3/20...  Training Step: 536...  Training loss: 15655.6621...  7.6199 sec/batch\n",
      "Epoch: 3/20...  Training Step: 537...  Training loss: 15979.7461...  6.3952 sec/batch\n",
      "Epoch: 3/20...  Training Step: 538...  Training loss: 15605.7959...  7.7698 sec/batch\n",
      "Epoch: 3/20...  Training Step: 539...  Training loss: 15603.7871...  6.5488 sec/batch\n",
      "Epoch: 3/20...  Training Step: 540...  Training loss: 15768.9600...  8.3398 sec/batch\n",
      "Epoch: 3/20...  Training Step: 541...  Training loss: 15344.1113...  7.6760 sec/batch\n",
      "Epoch: 3/20...  Training Step: 542...  Training loss: 15709.5596...  6.4152 sec/batch\n",
      "Epoch: 3/20...  Training Step: 543...  Training loss: 15658.2383...  8.0964 sec/batch\n",
      "Epoch: 3/20...  Training Step: 544...  Training loss: 15853.2275...  5.7443 sec/batch\n",
      "Epoch: 3/20...  Training Step: 545...  Training loss: 15712.3496...  5.7530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 546...  Training loss: 15445.8672...  6.8416 sec/batch\n",
      "Epoch: 3/20...  Training Step: 547...  Training loss: 15191.1445...  5.6441 sec/batch\n",
      "Epoch: 3/20...  Training Step: 548...  Training loss: 15522.8936...  5.9243 sec/batch\n",
      "Epoch: 3/20...  Training Step: 549...  Training loss: 15589.2422...  6.6328 sec/batch\n",
      "Epoch: 3/20...  Training Step: 550...  Training loss: 15464.7773...  5.7960 sec/batch\n",
      "Epoch: 3/20...  Training Step: 551...  Training loss: 15580.5801...  6.8173 sec/batch\n",
      "Epoch: 3/20...  Training Step: 552...  Training loss: 15469.4766...  5.7318 sec/batch\n",
      "Epoch: 3/20...  Training Step: 553...  Training loss: 15584.6787...  5.7360 sec/batch\n",
      "Epoch: 3/20...  Training Step: 554...  Training loss: 15425.2695...  6.6631 sec/batch\n",
      "Epoch: 3/20...  Training Step: 555...  Training loss: 15096.4492...  5.6830 sec/batch\n",
      "Epoch: 3/20...  Training Step: 556...  Training loss: 15692.7539...  5.7365 sec/batch\n",
      "Epoch: 3/20...  Training Step: 557...  Training loss: 15870.2734...  6.8959 sec/batch\n",
      "Epoch: 3/20...  Training Step: 558...  Training loss: 15451.8477...  6.9555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 559...  Training loss: 15515.2500...  7.6547 sec/batch\n",
      "Epoch: 3/20...  Training Step: 560...  Training loss: 15466.9492...  6.4932 sec/batch\n",
      "Epoch: 3/20...  Training Step: 561...  Training loss: 15499.5078...  6.3410 sec/batch\n",
      "Epoch: 3/20...  Training Step: 562...  Training loss: 15515.9609...  7.7730 sec/batch\n",
      "Epoch: 3/20...  Training Step: 563...  Training loss: 15728.4414...  6.4756 sec/batch\n",
      "Epoch: 3/20...  Training Step: 564...  Training loss: 16137.7031...  7.7105 sec/batch\n",
      "Epoch: 3/20...  Training Step: 565...  Training loss: 15516.1240...  6.3674 sec/batch\n",
      "Epoch: 3/20...  Training Step: 566...  Training loss: 15348.3672...  6.5008 sec/batch\n",
      "Epoch: 3/20...  Training Step: 567...  Training loss: 15443.1094...  7.6208 sec/batch\n",
      "Epoch: 3/20...  Training Step: 568...  Training loss: 15289.5654...  6.3558 sec/batch\n",
      "Epoch: 3/20...  Training Step: 569...  Training loss: 15824.0010...  7.6294 sec/batch\n",
      "Epoch: 3/20...  Training Step: 570...  Training loss: 15595.2402...  6.3327 sec/batch\n",
      "Epoch: 3/20...  Training Step: 571...  Training loss: 15612.4287...  6.3678 sec/batch\n",
      "Epoch: 3/20...  Training Step: 572...  Training loss: 15269.3389...  7.5792 sec/batch\n",
      "Epoch: 3/20...  Training Step: 573...  Training loss: 15255.1719...  6.2881 sec/batch\n",
      "Epoch: 3/20...  Training Step: 574...  Training loss: 15560.4609...  7.5513 sec/batch\n",
      "Epoch: 3/20...  Training Step: 575...  Training loss: 15182.6602...  6.3109 sec/batch\n",
      "Epoch: 3/20...  Training Step: 576...  Training loss: 15042.1855...  6.3128 sec/batch\n",
      "Epoch: 3/20...  Training Step: 577...  Training loss: 15110.3740...  7.5474 sec/batch\n",
      "Epoch: 3/20...  Training Step: 578...  Training loss: 15427.6191...  6.2618 sec/batch\n",
      "Epoch: 3/20...  Training Step: 579...  Training loss: 15426.1094...  7.5582 sec/batch\n",
      "Epoch: 3/20...  Training Step: 580...  Training loss: 15407.8389...  6.2691 sec/batch\n",
      "Epoch: 3/20...  Training Step: 581...  Training loss: 15459.1689...  6.2934 sec/batch\n",
      "Epoch: 3/20...  Training Step: 582...  Training loss: 15250.0732...  7.6038 sec/batch\n",
      "Epoch: 3/20...  Training Step: 583...  Training loss: 15591.1904...  6.2910 sec/batch\n",
      "Epoch: 3/20...  Training Step: 584...  Training loss: 15442.0635...  7.6614 sec/batch\n",
      "Epoch: 3/20...  Training Step: 585...  Training loss: 15273.1016...  6.3005 sec/batch\n",
      "Epoch: 3/20...  Training Step: 586...  Training loss: 15268.5908...  6.2977 sec/batch\n",
      "Epoch: 3/20...  Training Step: 587...  Training loss: 15233.5996...  7.7324 sec/batch\n",
      "Epoch: 3/20...  Training Step: 588...  Training loss: 15109.0449...  5.8789 sec/batch\n",
      "Epoch: 3/20...  Training Step: 589...  Training loss: 15305.7734...  7.3749 sec/batch\n",
      "Epoch: 3/20...  Training Step: 590...  Training loss: 15171.3457...  6.2798 sec/batch\n",
      "Epoch: 3/20...  Training Step: 591...  Training loss: 15055.6367...  6.2251 sec/batch\n",
      "Epoch: 3/20...  Training Step: 592...  Training loss: 15320.8086...  7.3922 sec/batch\n",
      "Epoch: 3/20...  Training Step: 593...  Training loss: 15249.6377...  6.3576 sec/batch\n",
      "Epoch: 3/20...  Training Step: 594...  Training loss: 15195.9072...  7.3311 sec/batch\n",
      "Epoch: 4/20...  Training Step: 595...  Training loss: 16339.4150...  6.2906 sec/batch\n",
      "Epoch: 4/20...  Training Step: 596...  Training loss: 15309.7363...  6.2273 sec/batch\n",
      "Epoch: 4/20...  Training Step: 597...  Training loss: 15209.2754...  7.5816 sec/batch\n",
      "Epoch: 4/20...  Training Step: 598...  Training loss: 15402.8389...  6.2449 sec/batch\n",
      "Epoch: 4/20...  Training Step: 599...  Training loss: 15139.1973...  7.4690 sec/batch\n",
      "Epoch: 4/20...  Training Step: 600...  Training loss: 14923.1289...  6.2929 sec/batch\n",
      "Epoch: 4/20...  Training Step: 601...  Training loss: 15311.9844...  6.3316 sec/batch\n",
      "Epoch: 4/20...  Training Step: 602...  Training loss: 15191.5078...  7.4583 sec/batch\n",
      "Epoch: 4/20...  Training Step: 603...  Training loss: 15477.8809...  6.2935 sec/batch\n",
      "Epoch: 4/20...  Training Step: 604...  Training loss: 15252.4766...  7.4927 sec/batch\n",
      "Epoch: 4/20...  Training Step: 605...  Training loss: 15054.1719...  6.2382 sec/batch\n",
      "Epoch: 4/20...  Training Step: 606...  Training loss: 15064.7754...  6.2644 sec/batch\n",
      "Epoch: 4/20...  Training Step: 607...  Training loss: 15291.6621...  7.3689 sec/batch\n",
      "Epoch: 4/20...  Training Step: 608...  Training loss: 15565.7227...  6.8148 sec/batch\n",
      "Epoch: 4/20...  Training Step: 609...  Training loss: 15140.2178...  9.0672 sec/batch\n",
      "Epoch: 4/20...  Training Step: 610...  Training loss: 14993.0498...  6.7610 sec/batch\n",
      "Epoch: 4/20...  Training Step: 611...  Training loss: 15424.5615...  6.5275 sec/batch\n",
      "Epoch: 4/20...  Training Step: 612...  Training loss: 15570.5703...  7.7791 sec/batch\n",
      "Epoch: 4/20...  Training Step: 613...  Training loss: 15297.4258...  6.3357 sec/batch\n",
      "Epoch: 4/20...  Training Step: 614...  Training loss: 15549.9844...  7.5953 sec/batch\n",
      "Epoch: 4/20...  Training Step: 615...  Training loss: 15139.2998...  6.2727 sec/batch\n",
      "Epoch: 4/20...  Training Step: 616...  Training loss: 15405.4609...  6.6873 sec/batch\n",
      "Epoch: 4/20...  Training Step: 617...  Training loss: 15215.3223...  7.0619 sec/batch\n",
      "Epoch: 4/20...  Training Step: 618...  Training loss: 15167.7627...  6.6819 sec/batch\n",
      "Epoch: 4/20...  Training Step: 619...  Training loss: 15139.7832...  7.6108 sec/batch\n",
      "Epoch: 4/20...  Training Step: 620...  Training loss: 14804.0625...  6.2516 sec/batch\n",
      "Epoch: 4/20...  Training Step: 621...  Training loss: 14985.0107...  6.8354 sec/batch\n",
      "Epoch: 4/20...  Training Step: 622...  Training loss: 15353.2109...  7.0477 sec/batch\n",
      "Epoch: 4/20...  Training Step: 623...  Training loss: 15336.4297...  6.3088 sec/batch\n",
      "Epoch: 4/20...  Training Step: 624...  Training loss: 15477.8340...  7.4168 sec/batch\n",
      "Epoch: 4/20...  Training Step: 625...  Training loss: 15076.6660...  6.2560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 626...  Training loss: 14920.4043...  6.5485 sec/batch\n",
      "Epoch: 4/20...  Training Step: 627...  Training loss: 15360.0137...  7.2042 sec/batch\n",
      "Epoch: 4/20...  Training Step: 628...  Training loss: 15332.4912...  6.2163 sec/batch\n",
      "Epoch: 4/20...  Training Step: 629...  Training loss: 15046.2402...  7.3447 sec/batch\n",
      "Epoch: 4/20...  Training Step: 630...  Training loss: 15126.6533...  6.2491 sec/batch\n",
      "Epoch: 4/20...  Training Step: 631...  Training loss: 14870.7998...  6.2423 sec/batch\n",
      "Epoch: 4/20...  Training Step: 632...  Training loss: 14788.1230...  8.0745 sec/batch\n",
      "Epoch: 4/20...  Training Step: 633...  Training loss: 14611.9717...  7.5477 sec/batch\n",
      "Epoch: 4/20...  Training Step: 634...  Training loss: 14995.0645...  8.6906 sec/batch\n",
      "Epoch: 4/20...  Training Step: 635...  Training loss: 14962.8643...  7.2971 sec/batch\n",
      "Epoch: 4/20...  Training Step: 636...  Training loss: 15402.5459...  7.8021 sec/batch\n",
      "Epoch: 4/20...  Training Step: 637...  Training loss: 14904.7949...  7.5292 sec/batch\n",
      "Epoch: 4/20...  Training Step: 638...  Training loss: 14791.2344...  8.7673 sec/batch\n",
      "Epoch: 4/20...  Training Step: 639...  Training loss: 15216.0244...  6.7728 sec/batch\n",
      "Epoch: 4/20...  Training Step: 640...  Training loss: 14633.0742...  6.3601 sec/batch\n",
      "Epoch: 4/20...  Training Step: 641...  Training loss: 15054.2969...  7.4680 sec/batch\n",
      "Epoch: 4/20...  Training Step: 642...  Training loss: 14937.4609...  6.7900 sec/batch\n",
      "Epoch: 4/20...  Training Step: 643...  Training loss: 15027.4033...  8.5550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 644...  Training loss: 15248.3789...  6.2337 sec/batch\n",
      "Epoch: 4/20...  Training Step: 645...  Training loss: 14746.5449...  6.2227 sec/batch\n",
      "Epoch: 4/20...  Training Step: 646...  Training loss: 15559.5332...  7.3652 sec/batch\n",
      "Epoch: 4/20...  Training Step: 647...  Training loss: 15147.5801...  6.2709 sec/batch\n",
      "Epoch: 4/20...  Training Step: 648...  Training loss: 15210.6914...  7.5472 sec/batch\n",
      "Epoch: 4/20...  Training Step: 649...  Training loss: 15023.3838...  6.1826 sec/batch\n",
      "Epoch: 4/20...  Training Step: 650...  Training loss: 15007.4268...  6.2219 sec/batch\n",
      "Epoch: 4/20...  Training Step: 651...  Training loss: 15212.9736...  7.4899 sec/batch\n",
      "Epoch: 4/20...  Training Step: 652...  Training loss: 14858.1631...  6.2186 sec/batch\n",
      "Epoch: 4/20...  Training Step: 653...  Training loss: 14800.9336...  7.4263 sec/batch\n",
      "Epoch: 4/20...  Training Step: 654...  Training loss: 15423.8770...  6.1911 sec/batch\n",
      "Epoch: 4/20...  Training Step: 655...  Training loss: 15103.3047...  6.2725 sec/batch\n",
      "Epoch: 4/20...  Training Step: 656...  Training loss: 15562.2148...  7.4703 sec/batch\n",
      "Epoch: 4/20...  Training Step: 657...  Training loss: 15343.7236...  6.3459 sec/batch\n",
      "Epoch: 4/20...  Training Step: 658...  Training loss: 15167.6357...  7.6512 sec/batch\n",
      "Epoch: 4/20...  Training Step: 659...  Training loss: 14959.0098...  6.2375 sec/batch\n",
      "Epoch: 4/20...  Training Step: 660...  Training loss: 15182.2900...  6.2021 sec/batch\n",
      "Epoch: 4/20...  Training Step: 661...  Training loss: 15247.8301...  7.4326 sec/batch\n",
      "Epoch: 4/20...  Training Step: 662...  Training loss: 14725.4102...  6.3499 sec/batch\n",
      "Epoch: 4/20...  Training Step: 663...  Training loss: 14985.3242...  7.5204 sec/batch\n",
      "Epoch: 4/20...  Training Step: 664...  Training loss: 14984.7461...  6.3624 sec/batch\n",
      "Epoch: 4/20...  Training Step: 665...  Training loss: 15370.3750...  6.2594 sec/batch\n",
      "Epoch: 4/20...  Training Step: 666...  Training loss: 15159.1602...  7.4401 sec/batch\n",
      "Epoch: 4/20...  Training Step: 667...  Training loss: 15223.6992...  6.1601 sec/batch\n",
      "Epoch: 4/20...  Training Step: 668...  Training loss: 14910.0605...  7.8426 sec/batch\n",
      "Epoch: 4/20...  Training Step: 669...  Training loss: 14884.8184...  6.2586 sec/batch\n",
      "Epoch: 4/20...  Training Step: 670...  Training loss: 15288.5068...  6.2779 sec/batch\n",
      "Epoch: 4/20...  Training Step: 671...  Training loss: 14853.3887...  7.3879 sec/batch\n",
      "Epoch: 4/20...  Training Step: 672...  Training loss: 14961.8809...  6.1953 sec/batch\n",
      "Epoch: 4/20...  Training Step: 673...  Training loss: 14620.4219...  7.4685 sec/batch\n",
      "Epoch: 4/20...  Training Step: 674...  Training loss: 15041.4102...  6.4145 sec/batch\n",
      "Epoch: 4/20...  Training Step: 675...  Training loss: 14507.9951...  6.6606 sec/batch\n",
      "Epoch: 4/20...  Training Step: 676...  Training loss: 14919.6475...  7.5870 sec/batch\n",
      "Epoch: 4/20...  Training Step: 677...  Training loss: 14734.2246...  6.2293 sec/batch\n",
      "Epoch: 4/20...  Training Step: 678...  Training loss: 14939.8145...  7.5765 sec/batch\n",
      "Epoch: 4/20...  Training Step: 679...  Training loss: 14635.6855...  6.3312 sec/batch\n",
      "Epoch: 4/20...  Training Step: 680...  Training loss: 14848.0176...  6.3163 sec/batch\n",
      "Epoch: 4/20...  Training Step: 681...  Training loss: 14667.6035...  7.5164 sec/batch\n",
      "Epoch: 4/20...  Training Step: 682...  Training loss: 14686.3223...  6.2346 sec/batch\n",
      "Epoch: 4/20...  Training Step: 683...  Training loss: 14549.3311...  7.5029 sec/batch\n",
      "Epoch: 4/20...  Training Step: 684...  Training loss: 15062.4395...  6.3384 sec/batch\n",
      "Epoch: 4/20...  Training Step: 685...  Training loss: 14664.2207...  6.3242 sec/batch\n",
      "Epoch: 4/20...  Training Step: 686...  Training loss: 14704.2930...  7.6407 sec/batch\n",
      "Epoch: 4/20...  Training Step: 687...  Training loss: 14625.2637...  7.2103 sec/batch\n",
      "Epoch: 4/20...  Training Step: 688...  Training loss: 14684.0781...  7.8973 sec/batch\n",
      "Epoch: 4/20...  Training Step: 689...  Training loss: 14610.8018...  6.4017 sec/batch\n",
      "Epoch: 4/20...  Training Step: 690...  Training loss: 14989.5742...  6.3696 sec/batch\n",
      "Epoch: 4/20...  Training Step: 691...  Training loss: 14909.6504...  7.6895 sec/batch\n",
      "Epoch: 4/20...  Training Step: 692...  Training loss: 14537.5742...  6.3607 sec/batch\n",
      "Epoch: 4/20...  Training Step: 693...  Training loss: 14744.5762...  7.5121 sec/batch\n",
      "Epoch: 4/20...  Training Step: 694...  Training loss: 14508.9121...  6.3840 sec/batch\n",
      "Epoch: 4/20...  Training Step: 695...  Training loss: 14906.8691...  6.3728 sec/batch\n",
      "Epoch: 4/20...  Training Step: 696...  Training loss: 14818.7266...  7.5198 sec/batch\n",
      "Epoch: 4/20...  Training Step: 697...  Training loss: 14788.6094...  6.2566 sec/batch\n",
      "Epoch: 4/20...  Training Step: 698...  Training loss: 14657.0244...  7.5958 sec/batch\n",
      "Epoch: 4/20...  Training Step: 699...  Training loss: 14774.0137...  8.4345 sec/batch\n",
      "Epoch: 4/20...  Training Step: 700...  Training loss: 14808.2383...  8.3097 sec/batch\n",
      "Epoch: 4/20...  Training Step: 701...  Training loss: 14788.0508...  6.3960 sec/batch\n",
      "Epoch: 4/20...  Training Step: 702...  Training loss: 14674.0352...  6.5506 sec/batch\n",
      "Epoch: 4/20...  Training Step: 703...  Training loss: 14715.1123...  7.6723 sec/batch\n",
      "Epoch: 4/20...  Training Step: 704...  Training loss: 14969.2793...  6.2460 sec/batch\n",
      "Epoch: 4/20...  Training Step: 705...  Training loss: 14646.8359...  7.5513 sec/batch\n",
      "Epoch: 4/20...  Training Step: 706...  Training loss: 14842.4834...  6.2442 sec/batch\n",
      "Epoch: 4/20...  Training Step: 707...  Training loss: 14737.5801...  6.2134 sec/batch\n",
      "Epoch: 4/20...  Training Step: 708...  Training loss: 14633.1895...  7.9164 sec/batch\n",
      "Epoch: 4/20...  Training Step: 709...  Training loss: 14560.3730...  6.4289 sec/batch\n",
      "Epoch: 4/20...  Training Step: 710...  Training loss: 14362.2637...  7.5606 sec/batch\n",
      "Epoch: 4/20...  Training Step: 711...  Training loss: 14842.7363...  6.1833 sec/batch\n",
      "Epoch: 4/20...  Training Step: 712...  Training loss: 14775.3477...  6.4479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 713...  Training loss: 14848.5020...  7.7424 sec/batch\n",
      "Epoch: 4/20...  Training Step: 714...  Training loss: 14679.1035...  6.9725 sec/batch\n",
      "Epoch: 4/20...  Training Step: 715...  Training loss: 14837.9434...  8.8116 sec/batch\n",
      "Epoch: 4/20...  Training Step: 716...  Training loss: 14370.0742...  7.0176 sec/batch\n",
      "Epoch: 4/20...  Training Step: 717...  Training loss: 14444.3027...  7.6885 sec/batch\n",
      "Epoch: 4/20...  Training Step: 718...  Training loss: 14861.2559...  7.1468 sec/batch\n",
      "Epoch: 4/20...  Training Step: 719...  Training loss: 14752.5557...  6.7674 sec/batch\n",
      "Epoch: 4/20...  Training Step: 720...  Training loss: 14378.8027...  7.8386 sec/batch\n",
      "Epoch: 4/20...  Training Step: 721...  Training loss: 14864.8311...  6.5976 sec/batch\n",
      "Epoch: 4/20...  Training Step: 722...  Training loss: 14887.6436...  7.8874 sec/batch\n",
      "Epoch: 4/20...  Training Step: 723...  Training loss: 14615.1562...  6.7671 sec/batch\n",
      "Epoch: 4/20...  Training Step: 724...  Training loss: 14465.9160...  6.4150 sec/batch\n",
      "Epoch: 4/20...  Training Step: 725...  Training loss: 14337.5234...  8.1538 sec/batch\n",
      "Epoch: 4/20...  Training Step: 726...  Training loss: 14602.8877...  6.4164 sec/batch\n",
      "Epoch: 4/20...  Training Step: 727...  Training loss: 14917.8203...  7.6268 sec/batch\n",
      "Epoch: 4/20...  Training Step: 728...  Training loss: 14758.1543...  6.3738 sec/batch\n",
      "Epoch: 4/20...  Training Step: 729...  Training loss: 14787.9316...  6.7189 sec/batch\n",
      "Epoch: 4/20...  Training Step: 730...  Training loss: 14618.2236...  7.3890 sec/batch\n",
      "Epoch: 4/20...  Training Step: 731...  Training loss: 14876.5020...  6.2884 sec/batch\n",
      "Epoch: 4/20...  Training Step: 732...  Training loss: 14929.9844...  7.7818 sec/batch\n",
      "Epoch: 4/20...  Training Step: 733...  Training loss: 14772.6631...  6.7208 sec/batch\n",
      "Epoch: 4/20...  Training Step: 734...  Training loss: 14737.4316...  7.5614 sec/batch\n",
      "Epoch: 4/20...  Training Step: 735...  Training loss: 15138.6406...  6.6618 sec/batch\n",
      "Epoch: 4/20...  Training Step: 736...  Training loss: 14764.4961...  6.4540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 737...  Training loss: 14754.6426...  7.6777 sec/batch\n",
      "Epoch: 4/20...  Training Step: 738...  Training loss: 14972.8135...  6.4204 sec/batch\n",
      "Epoch: 4/20...  Training Step: 739...  Training loss: 14494.3887...  7.5933 sec/batch\n",
      "Epoch: 4/20...  Training Step: 740...  Training loss: 14859.2383...  6.3456 sec/batch\n",
      "Epoch: 4/20...  Training Step: 741...  Training loss: 14887.8965...  6.5299 sec/batch\n",
      "Epoch: 4/20...  Training Step: 742...  Training loss: 15073.7109...  7.4819 sec/batch\n",
      "Epoch: 4/20...  Training Step: 743...  Training loss: 14855.2637...  6.4135 sec/batch\n",
      "Epoch: 4/20...  Training Step: 744...  Training loss: 14627.2090...  8.1078 sec/batch\n",
      "Epoch: 4/20...  Training Step: 745...  Training loss: 14415.9873...  6.2896 sec/batch\n",
      "Epoch: 4/20...  Training Step: 746...  Training loss: 14649.9102...  6.2657 sec/batch\n",
      "Epoch: 4/20...  Training Step: 747...  Training loss: 14885.4160...  7.6296 sec/batch\n",
      "Epoch: 4/20...  Training Step: 748...  Training loss: 14731.0078...  6.4043 sec/batch\n",
      "Epoch: 4/20...  Training Step: 749...  Training loss: 14714.5410...  7.6587 sec/batch\n",
      "Epoch: 4/20...  Training Step: 750...  Training loss: 14634.2773...  6.3773 sec/batch\n",
      "Epoch: 4/20...  Training Step: 751...  Training loss: 14687.6289...  6.3096 sec/batch\n",
      "Epoch: 4/20...  Training Step: 752...  Training loss: 14616.0918...  7.5818 sec/batch\n",
      "Epoch: 4/20...  Training Step: 753...  Training loss: 14315.1211...  6.3981 sec/batch\n",
      "Epoch: 4/20...  Training Step: 754...  Training loss: 14876.6094...  7.7756 sec/batch\n",
      "Epoch: 4/20...  Training Step: 755...  Training loss: 14904.6758...  6.3317 sec/batch\n",
      "Epoch: 4/20...  Training Step: 756...  Training loss: 14605.1914...  6.3092 sec/batch\n",
      "Epoch: 4/20...  Training Step: 757...  Training loss: 14630.8945...  7.4707 sec/batch\n",
      "Epoch: 4/20...  Training Step: 758...  Training loss: 14735.3594...  6.3159 sec/batch\n",
      "Epoch: 4/20...  Training Step: 759...  Training loss: 14642.9102...  7.6546 sec/batch\n",
      "Epoch: 4/20...  Training Step: 760...  Training loss: 14662.0830...  6.5062 sec/batch\n",
      "Epoch: 4/20...  Training Step: 761...  Training loss: 14881.1738...  6.3571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 762...  Training loss: 15352.8203...  7.6788 sec/batch\n",
      "Epoch: 4/20...  Training Step: 763...  Training loss: 14741.0918...  6.3544 sec/batch\n",
      "Epoch: 4/20...  Training Step: 764...  Training loss: 14614.8984...  7.6085 sec/batch\n",
      "Epoch: 4/20...  Training Step: 765...  Training loss: 14649.9453...  6.5365 sec/batch\n",
      "Epoch: 4/20...  Training Step: 766...  Training loss: 14558.4883...  6.5382 sec/batch\n",
      "Epoch: 4/20...  Training Step: 767...  Training loss: 15048.2529...  7.7668 sec/batch\n",
      "Epoch: 4/20...  Training Step: 768...  Training loss: 14713.1377...  6.4818 sec/batch\n",
      "Epoch: 4/20...  Training Step: 769...  Training loss: 14929.2715...  8.3212 sec/batch\n",
      "Epoch: 4/20...  Training Step: 770...  Training loss: 14423.1318...  6.8469 sec/batch\n",
      "Epoch: 4/20...  Training Step: 771...  Training loss: 14438.3398...  7.8271 sec/batch\n",
      "Epoch: 4/20...  Training Step: 772...  Training loss: 14844.0098...  6.5936 sec/batch\n",
      "Epoch: 4/20...  Training Step: 773...  Training loss: 14345.8047...  6.5684 sec/batch\n",
      "Epoch: 4/20...  Training Step: 774...  Training loss: 14312.6582...  7.7764 sec/batch\n",
      "Epoch: 4/20...  Training Step: 775...  Training loss: 14362.7891...  6.7028 sec/batch\n",
      "Epoch: 4/20...  Training Step: 776...  Training loss: 14696.1816...  7.8890 sec/batch\n",
      "Epoch: 4/20...  Training Step: 777...  Training loss: 14715.5029...  6.5537 sec/batch\n",
      "Epoch: 4/20...  Training Step: 778...  Training loss: 14661.0410...  7.3928 sec/batch\n",
      "Epoch: 4/20...  Training Step: 779...  Training loss: 14668.6641...  7.2474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 780...  Training loss: 14545.9424...  6.6693 sec/batch\n",
      "Epoch: 4/20...  Training Step: 781...  Training loss: 14874.3203...  7.8256 sec/batch\n",
      "Epoch: 4/20...  Training Step: 782...  Training loss: 14513.0049...  6.7189 sec/batch\n",
      "Epoch: 4/20...  Training Step: 783...  Training loss: 14636.6211...  7.7988 sec/batch\n",
      "Epoch: 4/20...  Training Step: 784...  Training loss: 14507.5156...  6.5456 sec/batch\n",
      "Epoch: 4/20...  Training Step: 785...  Training loss: 14470.8770...  6.4373 sec/batch\n",
      "Epoch: 4/20...  Training Step: 786...  Training loss: 14340.6250...  7.6625 sec/batch\n",
      "Epoch: 4/20...  Training Step: 787...  Training loss: 14501.3408...  6.3626 sec/batch\n",
      "Epoch: 4/20...  Training Step: 788...  Training loss: 14350.5771...  7.8217 sec/batch\n",
      "Epoch: 4/20...  Training Step: 789...  Training loss: 14371.5225...  6.5318 sec/batch\n",
      "Epoch: 4/20...  Training Step: 790...  Training loss: 14553.5840...  6.5809 sec/batch\n",
      "Epoch: 4/20...  Training Step: 791...  Training loss: 14518.5781...  7.6717 sec/batch\n",
      "Epoch: 4/20...  Training Step: 792...  Training loss: 14306.4785...  6.4551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 793...  Training loss: 15820.9316...  7.6753 sec/batch\n",
      "Epoch: 5/20...  Training Step: 794...  Training loss: 14760.2383...  6.3433 sec/batch\n",
      "Epoch: 5/20...  Training Step: 795...  Training loss: 14524.8672...  6.4358 sec/batch\n",
      "Epoch: 5/20...  Training Step: 796...  Training loss: 14849.2441...  7.5733 sec/batch\n",
      "Epoch: 5/20...  Training Step: 797...  Training loss: 14432.6504...  6.3899 sec/batch\n",
      "Epoch: 5/20...  Training Step: 798...  Training loss: 14195.1191...  7.6245 sec/batch\n",
      "Epoch: 5/20...  Training Step: 799...  Training loss: 14498.7881...  6.3917 sec/batch\n",
      "Epoch: 5/20...  Training Step: 800...  Training loss: 14470.3887...  6.3084 sec/batch\n",
      "Epoch: 5/20...  Training Step: 801...  Training loss: 14670.8027...  6.8895 sec/batch\n",
      "Epoch: 5/20...  Training Step: 802...  Training loss: 14447.4277...  6.1879 sec/batch\n",
      "Epoch: 5/20...  Training Step: 803...  Training loss: 14375.3691...  7.6004 sec/batch\n",
      "Epoch: 5/20...  Training Step: 804...  Training loss: 14464.1055...  6.4637 sec/batch\n",
      "Epoch: 5/20...  Training Step: 805...  Training loss: 14465.6426...  6.9815 sec/batch\n",
      "Epoch: 5/20...  Training Step: 806...  Training loss: 14843.1113...  6.9411 sec/batch\n",
      "Epoch: 5/20...  Training Step: 807...  Training loss: 14366.5664...  6.3194 sec/batch\n",
      "Epoch: 5/20...  Training Step: 808...  Training loss: 14233.6260...  7.6452 sec/batch\n",
      "Epoch: 5/20...  Training Step: 809...  Training loss: 14709.9004...  6.4008 sec/batch\n",
      "Epoch: 5/20...  Training Step: 810...  Training loss: 14730.3916...  7.0879 sec/batch\n",
      "Epoch: 5/20...  Training Step: 811...  Training loss: 14609.2764...  6.9220 sec/batch\n",
      "Epoch: 5/20...  Training Step: 812...  Training loss: 14712.2754...  6.3237 sec/batch\n",
      "Epoch: 5/20...  Training Step: 813...  Training loss: 14400.4844...  8.0883 sec/batch\n",
      "Epoch: 5/20...  Training Step: 814...  Training loss: 14662.3457...  6.5453 sec/batch\n",
      "Epoch: 5/20...  Training Step: 815...  Training loss: 14418.8164...  7.7346 sec/batch\n",
      "Epoch: 5/20...  Training Step: 816...  Training loss: 14541.7617...  6.6058 sec/batch\n",
      "Epoch: 5/20...  Training Step: 817...  Training loss: 14473.6816...  6.2806 sec/batch\n",
      "Epoch: 5/20...  Training Step: 818...  Training loss: 14038.1045...  7.6387 sec/batch\n",
      "Epoch: 5/20...  Training Step: 819...  Training loss: 14187.9092...  6.2684 sec/batch\n",
      "Epoch: 5/20...  Training Step: 820...  Training loss: 14680.6113...  7.3817 sec/batch\n",
      "Epoch: 5/20...  Training Step: 821...  Training loss: 14698.4824...  6.4383 sec/batch\n",
      "Epoch: 5/20...  Training Step: 822...  Training loss: 14634.4473...  6.7231 sec/batch\n",
      "Epoch: 5/20...  Training Step: 823...  Training loss: 14515.1729...  8.7555 sec/batch\n",
      "Epoch: 5/20...  Training Step: 824...  Training loss: 14359.1631...  7.3990 sec/batch\n",
      "Epoch: 5/20...  Training Step: 825...  Training loss: 14585.6816...  8.9485 sec/batch\n",
      "Epoch: 5/20...  Training Step: 826...  Training loss: 14524.2812...  6.5905 sec/batch\n",
      "Epoch: 5/20...  Training Step: 827...  Training loss: 14490.2676...  6.7701 sec/batch\n",
      "Epoch: 5/20...  Training Step: 828...  Training loss: 14482.9023...  7.5303 sec/batch\n",
      "Epoch: 5/20...  Training Step: 829...  Training loss: 14258.4336...  6.6705 sec/batch\n",
      "Epoch: 5/20...  Training Step: 830...  Training loss: 14050.2930...  7.5809 sec/batch\n",
      "Epoch: 5/20...  Training Step: 831...  Training loss: 14007.7939...  6.4856 sec/batch\n",
      "Epoch: 5/20...  Training Step: 832...  Training loss: 14293.7871...  7.6148 sec/batch\n",
      "Epoch: 5/20...  Training Step: 833...  Training loss: 14205.7344...  6.5844 sec/batch\n",
      "Epoch: 5/20...  Training Step: 834...  Training loss: 14686.6582...  6.1970 sec/batch\n",
      "Epoch: 5/20...  Training Step: 835...  Training loss: 14290.9414...  7.6497 sec/batch\n",
      "Epoch: 5/20...  Training Step: 836...  Training loss: 14239.8369...  6.2802 sec/batch\n",
      "Epoch: 5/20...  Training Step: 837...  Training loss: 14554.4785...  7.6143 sec/batch\n",
      "Epoch: 5/20...  Training Step: 838...  Training loss: 14118.1084...  6.4304 sec/batch\n",
      "Epoch: 5/20...  Training Step: 839...  Training loss: 14339.0410...  6.3444 sec/batch\n",
      "Epoch: 5/20...  Training Step: 840...  Training loss: 14285.3203...  7.5663 sec/batch\n",
      "Epoch: 5/20...  Training Step: 841...  Training loss: 14323.2412...  6.3325 sec/batch\n",
      "Epoch: 5/20...  Training Step: 842...  Training loss: 14558.8965...  7.6341 sec/batch\n",
      "Epoch: 5/20...  Training Step: 843...  Training loss: 14184.8105...  6.4931 sec/batch\n",
      "Epoch: 5/20...  Training Step: 844...  Training loss: 14805.3047...  6.3911 sec/batch\n",
      "Epoch: 5/20...  Training Step: 845...  Training loss: 14485.7109...  8.0285 sec/batch\n",
      "Epoch: 5/20...  Training Step: 846...  Training loss: 14454.0635...  6.6340 sec/batch\n",
      "Epoch: 5/20...  Training Step: 847...  Training loss: 14359.4463...  8.1401 sec/batch\n",
      "Epoch: 5/20...  Training Step: 848...  Training loss: 14442.7520...  6.5520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 849...  Training loss: 14635.6846...  8.0140 sec/batch\n",
      "Epoch: 5/20...  Training Step: 850...  Training loss: 14247.9570...  7.0818 sec/batch\n",
      "Epoch: 5/20...  Training Step: 851...  Training loss: 14225.1172...  6.7289 sec/batch\n",
      "Epoch: 5/20...  Training Step: 852...  Training loss: 14791.4023...  9.2667 sec/batch\n",
      "Epoch: 5/20...  Training Step: 853...  Training loss: 14447.5449...  7.8509 sec/batch\n",
      "Epoch: 5/20...  Training Step: 854...  Training loss: 14919.6846...  7.8348 sec/batch\n",
      "Epoch: 5/20...  Training Step: 855...  Training loss: 14772.2803...  6.3396 sec/batch\n",
      "Epoch: 5/20...  Training Step: 856...  Training loss: 14547.7520...  7.7369 sec/batch\n",
      "Epoch: 5/20...  Training Step: 857...  Training loss: 14338.4922...  6.5032 sec/batch\n",
      "Epoch: 5/20...  Training Step: 858...  Training loss: 14487.4521...  6.4064 sec/batch\n",
      "Epoch: 5/20...  Training Step: 859...  Training loss: 14662.9141...  7.6621 sec/batch\n",
      "Epoch: 5/20...  Training Step: 860...  Training loss: 14189.6992...  6.5031 sec/batch\n",
      "Epoch: 5/20...  Training Step: 861...  Training loss: 14475.8105...  7.5538 sec/batch\n",
      "Epoch: 5/20...  Training Step: 862...  Training loss: 14313.8574...  6.3829 sec/batch\n",
      "Epoch: 5/20...  Training Step: 863...  Training loss: 14773.4082...  6.3940 sec/batch\n",
      "Epoch: 5/20...  Training Step: 864...  Training loss: 14662.1387...  7.5842 sec/batch\n",
      "Epoch: 5/20...  Training Step: 865...  Training loss: 14699.0605...  6.4145 sec/batch\n",
      "Epoch: 5/20...  Training Step: 866...  Training loss: 14359.0078...  7.5410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 867...  Training loss: 14383.2734...  6.3718 sec/batch\n",
      "Epoch: 5/20...  Training Step: 868...  Training loss: 14676.0283...  6.2271 sec/batch\n",
      "Epoch: 5/20...  Training Step: 869...  Training loss: 14457.1445...  7.6734 sec/batch\n",
      "Epoch: 5/20...  Training Step: 870...  Training loss: 14305.9727...  6.3897 sec/batch\n",
      "Epoch: 5/20...  Training Step: 871...  Training loss: 13987.5371...  7.7944 sec/batch\n",
      "Epoch: 5/20...  Training Step: 872...  Training loss: 14494.0293...  6.6032 sec/batch\n",
      "Epoch: 5/20...  Training Step: 873...  Training loss: 13922.5508...  6.3566 sec/batch\n",
      "Epoch: 5/20...  Training Step: 874...  Training loss: 14481.8623...  7.6951 sec/batch\n",
      "Epoch: 5/20...  Training Step: 875...  Training loss: 14029.1689...  6.9675 sec/batch\n",
      "Epoch: 5/20...  Training Step: 876...  Training loss: 14221.2275...  7.5598 sec/batch\n",
      "Epoch: 5/20...  Training Step: 877...  Training loss: 14155.8965...  6.3425 sec/batch\n",
      "Epoch: 5/20...  Training Step: 878...  Training loss: 14106.3301...  6.4539 sec/batch\n",
      "Epoch: 5/20...  Training Step: 879...  Training loss: 14148.9902...  7.6086 sec/batch\n",
      "Epoch: 5/20...  Training Step: 880...  Training loss: 14150.2188...  6.3281 sec/batch\n",
      "Epoch: 5/20...  Training Step: 881...  Training loss: 14020.7725...  7.5392 sec/batch\n",
      "Epoch: 5/20...  Training Step: 882...  Training loss: 14457.1484...  6.2914 sec/batch\n",
      "Epoch: 5/20...  Training Step: 883...  Training loss: 14053.1045...  6.3168 sec/batch\n",
      "Epoch: 5/20...  Training Step: 884...  Training loss: 14179.8066...  7.7161 sec/batch\n",
      "Epoch: 5/20...  Training Step: 885...  Training loss: 14067.6260...  6.3797 sec/batch\n",
      "Epoch: 5/20...  Training Step: 886...  Training loss: 14195.1230...  7.5534 sec/batch\n",
      "Epoch: 5/20...  Training Step: 887...  Training loss: 14086.4199...  6.2824 sec/batch\n",
      "Epoch: 5/20...  Training Step: 888...  Training loss: 14548.1230...  6.3505 sec/batch\n",
      "Epoch: 5/20...  Training Step: 889...  Training loss: 14370.8701...  7.5465 sec/batch\n",
      "Epoch: 5/20...  Training Step: 890...  Training loss: 14084.2852...  6.3066 sec/batch\n",
      "Epoch: 5/20...  Training Step: 891...  Training loss: 14200.8184...  7.5410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 892...  Training loss: 13958.9121...  6.3284 sec/batch\n",
      "Epoch: 5/20...  Training Step: 893...  Training loss: 14361.9297...  9.7924 sec/batch\n",
      "Epoch: 5/20...  Training Step: 894...  Training loss: 14179.6934...  6.5073 sec/batch\n",
      "Epoch: 5/20...  Training Step: 895...  Training loss: 14163.4980...  6.5425 sec/batch\n",
      "Epoch: 5/20...  Training Step: 896...  Training loss: 14249.5830...  7.5765 sec/batch\n",
      "Epoch: 5/20...  Training Step: 897...  Training loss: 14244.3193...  6.2390 sec/batch\n",
      "Epoch: 5/20...  Training Step: 898...  Training loss: 14108.1748...  7.7443 sec/batch\n",
      "Epoch: 5/20...  Training Step: 899...  Training loss: 14313.6250...  6.3075 sec/batch\n",
      "Epoch: 5/20...  Training Step: 900...  Training loss: 14252.0576...  6.2552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 901...  Training loss: 14184.3008...  9.1938 sec/batch\n",
      "Epoch: 5/20...  Training Step: 902...  Training loss: 14548.1885...  6.9016 sec/batch\n",
      "Epoch: 5/20...  Training Step: 903...  Training loss: 14101.7461...  9.1242 sec/batch\n",
      "Epoch: 5/20...  Training Step: 904...  Training loss: 14226.4180...  6.5399 sec/batch\n",
      "Epoch: 5/20...  Training Step: 905...  Training loss: 14230.1221...  6.3200 sec/batch\n",
      "Epoch: 5/20...  Training Step: 906...  Training loss: 14159.2764...  6.4739 sec/batch\n",
      "Epoch: 5/20...  Training Step: 907...  Training loss: 14094.1670...  6.4196 sec/batch\n",
      "Epoch: 5/20...  Training Step: 908...  Training loss: 13854.3350...  7.8990 sec/batch\n",
      "Epoch: 5/20...  Training Step: 909...  Training loss: 14303.7695...  5.8119 sec/batch\n",
      "Epoch: 5/20...  Training Step: 910...  Training loss: 14371.7812...  5.8148 sec/batch\n",
      "Epoch: 5/20...  Training Step: 911...  Training loss: 14164.5098...  7.2016 sec/batch\n",
      "Epoch: 5/20...  Training Step: 912...  Training loss: 14177.6562...  6.6669 sec/batch\n",
      "Epoch: 5/20...  Training Step: 913...  Training loss: 14323.8164...  7.0877 sec/batch\n",
      "Epoch: 5/20...  Training Step: 914...  Training loss: 13918.0244...  5.9299 sec/batch\n",
      "Epoch: 5/20...  Training Step: 915...  Training loss: 13781.9736...  5.9102 sec/batch\n",
      "Epoch: 5/20...  Training Step: 916...  Training loss: 14313.6660...  7.7444 sec/batch\n",
      "Epoch: 5/20...  Training Step: 917...  Training loss: 14127.6211...  6.5346 sec/batch\n",
      "Epoch: 5/20...  Training Step: 918...  Training loss: 13908.1719...  6.8881 sec/batch\n",
      "Epoch: 5/20...  Training Step: 919...  Training loss: 14359.2773...  5.7176 sec/batch\n",
      "Epoch: 5/20...  Training Step: 920...  Training loss: 14313.7842...  6.4447 sec/batch\n",
      "Epoch: 5/20...  Training Step: 921...  Training loss: 14134.5205...  7.8873 sec/batch\n",
      "Epoch: 5/20...  Training Step: 922...  Training loss: 13855.5605...  5.6966 sec/batch\n",
      "Epoch: 5/20...  Training Step: 923...  Training loss: 13816.7617...  5.7683 sec/batch\n",
      "Epoch: 5/20...  Training Step: 924...  Training loss: 13987.7969...  7.1993 sec/batch\n",
      "Epoch: 5/20...  Training Step: 925...  Training loss: 14499.2754...  6.8750 sec/batch\n",
      "Epoch: 5/20...  Training Step: 926...  Training loss: 14310.2852...  7.8361 sec/batch\n",
      "Epoch: 5/20...  Training Step: 927...  Training loss: 14308.4951...  6.2453 sec/batch\n",
      "Epoch: 5/20...  Training Step: 928...  Training loss: 14143.1191...  5.7102 sec/batch\n",
      "Epoch: 5/20...  Training Step: 929...  Training loss: 14457.3652...  6.9040 sec/batch\n",
      "Epoch: 5/20...  Training Step: 930...  Training loss: 14344.5225...  6.1493 sec/batch\n",
      "Epoch: 5/20...  Training Step: 931...  Training loss: 14277.8730...  6.7449 sec/batch\n",
      "Epoch: 5/20...  Training Step: 932...  Training loss: 14312.6738...  5.7706 sec/batch\n",
      "Epoch: 5/20...  Training Step: 933...  Training loss: 14656.8203...  5.7904 sec/batch\n",
      "Epoch: 5/20...  Training Step: 934...  Training loss: 14337.0391...  6.7560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 935...  Training loss: 14095.0068...  5.7148 sec/batch\n",
      "Epoch: 5/20...  Training Step: 936...  Training loss: 14439.1279...  5.7276 sec/batch\n",
      "Epoch: 5/20...  Training Step: 937...  Training loss: 14016.4395...  6.7778 sec/batch\n",
      "Epoch: 5/20...  Training Step: 938...  Training loss: 14476.7705...  5.7106 sec/batch\n",
      "Epoch: 5/20...  Training Step: 939...  Training loss: 14318.2891...  5.9372 sec/batch\n",
      "Epoch: 5/20...  Training Step: 940...  Training loss: 14557.3760...  6.5237 sec/batch\n",
      "Epoch: 5/20...  Training Step: 941...  Training loss: 14407.9941...  5.7636 sec/batch\n",
      "Epoch: 5/20...  Training Step: 942...  Training loss: 14241.8281...  6.8213 sec/batch\n",
      "Epoch: 5/20...  Training Step: 943...  Training loss: 13918.4932...  5.8069 sec/batch\n",
      "Epoch: 5/20...  Training Step: 944...  Training loss: 14002.8047...  5.7029 sec/batch\n",
      "Epoch: 5/20...  Training Step: 945...  Training loss: 14428.7090...  6.7877 sec/batch\n",
      "Epoch: 5/20...  Training Step: 946...  Training loss: 14207.2344...  5.7139 sec/batch\n",
      "Epoch: 5/20...  Training Step: 947...  Training loss: 14199.4229...  5.7696 sec/batch\n",
      "Epoch: 5/20...  Training Step: 948...  Training loss: 14060.6865...  6.8930 sec/batch\n",
      "Epoch: 5/20...  Training Step: 949...  Training loss: 14375.6680...  5.7668 sec/batch\n",
      "Epoch: 5/20...  Training Step: 950...  Training loss: 14195.6611...  6.2194 sec/batch\n",
      "Epoch: 5/20...  Training Step: 951...  Training loss: 13855.8750...  6.2893 sec/batch\n",
      "Epoch: 5/20...  Training Step: 952...  Training loss: 14344.6904...  5.7722 sec/batch\n",
      "Epoch: 5/20...  Training Step: 953...  Training loss: 14476.9775...  6.7643 sec/batch\n",
      "Epoch: 5/20...  Training Step: 954...  Training loss: 14196.3701...  5.7111 sec/batch\n",
      "Epoch: 5/20...  Training Step: 955...  Training loss: 14140.3613...  5.7734 sec/batch\n",
      "Epoch: 5/20...  Training Step: 956...  Training loss: 14158.8672...  6.7743 sec/batch\n",
      "Epoch: 5/20...  Training Step: 957...  Training loss: 14253.6426...  5.7402 sec/batch\n",
      "Epoch: 5/20...  Training Step: 958...  Training loss: 14194.7217...  5.6223 sec/batch\n",
      "Epoch: 5/20...  Training Step: 959...  Training loss: 14445.2539...  6.6626 sec/batch\n",
      "Epoch: 5/20...  Training Step: 960...  Training loss: 14866.8633...  5.6991 sec/batch\n",
      "Epoch: 5/20...  Training Step: 961...  Training loss: 14191.6729...  5.6967 sec/batch\n",
      "Epoch: 5/20...  Training Step: 962...  Training loss: 14245.4590...  6.7797 sec/batch\n",
      "Epoch: 5/20...  Training Step: 963...  Training loss: 14121.0117...  5.6113 sec/batch\n",
      "Epoch: 5/20...  Training Step: 964...  Training loss: 14053.7920...  6.7608 sec/batch\n",
      "Epoch: 5/20...  Training Step: 965...  Training loss: 14493.2285...  5.8750 sec/batch\n",
      "Epoch: 5/20...  Training Step: 966...  Training loss: 14225.7559...  5.6976 sec/batch\n",
      "Epoch: 5/20...  Training Step: 967...  Training loss: 14315.7646...  6.9078 sec/batch\n",
      "Epoch: 5/20...  Training Step: 968...  Training loss: 13967.5693...  5.6848 sec/batch\n",
      "Epoch: 5/20...  Training Step: 969...  Training loss: 13975.1250...  5.6670 sec/batch\n",
      "Epoch: 5/20...  Training Step: 970...  Training loss: 14515.1299...  6.8935 sec/batch\n",
      "Epoch: 5/20...  Training Step: 971...  Training loss: 13916.0996...  5.7444 sec/batch\n",
      "Epoch: 5/20...  Training Step: 972...  Training loss: 13879.0850...  5.7480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 973...  Training loss: 13878.3652...  6.8202 sec/batch\n",
      "Epoch: 5/20...  Training Step: 974...  Training loss: 14152.7910...  5.7672 sec/batch\n",
      "Epoch: 5/20...  Training Step: 975...  Training loss: 14233.7520...  6.7255 sec/batch\n",
      "Epoch: 5/20...  Training Step: 976...  Training loss: 14087.8252...  5.8888 sec/batch\n",
      "Epoch: 5/20...  Training Step: 977...  Training loss: 14165.8594...  5.7234 sec/batch\n",
      "Epoch: 5/20...  Training Step: 978...  Training loss: 14009.3867...  6.7498 sec/batch\n",
      "Epoch: 5/20...  Training Step: 979...  Training loss: 14345.5303...  5.8744 sec/batch\n",
      "Epoch: 5/20...  Training Step: 980...  Training loss: 14035.0195...  5.8629 sec/batch\n",
      "Epoch: 5/20...  Training Step: 981...  Training loss: 13988.7754...  6.7782 sec/batch\n",
      "Epoch: 5/20...  Training Step: 982...  Training loss: 14058.2490...  5.7543 sec/batch\n",
      "Epoch: 5/20...  Training Step: 983...  Training loss: 13950.1973...  5.7562 sec/batch\n",
      "Epoch: 5/20...  Training Step: 984...  Training loss: 13887.5586...  6.8708 sec/batch\n",
      "Epoch: 5/20...  Training Step: 985...  Training loss: 14158.1211...  5.7395 sec/batch\n",
      "Epoch: 5/20...  Training Step: 986...  Training loss: 13979.9053...  6.7279 sec/batch\n",
      "Epoch: 5/20...  Training Step: 987...  Training loss: 13794.2109...  5.8118 sec/batch\n",
      "Epoch: 5/20...  Training Step: 988...  Training loss: 14129.7852...  5.7738 sec/batch\n",
      "Epoch: 5/20...  Training Step: 989...  Training loss: 13978.9580...  6.9374 sec/batch\n",
      "Epoch: 5/20...  Training Step: 990...  Training loss: 13972.7930...  5.6500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 991...  Training loss: 15523.1787...  5.7485 sec/batch\n",
      "Epoch: 6/20...  Training Step: 992...  Training loss: 14348.1934...  6.7353 sec/batch\n",
      "Epoch: 6/20...  Training Step: 993...  Training loss: 14080.4512...  5.7235 sec/batch\n",
      "Epoch: 6/20...  Training Step: 994...  Training loss: 14295.7109...  6.1545 sec/batch\n",
      "Epoch: 6/20...  Training Step: 995...  Training loss: 13964.3564...  6.5073 sec/batch\n",
      "Epoch: 6/20...  Training Step: 996...  Training loss: 13814.5645...  5.8319 sec/batch\n",
      "Epoch: 6/20...  Training Step: 997...  Training loss: 14154.8486...  6.7298 sec/batch\n",
      "Epoch: 6/20...  Training Step: 998...  Training loss: 13966.2227...  5.6774 sec/batch\n",
      "Epoch: 6/20...  Training Step: 999...  Training loss: 14175.5967...  5.7312 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1000...  Training loss: 14108.5420...  6.7652 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1001...  Training loss: 13848.9658...  5.8300 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1002...  Training loss: 14018.5547...  5.7035 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1003...  Training loss: 14058.8574...  6.6971 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1004...  Training loss: 14274.8555...  5.7921 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1005...  Training loss: 13994.7510...  6.7706 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1006...  Training loss: 13739.5488...  5.9259 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1007...  Training loss: 14366.2588...  5.7179 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1008...  Training loss: 14406.7354...  6.8123 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1009...  Training loss: 14138.4336...  5.5958 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1010...  Training loss: 14367.8154...  5.6997 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1011...  Training loss: 14026.9561...  6.9366 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1012...  Training loss: 14278.2988...  5.6977 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1013...  Training loss: 13979.6816...  5.7680 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1014...  Training loss: 14178.7598...  6.8488 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1015...  Training loss: 14051.7910...  5.8950 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1016...  Training loss: 13660.2461...  6.7525 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1017...  Training loss: 13902.5244...  5.8772 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1018...  Training loss: 14252.5098...  5.8689 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1019...  Training loss: 14191.8438...  7.2792 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1020...  Training loss: 14323.2314...  5.7294 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1021...  Training loss: 14023.8223...  5.8040 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1022...  Training loss: 13848.8438...  6.8228 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1023...  Training loss: 14087.3145...  5.7624 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1024...  Training loss: 14234.0273...  6.3570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1025...  Training loss: 14068.6143...  6.1095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1026...  Training loss: 13999.9775...  5.7046 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1027...  Training loss: 13860.7324...  6.8334 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1028...  Training loss: 13700.4219...  5.7132 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1029...  Training loss: 13487.2285...  5.7028 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1030...  Training loss: 13692.0977...  6.8802 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1031...  Training loss: 13838.5557...  5.7153 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1032...  Training loss: 14274.4365...  5.7001 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1033...  Training loss: 13829.7979...  7.2242 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1034...  Training loss: 13798.8691...  5.7346 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1035...  Training loss: 13967.4961...  6.7097 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1036...  Training loss: 13577.4180...  6.0537 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1037...  Training loss: 14002.2207...  5.7702 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1038...  Training loss: 13985.4590...  6.7672 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1039...  Training loss: 13896.4473...  5.7535 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1040...  Training loss: 14118.5029...  5.7745 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1041...  Training loss: 13745.6436...  6.7356 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1042...  Training loss: 14462.6836...  6.3745 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1043...  Training loss: 14167.6543...  6.7210 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1044...  Training loss: 14206.3213...  6.8612 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1045...  Training loss: 14050.6719...  6.2085 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1046...  Training loss: 13968.6270...  7.3711 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1047...  Training loss: 14212.0664...  6.1509 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1048...  Training loss: 13748.2637...  6.1792 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1049...  Training loss: 13693.8467...  7.4556 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1050...  Training loss: 14365.6719...  6.1465 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1051...  Training loss: 14090.5820...  7.4734 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1052...  Training loss: 14500.1836...  6.4573 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1053...  Training loss: 14347.8789...  6.2944 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1054...  Training loss: 14204.9199...  7.6232 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1055...  Training loss: 13879.1816...  6.1863 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1056...  Training loss: 14121.4111...  7.5817 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1057...  Training loss: 14294.2559...  6.3218 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1058...  Training loss: 13835.9941...  6.2883 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1059...  Training loss: 13967.8750...  7.4804 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1060...  Training loss: 14039.6572...  6.3438 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1061...  Training loss: 14389.6758...  8.3046 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1062...  Training loss: 14196.6172...  7.0641 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1063...  Training loss: 14342.7109...  6.5710 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1064...  Training loss: 13905.5859...  7.6988 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1065...  Training loss: 14030.1680...  6.3953 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1066...  Training loss: 14151.8262...  7.6511 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1067...  Training loss: 13932.5225...  6.3873 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1068...  Training loss: 13921.2734...  6.5280 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1069...  Training loss: 13592.4316...  7.7781 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1070...  Training loss: 14135.5420...  9.2219 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1071...  Training loss: 13635.2881...  10.5355 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1072...  Training loss: 14037.9609...  6.7731 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1073...  Training loss: 13615.4287...  8.1069 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1074...  Training loss: 13948.2109...  6.9271 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1075...  Training loss: 13636.4688...  8.2869 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1076...  Training loss: 13890.7051...  6.9731 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1077...  Training loss: 13729.0400...  7.8692 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1078...  Training loss: 13829.7959...  7.0847 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1079...  Training loss: 13650.1191...  6.9033 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1080...  Training loss: 14011.9980...  8.2376 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1081...  Training loss: 13726.8164...  6.7231 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1082...  Training loss: 13778.2285...  8.1871 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1083...  Training loss: 13696.0879...  6.8422 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1084...  Training loss: 13823.0703...  8.1600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1085...  Training loss: 13671.4785...  6.8218 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1086...  Training loss: 14125.7871...  6.9148 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1087...  Training loss: 14014.7129...  8.2308 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1088...  Training loss: 13614.2275...  6.8607 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1089...  Training loss: 13720.3379...  8.1732 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1090...  Training loss: 13643.8359...  6.9201 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1091...  Training loss: 13994.4170...  8.3540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1092...  Training loss: 13793.0723...  8.7307 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1093...  Training loss: 13855.2793...  8.3197 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1094...  Training loss: 13791.7070...  6.8712 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1095...  Training loss: 13833.4385...  6.7661 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1096...  Training loss: 13852.9316...  8.1170 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1097...  Training loss: 13881.6289...  6.7754 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1098...  Training loss: 13924.9766...  8.2654 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1099...  Training loss: 13833.9932...  6.8497 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1100...  Training loss: 14033.3672...  8.1989 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1101...  Training loss: 13769.4092...  6.7413 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1102...  Training loss: 14016.7773...  7.3932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1103...  Training loss: 13879.1953...  7.8300 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1104...  Training loss: 13838.8799...  6.9757 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1105...  Training loss: 13607.4004...  8.0512 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1106...  Training loss: 13463.4688...  6.9252 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1107...  Training loss: 13939.5176...  8.1688 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1108...  Training loss: 13951.6875...  7.0128 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1109...  Training loss: 13774.3867...  8.1038 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1110...  Training loss: 13827.7939...  7.2756 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1111...  Training loss: 13914.5156...  7.0527 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1112...  Training loss: 13614.1963...  8.3069 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1113...  Training loss: 13430.4590...  6.7685 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1114...  Training loss: 13995.7148...  8.6135 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1115...  Training loss: 13846.9512...  7.4517 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1116...  Training loss: 13527.7480...  7.7396 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1117...  Training loss: 13980.3281...  6.4702 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1118...  Training loss: 13970.3633...  6.6010 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1119...  Training loss: 13711.2246...  7.5635 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1120...  Training loss: 13633.9404...  6.7283 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1121...  Training loss: 13393.6289...  7.4555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1122...  Training loss: 13627.4932...  6.4455 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1123...  Training loss: 14008.1211...  6.2866 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1124...  Training loss: 13849.6836...  7.4440 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1125...  Training loss: 13753.6562...  6.2795 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1126...  Training loss: 13750.1367...  7.6007 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1127...  Training loss: 14012.5605...  6.3384 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1128...  Training loss: 14061.1406...  6.3421 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1129...  Training loss: 13903.2959...  7.5832 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1130...  Training loss: 13881.9355...  6.1499 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1131...  Training loss: 14271.4785...  7.4628 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1132...  Training loss: 13913.7051...  6.3513 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1133...  Training loss: 13814.4980...  6.2344 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1134...  Training loss: 14063.6211...  7.5359 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1135...  Training loss: 13701.0078...  6.3247 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1136...  Training loss: 14113.5234...  7.5850 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1137...  Training loss: 13993.6201...  6.2620 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1138...  Training loss: 14128.8008...  6.8867 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1139...  Training loss: 13995.5332...  11.1784 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1140...  Training loss: 13817.7891...  9.8910 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1141...  Training loss: 13514.8838...  7.5601 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1142...  Training loss: 13718.1611...  6.4017 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1143...  Training loss: 13977.9902...  10.3885 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1144...  Training loss: 13685.8047...  6.8345 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1145...  Training loss: 13842.2217...  7.7681 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1146...  Training loss: 13831.7656...  7.1481 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1147...  Training loss: 13990.1035...  8.5445 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1148...  Training loss: 13823.9658...  8.0933 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1149...  Training loss: 13371.4453...  8.5512 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1150...  Training loss: 14035.1299...  6.2665 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1151...  Training loss: 14155.1514...  6.3433 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1152...  Training loss: 13910.0869...  8.6166 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1153...  Training loss: 13762.5586...  6.8431 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1154...  Training loss: 13817.4248...  7.7152 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1155...  Training loss: 13819.6494...  6.4046 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1156...  Training loss: 13911.5410...  7.6310 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1157...  Training loss: 14024.2402...  6.9185 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1158...  Training loss: 14561.8281...  6.3442 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1159...  Training loss: 14008.6191...  8.7287 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1160...  Training loss: 13889.3730...  8.7611 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1161...  Training loss: 13693.7715...  7.8211 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1162...  Training loss: 13640.0439...  5.8747 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1163...  Training loss: 14237.7705...  6.3462 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1164...  Training loss: 13918.9053...  6.4797 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1165...  Training loss: 13848.9932...  5.9165 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1166...  Training loss: 13636.2773...  6.9876 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1167...  Training loss: 13715.4424...  6.8362 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1168...  Training loss: 14081.9268...  7.3515 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1169...  Training loss: 13641.3564...  7.5763 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1170...  Training loss: 13459.9863...  6.5584 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1171...  Training loss: 13626.8613...  6.4565 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1172...  Training loss: 13888.5156...  7.8881 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1173...  Training loss: 13972.5635...  7.1829 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1174...  Training loss: 13765.5391...  6.1409 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1175...  Training loss: 13837.1582...  9.9772 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1176...  Training loss: 13683.7549...  10.4335 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1177...  Training loss: 14049.0273...  9.7510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1178...  Training loss: 13684.7383...  6.3575 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1179...  Training loss: 13719.2305...  8.2407 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1180...  Training loss: 13633.8613...  6.1247 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1181...  Training loss: 13510.6367...  5.9790 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1182...  Training loss: 13566.1045...  7.0534 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1183...  Training loss: 13795.2930...  5.7720 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1184...  Training loss: 13749.2178...  6.9313 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1185...  Training loss: 13385.1152...  6.2252 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1186...  Training loss: 13832.9668...  5.7886 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1187...  Training loss: 13664.8564...  6.9455 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1188...  Training loss: 13639.4199...  5.9838 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1189...  Training loss: 15472.1533...  5.8992 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1190...  Training loss: 14005.7354...  7.1004 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1191...  Training loss: 13730.5859...  6.0101 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1192...  Training loss: 14077.3145...  6.0101 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1193...  Training loss: 13639.5557...  8.6552 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1194...  Training loss: 13395.8750...  9.2068 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1195...  Training loss: 13718.5215...  9.0967 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1196...  Training loss: 13716.1289...  6.3166 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1197...  Training loss: 13855.2754...  6.4612 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1198...  Training loss: 13679.7207...  7.4347 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1199...  Training loss: 13622.8145...  6.5133 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1200...  Training loss: 13561.9531...  7.5853 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1201...  Training loss: 13688.9873...  6.3357 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1202...  Training loss: 13965.8066...  6.5693 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1203...  Training loss: 13565.3203...  7.0068 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1204...  Training loss: 13496.7109...  6.2852 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1205...  Training loss: 13900.7686...  7.4456 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1206...  Training loss: 13938.5977...  6.2832 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1207...  Training loss: 13762.0020...  6.1397 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1208...  Training loss: 14067.2852...  7.4911 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1209...  Training loss: 13742.0840...  6.3471 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1210...  Training loss: 14016.5000...  7.5810 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1211...  Training loss: 13709.6602...  6.2055 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1212...  Training loss: 13791.4492...  6.3047 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1213...  Training loss: 13717.6738...  7.5527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1214...  Training loss: 13249.9316...  6.3789 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1215...  Training loss: 13608.4814...  7.5114 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1216...  Training loss: 14020.1885...  6.2829 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1217...  Training loss: 13848.4219...  6.2887 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1218...  Training loss: 13964.3711...  7.5199 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1219...  Training loss: 13673.6279...  6.2871 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1220...  Training loss: 13580.4229...  7.5614 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1221...  Training loss: 13835.5703...  6.2853 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1222...  Training loss: 13850.2500...  6.3226 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1223...  Training loss: 13678.4746...  7.6220 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1224...  Training loss: 13656.9473...  6.3316 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1225...  Training loss: 13485.8691...  7.6047 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1226...  Training loss: 13368.3789...  6.2544 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1227...  Training loss: 13242.1055...  6.2775 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1228...  Training loss: 13569.2148...  7.4310 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1229...  Training loss: 13563.7051...  6.3248 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1230...  Training loss: 13979.7715...  7.5383 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1231...  Training loss: 13596.6133...  6.3420 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1232...  Training loss: 13425.8887...  6.3523 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1233...  Training loss: 13766.9873...  7.5636 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1234...  Training loss: 13343.0703...  6.5681 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1235...  Training loss: 13734.4688...  7.4860 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1236...  Training loss: 13604.8057...  6.4655 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1237...  Training loss: 13689.8848...  6.3277 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1238...  Training loss: 13896.2461...  7.4640 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1239...  Training loss: 13471.9355...  6.2751 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1240...  Training loss: 14147.7529...  7.5949 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1241...  Training loss: 13778.6035...  6.3076 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1242...  Training loss: 13778.3799...  6.2398 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1243...  Training loss: 13638.4023...  7.5572 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1244...  Training loss: 13601.4053...  6.2877 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1245...  Training loss: 13834.5117...  7.5817 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1246...  Training loss: 13523.3379...  6.3196 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1247...  Training loss: 13414.8154...  6.2566 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1248...  Training loss: 14005.6973...  7.5193 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1249...  Training loss: 13785.8643...  6.3288 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1250...  Training loss: 14205.8125...  7.6388 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1251...  Training loss: 13955.8398...  6.1337 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1252...  Training loss: 13772.6113...  6.2317 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1253...  Training loss: 13581.0088...  7.4528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1254...  Training loss: 13807.0645...  6.4870 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1255...  Training loss: 13861.6152...  7.4873 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1256...  Training loss: 13406.8350...  6.3713 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1257...  Training loss: 13613.3047...  6.3819 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1258...  Training loss: 13590.3545...  7.4700 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1259...  Training loss: 14084.8662...  6.2303 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1260...  Training loss: 13829.3945...  7.2724 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1261...  Training loss: 13954.4180...  6.3644 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1262...  Training loss: 13553.9268...  6.2360 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1263...  Training loss: 13689.5410...  7.5129 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1264...  Training loss: 14025.4229...  6.1191 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1265...  Training loss: 13559.2559...  7.0321 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1266...  Training loss: 13626.0723...  6.5728 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1267...  Training loss: 13235.1289...  6.2649 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1268...  Training loss: 13794.4473...  7.6072 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1269...  Training loss: 13285.4023...  8.4810 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1270...  Training loss: 13787.4678...  9.6631 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1271...  Training loss: 13412.3867...  6.3817 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1272...  Training loss: 13621.4736...  6.6847 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1273...  Training loss: 13364.9658...  7.1662 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1274...  Training loss: 13599.1504...  6.3004 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1275...  Training loss: 13482.4531...  7.5257 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1276...  Training loss: 13388.3047...  6.2601 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1277...  Training loss: 13356.6963...  6.8046 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1278...  Training loss: 13747.5830...  7.1748 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1279...  Training loss: 13505.0176...  6.3796 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1280...  Training loss: 13555.0117...  8.0088 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1281...  Training loss: 13403.8203...  6.1835 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1282...  Training loss: 13445.7090...  6.5873 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1283...  Training loss: 13394.7773...  7.3953 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1284...  Training loss: 13736.3447...  6.4523 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1285...  Training loss: 13630.9629...  7.4887 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1286...  Training loss: 13339.8799...  6.1748 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1287...  Training loss: 13493.4736...  6.8101 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1288...  Training loss: 13406.8926...  7.1449 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1289...  Training loss: 13692.8105...  6.3736 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1290...  Training loss: 13510.0137...  7.8806 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1291...  Training loss: 13563.8857...  6.2230 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1292...  Training loss: 13433.4531...  6.6982 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1293...  Training loss: 13583.6914...  7.1391 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1294...  Training loss: 13501.9844...  6.3393 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1295...  Training loss: 13641.4004...  7.4960 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1296...  Training loss: 13561.6064...  9.1225 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1297...  Training loss: 13510.6875...  8.1966 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1298...  Training loss: 13863.6562...  6.2808 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1299...  Training loss: 13456.8887...  6.6453 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1300...  Training loss: 13716.5420...  7.1613 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1301...  Training loss: 13527.3320...  6.3837 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1302...  Training loss: 13573.9004...  7.6070 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1303...  Training loss: 13368.5332...  6.6015 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1304...  Training loss: 13171.6855...  7.9569 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1305...  Training loss: 13690.3008...  8.0080 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1306...  Training loss: 13664.4102...  6.4807 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1307...  Training loss: 13683.1768...  7.7067 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1308...  Training loss: 13547.4297...  6.3080 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1309...  Training loss: 13534.6016...  7.6286 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1310...  Training loss: 13261.3242...  6.4184 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1311...  Training loss: 13084.4189...  6.2790 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1312...  Training loss: 13647.7695...  7.7991 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1313...  Training loss: 13604.0859...  6.2622 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1314...  Training loss: 13017.6875...  7.6519 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1315...  Training loss: 13666.0459...  6.2194 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1316...  Training loss: 13674.5898...  6.3126 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1317...  Training loss: 13408.7480...  7.5097 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1318...  Training loss: 13248.1484...  6.3620 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1319...  Training loss: 13116.0830...  7.6905 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1320...  Training loss: 13322.2246...  6.5704 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1321...  Training loss: 13773.8516...  6.4122 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1322...  Training loss: 13553.0176...  7.8471 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1323...  Training loss: 13598.7871...  6.4079 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1324...  Training loss: 13433.8809...  7.5715 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1325...  Training loss: 13792.9541...  6.4075 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1326...  Training loss: 13762.9746...  6.3246 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1327...  Training loss: 13588.7148...  7.7176 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1328...  Training loss: 13575.9395...  6.4070 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1329...  Training loss: 13947.6934...  7.5466 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1330...  Training loss: 13573.6885...  6.3993 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1331...  Training loss: 13406.8086...  6.2447 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1332...  Training loss: 13741.8145...  7.6726 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1333...  Training loss: 13454.2178...  6.9918 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1334...  Training loss: 13793.8398...  8.4135 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1335...  Training loss: 13635.2510...  8.0269 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1336...  Training loss: 13874.7676...  9.6330 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1337...  Training loss: 13771.0137...  7.2246 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1338...  Training loss: 13536.9766...  8.0635 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1339...  Training loss: 13305.9902...  7.7387 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1340...  Training loss: 13490.5430...  8.5933 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1341...  Training loss: 13614.1943...  7.1082 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1342...  Training loss: 13477.4863...  6.6526 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1343...  Training loss: 13429.8398...  7.6568 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1344...  Training loss: 13530.3232...  6.2972 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1345...  Training loss: 13604.3584...  9.1932 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1346...  Training loss: 13416.0312...  5.9899 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1347...  Training loss: 13166.1250...  6.6497 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1348...  Training loss: 13641.9092...  7.5068 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1349...  Training loss: 13871.3467...  6.3459 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1350...  Training loss: 13583.5889...  7.7303 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1351...  Training loss: 13541.7080...  6.2702 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1352...  Training loss: 13499.8613...  6.3487 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1353...  Training loss: 13625.5645...  7.4366 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1354...  Training loss: 13527.5078...  6.2964 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1355...  Training loss: 13865.0273...  7.5771 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1356...  Training loss: 14232.2500...  6.4024 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1357...  Training loss: 13646.2285...  6.3450 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1358...  Training loss: 13663.2559...  7.5915 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1359...  Training loss: 13417.2666...  6.2364 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1360...  Training loss: 13350.2812...  7.5498 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1361...  Training loss: 13934.9189...  6.1704 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1362...  Training loss: 13633.8721...  6.2228 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1363...  Training loss: 13635.6348...  7.4986 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1364...  Training loss: 13354.3223...  6.3390 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1365...  Training loss: 13404.5195...  7.5175 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1366...  Training loss: 13884.1426...  6.2060 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1367...  Training loss: 13362.2910...  6.2199 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1368...  Training loss: 13185.9707...  7.3584 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1369...  Training loss: 13354.4980...  6.3145 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1370...  Training loss: 13586.6387...  7.4280 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1371...  Training loss: 13652.6152...  6.2625 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1372...  Training loss: 13434.7949...  6.3415 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1373...  Training loss: 13548.9102...  7.6367 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1374...  Training loss: 13298.2422...  7.2049 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1375...  Training loss: 13677.3506...  7.4268 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1376...  Training loss: 13436.0820...  6.3614 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1377...  Training loss: 13473.1562...  7.8574 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1378...  Training loss: 13473.2178...  8.3852 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1379...  Training loss: 13262.9062...  6.2836 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1380...  Training loss: 13266.3057...  7.4786 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1381...  Training loss: 13563.2480...  6.2798 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1382...  Training loss: 13376.3857...  6.6335 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1383...  Training loss: 13256.6953...  7.3480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1384...  Training loss: 13624.3457...  6.2820 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1385...  Training loss: 13458.8887...  7.5020 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1386...  Training loss: 13423.1318...  6.2427 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1387...  Training loss: 15048.3506...  6.2108 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1388...  Training loss: 13752.7275...  7.3449 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1389...  Training loss: 13547.0898...  6.3151 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1390...  Training loss: 13721.0322...  7.4757 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1391...  Training loss: 13294.8730...  6.3280 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1392...  Training loss: 13146.3438...  6.3154 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1393...  Training loss: 13410.6973...  7.5870 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1394...  Training loss: 13405.9316...  6.2094 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1395...  Training loss: 13582.9551...  7.4912 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1396...  Training loss: 13420.9365...  6.2002 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1397...  Training loss: 13261.3750...  6.3382 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1398...  Training loss: 13424.6191...  7.5850 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1399...  Training loss: 13449.4961...  6.3461 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1400...  Training loss: 13732.2148...  7.5779 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1401...  Training loss: 13252.3496...  6.2170 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1402...  Training loss: 13209.7441...  6.2600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1403...  Training loss: 13562.2246...  7.5020 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1404...  Training loss: 13739.5771...  6.3876 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1405...  Training loss: 13401.2803...  7.5503 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1406...  Training loss: 13745.4062...  6.2291 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1407...  Training loss: 13393.4229...  6.3298 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1408...  Training loss: 13647.7490...  7.5669 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1409...  Training loss: 13449.5186...  6.6163 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1410...  Training loss: 13618.9160...  7.4623 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1411...  Training loss: 13368.8457...  6.3022 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1412...  Training loss: 13069.6934...  6.8477 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1413...  Training loss: 13226.6260...  8.8972 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1414...  Training loss: 13697.1416...  7.7270 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1415...  Training loss: 13576.6484...  8.8681 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1416...  Training loss: 13636.8184...  8.0365 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1417...  Training loss: 13420.3887...  10.5907 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1418...  Training loss: 13353.1602...  8.2476 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1419...  Training loss: 13423.4043...  7.5947 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1420...  Training loss: 13620.5117...  6.3183 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1421...  Training loss: 13402.4941...  7.4426 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1422...  Training loss: 13498.5938...  6.1345 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1423...  Training loss: 13240.4316...  6.1775 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1424...  Training loss: 13109.4160...  7.9509 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1425...  Training loss: 12964.4492...  6.2891 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1426...  Training loss: 13207.9199...  7.5319 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1427...  Training loss: 13266.1445...  6.3271 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1428...  Training loss: 13730.8281...  6.3100 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1429...  Training loss: 13308.2422...  7.4945 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1430...  Training loss: 13195.1406...  6.2101 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1431...  Training loss: 13398.9648...  7.4719 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1432...  Training loss: 13122.3984...  6.3103 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1433...  Training loss: 13393.1152...  6.4348 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1434...  Training loss: 13359.3965...  7.5149 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1435...  Training loss: 13413.9004...  6.3668 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1436...  Training loss: 13632.1113...  7.5992 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1437...  Training loss: 13143.8193...  6.2844 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1438...  Training loss: 13865.1133...  6.2721 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1439...  Training loss: 13577.0342...  7.4808 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1440...  Training loss: 13497.6670...  6.3126 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1441...  Training loss: 13388.9922...  7.5375 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1442...  Training loss: 13399.5713...  6.2799 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1443...  Training loss: 13610.4082...  6.3005 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1444...  Training loss: 13415.1406...  7.4452 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1445...  Training loss: 13203.4863...  6.2116 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1446...  Training loss: 13756.5293...  7.5292 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1447...  Training loss: 13523.7793...  6.4601 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1448...  Training loss: 13920.0801...  6.2626 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1449...  Training loss: 13699.9180...  7.4042 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1450...  Training loss: 13505.7988...  6.2681 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1451...  Training loss: 13277.0713...  7.1421 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1452...  Training loss: 13519.9951...  6.6736 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1453...  Training loss: 13613.2773...  7.2506 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1454...  Training loss: 13187.6875...  8.5548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1455...  Training loss: 13428.6445...  6.9786 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1456...  Training loss: 13315.5967...  7.3947 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1457...  Training loss: 13827.6230...  7.1275 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1458...  Training loss: 13529.4336...  8.4591 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1459...  Training loss: 13820.4434...  6.3973 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1460...  Training loss: 13264.7715...  6.9610 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1461...  Training loss: 13480.2266...  9.6006 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1462...  Training loss: 13656.5176...  7.3165 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1463...  Training loss: 13383.3086...  7.5042 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1464...  Training loss: 13214.3291...  9.3917 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1465...  Training loss: 13019.5781...  11.0848 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1466...  Training loss: 13578.0400...  9.6489 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1467...  Training loss: 12973.1738...  8.9065 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1468...  Training loss: 13420.2363...  7.9152 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1469...  Training loss: 13194.3271...  10.9484 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1470...  Training loss: 13352.2793...  10.8010 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1471...  Training loss: 13145.0781...  9.2340 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1472...  Training loss: 13356.0029...  13.3842 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1473...  Training loss: 13176.4023...  10.9548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1474...  Training loss: 13127.8516...  10.6679 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1475...  Training loss: 13044.5137...  13.7168 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1476...  Training loss: 13461.1895...  8.4480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1477...  Training loss: 13288.9346...  10.2511 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1478...  Training loss: 13255.7168...  6.8916 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1479...  Training loss: 13028.7803...  7.0224 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1480...  Training loss: 13095.6621...  7.9493 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1481...  Training loss: 13114.3457...  6.3893 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1482...  Training loss: 13516.4023...  7.9969 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1483...  Training loss: 13465.1074...  9.3996 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1484...  Training loss: 13065.0195...  8.7895 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1485...  Training loss: 13187.0820...  6.8681 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1486...  Training loss: 13120.8848...  8.8587 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1487...  Training loss: 13398.5996...  7.2133 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1488...  Training loss: 13231.1523...  10.4245 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1489...  Training loss: 13331.1797...  7.1186 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1490...  Training loss: 13360.6016...  6.7304 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1491...  Training loss: 13418.4375...  8.1584 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1492...  Training loss: 13234.1289...  6.5414 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1493...  Training loss: 13309.1709...  8.2687 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1494...  Training loss: 13421.3867...  7.2383 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1495...  Training loss: 13326.4199...  9.3528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1496...  Training loss: 13524.5312...  6.4923 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1497...  Training loss: 13305.0430...  6.3200 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1498...  Training loss: 13402.6807...  7.3053 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1499...  Training loss: 13452.0889...  6.1098 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1500...  Training loss: 13393.7188...  6.7946 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1501...  Training loss: 13129.9238...  6.3092 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1502...  Training loss: 12983.6475...  5.8590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1503...  Training loss: 13410.4785...  6.9733 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1504...  Training loss: 13496.4443...  6.0355 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1505...  Training loss: 13342.4297...  6.0073 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1506...  Training loss: 13341.9619...  7.0037 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1507...  Training loss: 13326.5215...  5.8503 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1508...  Training loss: 13074.1191...  5.8407 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1509...  Training loss: 12842.5801...  7.4542 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1510...  Training loss: 13407.9043...  7.3615 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1511...  Training loss: 13275.1201...  7.7848 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1512...  Training loss: 12838.5801...  8.3056 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1513...  Training loss: 13356.9199...  6.2616 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1514...  Training loss: 13364.3438...  7.3680 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1515...  Training loss: 13126.5898...  10.3899 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1516...  Training loss: 12892.4980...  9.6402 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1517...  Training loss: 12849.1475...  10.5083 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1518...  Training loss: 13104.7676...  11.0751 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1519...  Training loss: 13575.2637...  10.9631 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1520...  Training loss: 13429.8174...  7.7185 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1521...  Training loss: 13329.9229...  8.8805 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1522...  Training loss: 13296.5312...  7.6462 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1523...  Training loss: 13573.8984...  8.9709 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1524...  Training loss: 13484.3809...  7.6204 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1525...  Training loss: 13362.4668...  8.7805 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1526...  Training loss: 13382.8623...  7.4684 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1527...  Training loss: 13752.7832...  8.7169 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1528...  Training loss: 13407.6680...  11.6067 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1529...  Training loss: 13270.7891...  15.3273 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1530...  Training loss: 13487.0049...  18.1323 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1531...  Training loss: 13163.9912...  15.8451 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1532...  Training loss: 13577.5723...  14.8845 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1533...  Training loss: 13468.3105...  15.6918 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1534...  Training loss: 13672.2383...  15.3510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1535...  Training loss: 13565.4297...  15.4621 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1536...  Training loss: 13353.3584...  15.5615 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1537...  Training loss: 13052.7900...  14.1214 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1538...  Training loss: 13184.4502...  15.4215 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1539...  Training loss: 13524.7832...  15.3798 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1540...  Training loss: 13209.8379...  15.5831 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1541...  Training loss: 13220.1914...  19.6960 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1542...  Training loss: 13191.2051...  19.9256 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1543...  Training loss: 13411.1426...  17.3805 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1544...  Training loss: 13261.7676...  14.4829 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1545...  Training loss: 12930.8828...  18.4820 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1546...  Training loss: 13378.5605...  15.4225 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1547...  Training loss: 13602.5918...  15.7525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1548...  Training loss: 13397.2256...  15.3264 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1549...  Training loss: 13254.8828...  14.0520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1550...  Training loss: 13318.3379...  14.9573 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1551...  Training loss: 13363.6445...  15.4692 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1552...  Training loss: 13359.6807...  15.3803 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1553...  Training loss: 13569.0352...  14.1910 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1554...  Training loss: 14020.9863...  15.1584 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1555...  Training loss: 13510.7363...  15.1230 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1556...  Training loss: 13376.3730...  16.3315 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1557...  Training loss: 13142.5947...  15.3211 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1558...  Training loss: 13145.9785...  14.8108 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1559...  Training loss: 13733.2100...  16.4302 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1560...  Training loss: 13355.4287...  16.4423 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1561...  Training loss: 13419.8574...  18.6652 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1562...  Training loss: 13076.5156...  15.9843 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1563...  Training loss: 13169.9746...  13.9111 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1564...  Training loss: 13588.5049...  18.7173 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1565...  Training loss: 13147.7148...  17.9713 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1566...  Training loss: 13071.3418...  15.3111 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1567...  Training loss: 13095.3916...  15.7011 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1568...  Training loss: 13362.9229...  14.7550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1569...  Training loss: 13501.9395...  16.1183 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1570...  Training loss: 13291.0840...  18.1604 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1571...  Training loss: 13304.4258...  16.3948 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1572...  Training loss: 13104.1758...  18.8830 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1573...  Training loss: 13499.5176...  18.6245 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1574...  Training loss: 13171.9600...  19.3326 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1575...  Training loss: 13253.8848...  17.6486 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1576...  Training loss: 13280.1816...  17.3926 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1577...  Training loss: 13131.5820...  18.6480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1578...  Training loss: 12988.2559...  19.7535 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1579...  Training loss: 13307.6816...  20.2268 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1580...  Training loss: 13227.8799...  16.3803 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1581...  Training loss: 12892.1904...  16.2030 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1582...  Training loss: 13252.9707...  14.2818 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1583...  Training loss: 13210.0469...  13.5057 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1584...  Training loss: 13154.3457...  13.8612 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1585...  Training loss: 14860.7773...  15.5560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1586...  Training loss: 13510.2910...  15.3831 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1587...  Training loss: 13303.6338...  15.3908 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1588...  Training loss: 13426.6348...  14.8947 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1589...  Training loss: 13093.4980...  16.0133 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1590...  Training loss: 12962.5635...  14.8821 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1591...  Training loss: 13243.8213...  14.5900 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1592...  Training loss: 13193.2773...  13.6269 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1593...  Training loss: 13337.3506...  13.0824 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1594...  Training loss: 13138.6514...  14.1345 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1595...  Training loss: 13045.9473...  16.9590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1596...  Training loss: 13164.9854...  17.7334 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1597...  Training loss: 13193.6816...  14.2128 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1598...  Training loss: 13511.7539...  13.4895 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1599...  Training loss: 13078.0195...  15.1022 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1600...  Training loss: 13009.3125...  15.4344 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1601...  Training loss: 13319.9727...  15.7772 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1602...  Training loss: 13430.3047...  15.0279 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1603...  Training loss: 13277.8008...  14.9828 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1604...  Training loss: 13528.4229...  13.6379 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1605...  Training loss: 13123.1504...  13.4624 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1606...  Training loss: 13297.6230...  13.1278 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1607...  Training loss: 13198.6426...  13.8737 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1608...  Training loss: 13415.5410...  14.6544 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1609...  Training loss: 13225.3213...  14.2974 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1610...  Training loss: 12769.4102...  13.1375 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1611...  Training loss: 13035.6777...  13.9506 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1612...  Training loss: 13472.6699...  13.8109 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1613...  Training loss: 13418.4336...  12.7623 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1614...  Training loss: 13484.2031...  14.7663 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1615...  Training loss: 13220.4648...  14.6155 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1616...  Training loss: 13117.1387...  13.6579 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1617...  Training loss: 13324.0039...  12.8784 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1618...  Training loss: 13354.0977...  13.7145 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1619...  Training loss: 13272.3896...  13.7627 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1620...  Training loss: 13207.8008...  14.9013 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1621...  Training loss: 13046.1699...  13.7798 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1622...  Training loss: 12819.7852...  13.8392 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1623...  Training loss: 12773.3555...  13.7301 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1624...  Training loss: 13053.5078...  13.5123 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1625...  Training loss: 12963.4609...  13.4391 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1626...  Training loss: 13586.7832...  15.0161 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1627...  Training loss: 13028.5273...  16.1107 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1628...  Training loss: 13007.9668...  15.6058 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1629...  Training loss: 13313.2402...  13.0010 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1630...  Training loss: 12890.2520...  15.4276 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1631...  Training loss: 13128.3594...  19.3249 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1632...  Training loss: 13203.5859...  20.2098 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1633...  Training loss: 13094.7178...  19.2503 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1634...  Training loss: 13320.6504...  19.3946 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1635...  Training loss: 12989.7910...  18.9963 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1636...  Training loss: 13596.3906...  19.8960 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1637...  Training loss: 13261.7988...  22.7262 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1638...  Training loss: 13358.0898...  18.5491 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1652...  Training loss: 13086.2139...  16.7267 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1653...  Training loss: 13136.9043...  17.8473 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1654...  Training loss: 13020.9639...  19.6090 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1655...  Training loss: 13671.0234...  18.2667 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1656...  Training loss: 13378.6611...  15.9042 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1667...  Training loss: 12947.3867...  7.2648 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1668...  Training loss: 13175.3965...  8.5084 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1669...  Training loss: 12947.1191...  6.9936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1670...  Training loss: 13165.3838...  10.2038 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1671...  Training loss: 12933.4053...  7.5402 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1672...  Training loss: 12912.4883...  8.6356 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1673...  Training loss: 12802.4600...  7.4173 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1674...  Training loss: 13186.0332...  8.6263 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1675...  Training loss: 12907.3945...  7.1554 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1676...  Training loss: 13024.0605...  8.5288 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1677...  Training loss: 12866.7217...  8.3461 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1678...  Training loss: 12849.1562...  10.2388 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1679...  Training loss: 12935.1875...  9.3107 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1680...  Training loss: 13287.4238...  8.4698 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1681...  Training loss: 13203.0264...  6.9223 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1682...  Training loss: 12825.2402...  7.4117 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1683...  Training loss: 12914.2207...  7.7360 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1684...  Training loss: 12901.7070...  6.9040 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1685...  Training loss: 13221.3760...  8.2714 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1686...  Training loss: 13030.3955...  6.9456 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1687...  Training loss: 13079.5498...  9.7917 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1688...  Training loss: 13065.6895...  6.9787 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1689...  Training loss: 13129.9414...  8.3072 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1690...  Training loss: 13028.4941...  8.6602 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1691...  Training loss: 13188.5977...  9.2560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1692...  Training loss: 13221.8379...  6.8215 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1693...  Training loss: 13159.6455...  7.4424 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1694...  Training loss: 13418.9961...  7.5500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1695...  Training loss: 13061.2695...  6.9208 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1696...  Training loss: 13361.7881...  8.1664 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1697...  Training loss: 13142.4121...  6.8853 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1698...  Training loss: 13177.6367...  8.2639 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1699...  Training loss: 12924.3691...  6.9171 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1700...  Training loss: 12777.6299...  7.7183 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1701...  Training loss: 13205.6221...  7.4327 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1702...  Training loss: 13241.9434...  10.8981 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1703...  Training loss: 13099.6484...  8.6128 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1704...  Training loss: 13060.7109...  9.3208 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1705...  Training loss: 13034.8027...  8.5811 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1706...  Training loss: 12835.6748...  6.3242 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1707...  Training loss: 12621.7822...  7.6452 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1708...  Training loss: 13192.7012...  6.3919 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1709...  Training loss: 13047.3447...  7.7572 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1710...  Training loss: 12712.7480...  6.4198 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1711...  Training loss: 13105.6846...  6.3894 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1712...  Training loss: 13109.1543...  7.6598 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1713...  Training loss: 12852.2480...  6.3285 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1714...  Training loss: 12711.7988...  7.5425 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1715...  Training loss: 12628.9932...  6.4966 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1716...  Training loss: 12928.7617...  6.4250 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1717...  Training loss: 13319.0547...  7.7260 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1718...  Training loss: 13183.0371...  6.3715 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1719...  Training loss: 13233.8740...  7.5626 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1720...  Training loss: 13157.1289...  6.2290 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1721...  Training loss: 13351.9434...  6.2952 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1722...  Training loss: 13341.6611...  7.6634 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1723...  Training loss: 13152.1582...  6.3547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1724...  Training loss: 13141.8447...  7.6405 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1725...  Training loss: 13523.9639...  6.3013 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1726...  Training loss: 13239.5625...  6.3912 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1727...  Training loss: 13046.1064...  7.4830 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1728...  Training loss: 13363.8125...  6.4547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1729...  Training loss: 12913.3604...  7.5385 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1730...  Training loss: 13354.9062...  6.4611 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1731...  Training loss: 13085.4004...  6.3785 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1732...  Training loss: 13425.0859...  7.6392 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1733...  Training loss: 13421.2148...  6.4419 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1734...  Training loss: 13196.1201...  7.6905 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1735...  Training loss: 12848.8809...  6.4481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1736...  Training loss: 12912.3086...  6.5584 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1737...  Training loss: 13231.4492...  7.5145 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1738...  Training loss: 12938.8535...  6.5314 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1739...  Training loss: 12991.1230...  7.6442 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1740...  Training loss: 13073.6377...  6.4824 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1741...  Training loss: 13219.5059...  7.2821 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1742...  Training loss: 13025.6504...  7.3135 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1743...  Training loss: 12736.0117...  6.4344 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1744...  Training loss: 13259.8457...  7.5411 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1745...  Training loss: 13367.5420...  6.2385 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1746...  Training loss: 13072.1230...  7.5523 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1747...  Training loss: 13001.6768...  6.5470 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1748...  Training loss: 13053.6426...  6.4864 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1749...  Training loss: 13186.7637...  7.4467 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1750...  Training loss: 13142.5908...  6.3111 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1751...  Training loss: 13350.6377...  7.4802 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1752...  Training loss: 13793.0352...  6.5805 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1753...  Training loss: 13288.1367...  6.3651 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1754...  Training loss: 13108.2676...  7.8693 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1755...  Training loss: 12973.3154...  7.1534 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1756...  Training loss: 12939.3555...  7.5148 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1757...  Training loss: 13463.3271...  7.2331 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1758...  Training loss: 13209.6650...  5.7555 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1759...  Training loss: 13254.9336...  6.9436 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1760...  Training loss: 12866.6689...  5.7586 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1761...  Training loss: 13041.9209...  6.8736 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1762...  Training loss: 13378.1875...  5.6678 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1763...  Training loss: 12905.6123...  5.7807 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1764...  Training loss: 12818.6357...  6.7617 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1765...  Training loss: 12956.4258...  5.7451 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1766...  Training loss: 13179.4756...  5.7702 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1767...  Training loss: 13096.5820...  6.7962 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1768...  Training loss: 12997.0254...  5.8502 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1769...  Training loss: 13032.3115...  5.9325 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1770...  Training loss: 12931.0352...  6.7363 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1771...  Training loss: 13305.1729...  5.6987 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1772...  Training loss: 13042.5283...  6.7709 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1773...  Training loss: 13012.7119...  5.7476 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1774...  Training loss: 13021.0566...  5.7757 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1775...  Training loss: 12845.9355...  6.8753 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1776...  Training loss: 12888.0156...  5.8075 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1777...  Training loss: 13064.8867...  5.6887 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1778...  Training loss: 12856.0801...  6.8141 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1779...  Training loss: 12656.2383...  5.6942 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1780...  Training loss: 13086.4844...  6.2014 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1781...  Training loss: 12972.5703...  6.4660 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1782...  Training loss: 12927.5938...  5.7610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1783...  Training loss: 14723.1787...  6.8196 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1784...  Training loss: 13433.7646...  5.7269 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1785...  Training loss: 13081.4824...  5.7914 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1786...  Training loss: 13288.8105...  8.5398 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1787...  Training loss: 12959.0088...  7.2459 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1788...  Training loss: 12724.3545...  8.3094 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1789...  Training loss: 13131.4629...  6.2530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1790...  Training loss: 13008.7168...  5.8396 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1791...  Training loss: 13085.3926...  6.7461 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1792...  Training loss: 12984.7051...  5.6969 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1793...  Training loss: 12961.8398...  6.1981 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1794...  Training loss: 12911.5352...  6.1556 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1795...  Training loss: 13029.3604...  5.7583 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1796...  Training loss: 13154.2344...  6.4092 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1797...  Training loss: 12757.0645...  6.0696 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1798...  Training loss: 12777.4746...  6.5479 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1799...  Training loss: 13168.9492...  7.5858 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1800...  Training loss: 13206.9766...  5.7543 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1801...  Training loss: 13016.3604...  5.7373 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1802...  Training loss: 13377.9834...  7.4635 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1803...  Training loss: 13028.8516...  6.5883 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1804...  Training loss: 13162.7715...  6.7791 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1805...  Training loss: 13053.5566...  5.6040 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1806...  Training loss: 13089.2129...  6.1211 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1807...  Training loss: 13108.1934...  7.9498 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1808...  Training loss: 12575.3369...  6.0806 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1809...  Training loss: 12857.4004...  6.7330 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1810...  Training loss: 13277.5967...  5.8329 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1811...  Training loss: 13129.9971...  6.6447 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1812...  Training loss: 13422.9355...  7.3202 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1813...  Training loss: 13017.4912...  5.7462 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1814...  Training loss: 12933.3848...  5.6298 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1815...  Training loss: 13125.7842...  7.6072 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1816...  Training loss: 13131.2949...  6.5274 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1817...  Training loss: 13034.7939...  6.8424 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1818...  Training loss: 13080.2412...  5.6475 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1819...  Training loss: 12943.2539...  6.2676 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1820...  Training loss: 12626.0996...  7.9456 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1821...  Training loss: 12545.6191...  6.4970 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1822...  Training loss: 12845.0166...  6.9604 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1823...  Training loss: 12817.9160...  5.6695 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1824...  Training loss: 13439.7793...  5.8903 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1825...  Training loss: 12887.6270...  7.1414 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1826...  Training loss: 12730.4648...  5.6917 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1827...  Training loss: 13077.2529...  5.6688 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1828...  Training loss: 12670.8633...  6.6610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1829...  Training loss: 12987.3623...  5.6785 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1830...  Training loss: 12929.2363...  6.2276 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1831...  Training loss: 12949.2266...  6.1030 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1832...  Training loss: 13254.4785...  5.6500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1833...  Training loss: 12763.0840...  6.7220 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1834...  Training loss: 13428.8574...  5.7425 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1835...  Training loss: 13145.9414...  5.6251 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1836...  Training loss: 13126.7168...  6.7495 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1837...  Training loss: 13056.1113...  5.7373 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1838...  Training loss: 12952.5078...  5.6888 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1839...  Training loss: 13142.6465...  6.6647 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1840...  Training loss: 13017.0547...  5.9621 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1841...  Training loss: 12724.3926...  6.2077 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1842...  Training loss: 13302.1562...  6.2754 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1843...  Training loss: 13117.3652...  5.9679 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1844...  Training loss: 13464.7402...  6.7129 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1845...  Training loss: 13268.9219...  5.6510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1846...  Training loss: 13107.7656...  5.7613 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1847...  Training loss: 13111.2910...  6.7003 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1848...  Training loss: 13035.9424...  5.7508 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1849...  Training loss: 13171.3018...  5.6695 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1850...  Training loss: 12907.6582...  6.6805 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1851...  Training loss: 13067.2617...  5.6959 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1852...  Training loss: 12800.9795...  5.6298 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1853...  Training loss: 13511.1377...  6.7471 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1854...  Training loss: 13149.0898...  5.7274 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1855...  Training loss: 13318.4463...  6.5234 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1856...  Training loss: 12836.5967...  5.9185 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1857...  Training loss: 13130.7402...  5.6158 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1858...  Training loss: 13217.8027...  6.7307 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1859...  Training loss: 12953.1934...  5.7285 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1860...  Training loss: 12924.8828...  5.6830 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1861...  Training loss: 12699.2734...  6.7524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1862...  Training loss: 13127.5645...  5.7392 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1863...  Training loss: 12614.4219...  5.6926 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1864...  Training loss: 13078.8730...  6.7962 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1865...  Training loss: 12775.4023...  5.8389 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1866...  Training loss: 12917.4883...  5.6888 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1867...  Training loss: 12698.0127...  6.6609 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1868...  Training loss: 12917.9111...  5.6323 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1869...  Training loss: 12725.9375...  5.7389 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1870...  Training loss: 12749.6758...  6.6788 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1871...  Training loss: 12624.6865...  5.7061 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1872...  Training loss: 12991.8369...  6.6104 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1873...  Training loss: 12652.0664...  5.9083 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1874...  Training loss: 12749.2969...  5.6889 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1875...  Training loss: 12711.1084...  6.7647 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1876...  Training loss: 12671.9658...  5.7313 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1877...  Training loss: 12849.0459...  5.6654 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1878...  Training loss: 13107.1699...  6.7515 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1879...  Training loss: 13027.6133...  5.5792 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1880...  Training loss: 12629.5596...  5.6563 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1881...  Training loss: 12749.9062...  6.7012 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1882...  Training loss: 12580.2871...  5.5976 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1883...  Training loss: 13048.9385...  5.7782 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1884...  Training loss: 12891.0566...  6.9782 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1885...  Training loss: 12878.5879...  5.9509 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1886...  Training loss: 12873.7637...  6.7391 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1887...  Training loss: 12938.2578...  6.1918 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1888...  Training loss: 12925.9941...  5.6601 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1889...  Training loss: 13011.2285...  6.7797 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1890...  Training loss: 13076.4512...  5.6976 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1891...  Training loss: 12887.1738...  5.7050 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1892...  Training loss: 13116.1660...  6.7410 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1893...  Training loss: 12919.6670...  5.7530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1894...  Training loss: 13056.7578...  5.7020 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1895...  Training loss: 13061.7861...  6.7256 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1896...  Training loss: 12953.6719...  5.7826 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1897...  Training loss: 12800.4004...  6.6373 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1898...  Training loss: 12689.9512...  5.7809 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1899...  Training loss: 13044.0293...  5.6843 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1900...  Training loss: 13090.9922...  6.8141 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1901...  Training loss: 13007.6035...  5.6649 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1902...  Training loss: 12995.7070...  5.6166 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1903...  Training loss: 12923.9932...  6.7473 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1904...  Training loss: 12692.4297...  5.6551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1905...  Training loss: 12517.4033...  5.7013 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1906...  Training loss: 13060.6133...  7.2800 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1907...  Training loss: 12855.9775...  6.1902 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1908...  Training loss: 12545.5352...  6.7736 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1909...  Training loss: 12985.5547...  5.6202 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1910...  Training loss: 13008.8750...  5.7948 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1911...  Training loss: 12744.6191...  6.7578 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1912...  Training loss: 12568.3203...  5.6820 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1913...  Training loss: 12443.4424...  5.7986 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1914...  Training loss: 12726.4697...  6.7275 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1915...  Training loss: 13191.0938...  5.7778 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1916...  Training loss: 12960.2578...  5.6311 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1917...  Training loss: 12979.2197...  6.6797 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1918...  Training loss: 12939.1719...  5.7441 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1919...  Training loss: 13173.3916...  6.5015 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1920...  Training loss: 13063.1436...  5.8284 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1921...  Training loss: 12845.4297...  5.6286 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1922...  Training loss: 12941.1875...  6.6338 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1923...  Training loss: 13398.8398...  5.7228 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1924...  Training loss: 13053.4541...  5.6281 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1925...  Training loss: 12711.4023...  6.7441 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1926...  Training loss: 13142.7344...  5.6951 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1927...  Training loss: 12736.2422...  5.6696 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1928...  Training loss: 13129.7207...  6.7221 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1929...  Training loss: 13013.4736...  5.7203 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1930...  Training loss: 13214.1895...  5.6499 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1931...  Training loss: 13239.1963...  6.6423 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1932...  Training loss: 13011.4961...  5.6663 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1933...  Training loss: 12643.7539...  6.1275 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1934...  Training loss: 12828.6211...  6.4357 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1935...  Training loss: 12973.8955...  5.6721 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1936...  Training loss: 12836.1172...  7.1346 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1937...  Training loss: 12818.1074...  5.8651 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1938...  Training loss: 12963.8926...  5.6206 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1939...  Training loss: 12945.0029...  6.6690 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1940...  Training loss: 12818.8984...  5.6722 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1941...  Training loss: 12625.8496...  5.6406 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1942...  Training loss: 13074.6025...  6.7088 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1943...  Training loss: 13131.9697...  5.6221 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1944...  Training loss: 12981.9316...  5.5596 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1945...  Training loss: 12894.9141...  6.6207 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1946...  Training loss: 12942.4844...  5.7062 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1947...  Training loss: 13016.7002...  5.7182 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1948...  Training loss: 12931.6348...  6.3919 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1949...  Training loss: 13123.3086...  5.9540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1950...  Training loss: 13665.9766...  5.6061 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1951...  Training loss: 13049.1475...  5.6914 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1952...  Training loss: 13017.2656...  6.6886 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1953...  Training loss: 12768.9355...  5.5580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1954...  Training loss: 12780.3506...  5.8462 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1955...  Training loss: 13372.5352...  7.0621 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1956...  Training loss: 12963.4590...  5.9598 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1957...  Training loss: 12973.0020...  5.6907 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1958...  Training loss: 12635.8926...  5.6563 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1959...  Training loss: 12823.6719...  6.6101 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1960...  Training loss: 13268.5273...  5.7161 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1961...  Training loss: 12814.4062...  5.6533 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1962...  Training loss: 12691.4414...  6.7266 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1963...  Training loss: 12751.6504...  5.6146 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1964...  Training loss: 12902.6270...  5.5681 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1965...  Training loss: 12970.6865...  6.5856 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1966...  Training loss: 12866.6396...  5.7447 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1967...  Training loss: 12787.2881...  5.6523 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1968...  Training loss: 12723.5088...  5.6126 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1969...  Training loss: 13147.3281...  6.6380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1970...  Training loss: 12823.9707...  5.6552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1971...  Training loss: 12910.4922...  5.6755 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1972...  Training loss: 12815.5996...  6.6707 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1973...  Training loss: 12696.3818...  5.6700 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1974...  Training loss: 12646.3652...  5.6869 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1975...  Training loss: 12973.4199...  6.5511 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1976...  Training loss: 12868.9834...  5.7930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1977...  Training loss: 12598.5352...  5.6479 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1978...  Training loss: 12873.6250...  5.6648 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1979...  Training loss: 12782.8271...  6.5967 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1980...  Training loss: 12676.2559...  5.6810 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1981...  Training loss: 14627.6045...  5.6538 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1982...  Training loss: 13286.7695...  6.6870 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1983...  Training loss: 12834.8789...  5.6157 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1984...  Training loss: 13190.0469...  5.6017 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1985...  Training loss: 12736.7842...  6.6152 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1986...  Training loss: 12630.1797...  5.7134 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1987...  Training loss: 12892.5449...  5.6459 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1988...  Training loss: 12947.3965...  5.9130 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1989...  Training loss: 12990.0078...  6.3729 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1990...  Training loss: 12707.3545...  5.7270 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1991...  Training loss: 12718.8916...  5.5958 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1992...  Training loss: 12789.3398...  6.7020 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1993...  Training loss: 12818.3594...  5.6943 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1994...  Training loss: 13033.7910...  5.6395 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1995...  Training loss: 12634.0391...  6.7229 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1996...  Training loss: 12603.2920...  5.6107 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1997...  Training loss: 13037.6914...  5.6607 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1998...  Training loss: 12977.3164...  6.6899 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1999...  Training loss: 12860.5176...  5.6496 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2000...  Training loss: 13169.0713...  5.6946 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2001...  Training loss: 12828.2539...  6.6775 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2002...  Training loss: 13103.0576...  5.7212 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2003...  Training loss: 12740.4307...  5.5887 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2004...  Training loss: 13053.7266...  7.2284 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2005...  Training loss: 12877.4180...  6.5601 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2006...  Training loss: 12427.9219...  5.6446 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2007...  Training loss: 12700.4766...  5.9623 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2008...  Training loss: 12975.5918...  7.0128 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2009...  Training loss: 12847.9482...  5.6204 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2010...  Training loss: 13195.8760...  5.6693 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2011...  Training loss: 12781.5293...  6.7237 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2012...  Training loss: 12799.6436...  5.6847 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2013...  Training loss: 12929.0498...  5.6278 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2014...  Training loss: 12882.9121...  6.6576 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2015...  Training loss: 12679.5098...  5.7293 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2016...  Training loss: 12940.3789...  5.7414 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2017...  Training loss: 12617.9102...  7.0510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2018...  Training loss: 12451.8438...  5.6628 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2019...  Training loss: 12378.4131...  5.5689 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2020...  Training loss: 12784.6719...  6.7230 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2021...  Training loss: 12645.9258...  5.7233 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2022...  Training loss: 13208.5254...  5.6891 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2023...  Training loss: 12743.5371...  6.3721 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2024...  Training loss: 12588.8965...  5.9610 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2025...  Training loss: 12905.1973...  5.6875 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2026...  Training loss: 12581.8545...  5.6869 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2027...  Training loss: 12808.3398...  6.5851 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2028...  Training loss: 12848.6816...  5.6656 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2029...  Training loss: 12785.0137...  5.5850 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2030...  Training loss: 13005.8594...  6.5250 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2031...  Training loss: 12598.1973...  5.8117 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2032...  Training loss: 13177.4268...  5.6619 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2033...  Training loss: 12872.6094...  5.6269 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2034...  Training loss: 12895.0156...  6.6293 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2035...  Training loss: 12824.6836...  5.6453 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2036...  Training loss: 12844.3457...  5.8793 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2037...  Training loss: 12941.1152...  6.7077 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2038...  Training loss: 12760.3398...  5.5544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2039...  Training loss: 12561.2090...  5.8847 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2040...  Training loss: 13255.6465...  6.8587 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2041...  Training loss: 12863.8906...  5.6574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2042...  Training loss: 13321.5098...  5.7356 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2043...  Training loss: 13177.6309...  6.6340 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2044...  Training loss: 12929.1328...  5.6187 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2045...  Training loss: 12877.7803...  5.7231 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2046...  Training loss: 12822.4531...  6.6083 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2047...  Training loss: 13136.0605...  5.6913 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2048...  Training loss: 12751.3926...  5.6471 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2049...  Training loss: 12840.1094...  6.6522 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2050...  Training loss: 12717.5352...  5.6654 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2051...  Training loss: 13309.0977...  5.6284 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2052...  Training loss: 12981.5566...  6.5324 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2053...  Training loss: 13209.6738...  5.7334 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2054...  Training loss: 12687.4883...  5.6234 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2055...  Training loss: 12948.0664...  5.7171 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2056...  Training loss: 13102.3447...  6.5336 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2057...  Training loss: 12818.7295...  5.7300 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2058...  Training loss: 12728.7480...  5.6620 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2059...  Training loss: 12440.7373...  6.4191 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2060...  Training loss: 12970.6357...  5.9774 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2061...  Training loss: 12474.2324...  5.6531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2062...  Training loss: 12799.1045...  6.4155 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2063...  Training loss: 12590.8027...  5.8757 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2064...  Training loss: 12862.2031...  5.6101 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2065...  Training loss: 12580.0225...  6.3909 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2066...  Training loss: 12817.1436...  5.8776 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2067...  Training loss: 12667.7031...  5.6647 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2068...  Training loss: 12496.5986...  6.4242 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2069...  Training loss: 12469.5938...  5.8159 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2070...  Training loss: 12874.8008...  5.7116 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2071...  Training loss: 12587.6191...  5.5916 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2072...  Training loss: 12756.0439...  6.6599 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2073...  Training loss: 12552.4121...  5.6779 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2074...  Training loss: 12610.9199...  5.5902 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2075...  Training loss: 12627.8594...  6.7239 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2076...  Training loss: 12938.8926...  5.6480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2077...  Training loss: 12807.7998...  5.7138 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2078...  Training loss: 12561.6895...  6.5663 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2079...  Training loss: 12811.9648...  5.6197 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2080...  Training loss: 12516.6074...  5.6366 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2081...  Training loss: 12832.6973...  6.6207 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2082...  Training loss: 12739.1973...  5.7746 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2083...  Training loss: 12842.3652...  5.6513 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2084...  Training loss: 12719.2656...  6.6346 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2085...  Training loss: 12909.7598...  5.7427 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2086...  Training loss: 12735.2402...  5.6983 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2087...  Training loss: 12850.2627...  7.7580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2088...  Training loss: 12932.1992...  5.9879 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2089...  Training loss: 12754.8633...  6.0826 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2090...  Training loss: 12978.4678...  9.0280 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2091...  Training loss: 12669.0215...  8.4814 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2092...  Training loss: 12898.8359...  9.3965 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2093...  Training loss: 12837.7559...  8.2573 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2094...  Training loss: 12636.4980...  8.3555 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2095...  Training loss: 12601.0000...  6.2828 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2096...  Training loss: 12437.2266...  7.6292 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2097...  Training loss: 12944.8555...  6.5273 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2098...  Training loss: 12901.5928...  6.9854 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2099...  Training loss: 12842.5859...  7.9567 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2100...  Training loss: 12661.4102...  6.3811 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2101...  Training loss: 12754.0254...  7.6169 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2102...  Training loss: 12490.8193...  6.1674 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2103...  Training loss: 12360.4824...  6.7175 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2104...  Training loss: 12896.3486...  7.2883 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2105...  Training loss: 12749.9170...  7.0763 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2106...  Training loss: 12449.0078...  8.0118 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2107...  Training loss: 12811.0176...  6.4237 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2108...  Training loss: 12852.5244...  7.8195 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2109...  Training loss: 12623.3027...  6.8719 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2110...  Training loss: 12386.0430...  6.9567 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2111...  Training loss: 12327.1348...  7.6427 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2112...  Training loss: 12594.9990...  6.7241 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2113...  Training loss: 12895.5654...  7.9080 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2114...  Training loss: 12865.0889...  6.7118 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2115...  Training loss: 12783.0039...  10.5523 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2116...  Training loss: 12798.6113...  8.0719 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2117...  Training loss: 12990.2949...  9.3765 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2118...  Training loss: 12951.7881...  7.0970 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2119...  Training loss: 12841.5664...  6.2527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2120...  Training loss: 12729.5840...  7.1823 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2121...  Training loss: 13254.3643...  5.7567 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2122...  Training loss: 12944.3945...  6.4757 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2123...  Training loss: 12712.9258...  6.2567 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2124...  Training loss: 13050.9268...  5.8621 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2125...  Training loss: 12629.6562...  7.5500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2126...  Training loss: 13064.6328...  5.8297 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2127...  Training loss: 12765.7012...  5.8129 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2128...  Training loss: 13085.5059...  7.3869 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2129...  Training loss: 13041.3105...  6.6224 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2130...  Training loss: 12765.1680...  6.9086 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2131...  Training loss: 12470.0000...  6.2624 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2132...  Training loss: 12605.6104...  6.1469 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2133...  Training loss: 12848.4395...  6.7923 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2134...  Training loss: 12660.6719...  5.6984 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2135...  Training loss: 12666.3105...  5.7047 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2136...  Training loss: 12684.2891...  6.7673 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2137...  Training loss: 12790.6406...  5.7633 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2138...  Training loss: 12703.8701...  5.7531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2139...  Training loss: 12526.7646...  6.6938 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2140...  Training loss: 12950.3916...  5.8081 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2141...  Training loss: 13103.6973...  5.6764 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2142...  Training loss: 12934.8223...  7.4681 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2143...  Training loss: 12698.0371...  7.0138 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2144...  Training loss: 12802.4424...  6.0796 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2145...  Training loss: 12800.4707...  7.9352 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2146...  Training loss: 12761.2852...  5.9203 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2147...  Training loss: 13179.1826...  5.8966 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2148...  Training loss: 13495.1455...  6.3392 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2149...  Training loss: 12887.0166...  6.9764 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2150...  Training loss: 12747.2920...  7.4044 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2151...  Training loss: 12670.1221...  6.8125 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2152...  Training loss: 12684.0215...  5.7666 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2153...  Training loss: 13216.6182...  5.6728 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2154...  Training loss: 12910.7148...  6.8708 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2155...  Training loss: 12848.4668...  5.6499 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2156...  Training loss: 12611.8506...  5.7608 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2157...  Training loss: 12745.2363...  6.8750 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2158...  Training loss: 13107.6533...  5.7273 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2159...  Training loss: 12688.3643...  6.5565 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2160...  Training loss: 12578.3223...  6.0755 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2161...  Training loss: 12691.6328...  5.7368 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2162...  Training loss: 12693.7793...  6.9612 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2163...  Training loss: 12931.4932...  5.7481 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2164...  Training loss: 12694.5693...  5.7453 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2165...  Training loss: 12594.0508...  6.7587 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2166...  Training loss: 12622.1387...  5.7154 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2167...  Training loss: 13024.1699...  5.7598 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2168...  Training loss: 12704.7178...  6.7505 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2169...  Training loss: 12656.7930...  5.7355 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2170...  Training loss: 12715.1201...  6.5550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2171...  Training loss: 12563.0723...  6.0366 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2172...  Training loss: 12493.2217...  5.7405 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2173...  Training loss: 12748.6465...  6.8806 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2174...  Training loss: 12627.1289...  7.1648 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2175...  Training loss: 12428.2549...  8.9734 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2176...  Training loss: 12767.5469...  7.1690 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2177...  Training loss: 12724.4150...  5.9710 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2178...  Training loss: 12600.8975...  8.2442 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2179...  Training loss: 14537.7402...  5.7720 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2180...  Training loss: 13167.1885...  5.8445 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2181...  Training loss: 12850.6680...  7.2571 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2182...  Training loss: 12977.2539...  5.7683 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2183...  Training loss: 12654.2754...  6.8844 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2184...  Training loss: 12467.2100...  5.6944 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2185...  Training loss: 12677.7822...  5.9822 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2186...  Training loss: 12738.0566...  7.9913 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2187...  Training loss: 12932.3096...  6.9226 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2188...  Training loss: 12520.6426...  7.7276 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2189...  Training loss: 12534.9766...  6.0300 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2190...  Training loss: 12703.1406...  6.2490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2191...  Training loss: 12742.5293...  8.0462 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2192...  Training loss: 12946.6602...  6.6988 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2193...  Training loss: 12504.0469...  6.8756 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2194...  Training loss: 12475.5439...  5.7865 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2195...  Training loss: 12919.7637...  5.7829 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2196...  Training loss: 12836.6074...  7.7186 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2197...  Training loss: 12724.9219...  6.3438 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2198...  Training loss: 12951.9238...  6.6318 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2199...  Training loss: 12672.8477...  5.9939 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2200...  Training loss: 12922.1270...  5.7232 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2201...  Training loss: 12660.8496...  6.8883 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2202...  Training loss: 12906.8047...  5.7379 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2203...  Training loss: 12780.7246...  5.7654 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2204...  Training loss: 12337.7656...  6.8118 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2205...  Training loss: 12554.9756...  5.8723 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2206...  Training loss: 12918.3047...  5.7478 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2207...  Training loss: 12781.0498...  6.7942 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2208...  Training loss: 13127.7324...  5.8105 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2209...  Training loss: 12595.0938...  6.8706 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2210...  Training loss: 12567.7021...  5.7169 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2211...  Training loss: 12796.0820...  5.7826 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2212...  Training loss: 12805.6885...  6.9337 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2213...  Training loss: 12612.7451...  6.0048 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2214...  Training loss: 12821.4795...  5.7842 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2215...  Training loss: 12571.3779...  6.7376 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2216...  Training loss: 12348.0273...  5.6596 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2217...  Training loss: 12238.6455...  6.0845 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2218...  Training loss: 12434.3076...  6.5180 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2219...  Training loss: 12470.4092...  5.8436 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2220...  Training loss: 13165.4365...  6.7980 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2221...  Training loss: 12610.6689...  5.7312 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2222...  Training loss: 12526.0166...  5.7417 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2223...  Training loss: 12758.4248...  6.8913 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2224...  Training loss: 12531.8350...  5.7542 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2225...  Training loss: 12636.1846...  5.7396 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2226...  Training loss: 12631.3584...  6.8817 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2227...  Training loss: 12649.6758...  5.8362 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2228...  Training loss: 12890.1357...  6.5609 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2229...  Training loss: 12405.9941...  5.9636 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2230...  Training loss: 13131.6094...  5.7810 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2231...  Training loss: 12746.9746...  6.8479 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2232...  Training loss: 12745.9912...  5.7590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2233...  Training loss: 12696.1875...  6.0838 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2234...  Training loss: 12704.4971...  8.8921 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2235...  Training loss: 12897.2949...  6.9880 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2236...  Training loss: 12622.7500...  7.7641 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2237...  Training loss: 12411.1182...  6.5856 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2238...  Training loss: 13043.1211...  9.0458 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2239...  Training loss: 12797.6641...  6.9083 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2240...  Training loss: 13168.3535...  6.3232 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2241...  Training loss: 12871.7656...  8.3192 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2242...  Training loss: 12828.6699...  6.9913 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2243...  Training loss: 12696.6406...  7.6493 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2244...  Training loss: 12835.1025...  6.2587 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2245...  Training loss: 12921.3555...  6.5992 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2246...  Training loss: 12558.0039...  7.3158 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2247...  Training loss: 12692.2188...  6.3814 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2248...  Training loss: 12593.7910...  7.6685 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2249...  Training loss: 13107.0410...  7.2319 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2250...  Training loss: 12786.8262...  7.5121 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2251...  Training loss: 13072.9922...  6.8442 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2252...  Training loss: 12577.9463...  6.9555 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2253...  Training loss: 12776.0410...  9.7243 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2254...  Training loss: 12908.4551...  6.5221 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2255...  Training loss: 12719.8086...  7.7325 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2256...  Training loss: 12529.9551...  6.5599 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2257...  Training loss: 12484.1914...  6.2034 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2258...  Training loss: 12893.1387...  7.1353 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2259...  Training loss: 12338.2314...  5.8442 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2260...  Training loss: 12776.1465...  6.7563 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2261...  Training loss: 12543.9209...  6.0591 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2262...  Training loss: 12745.1885...  6.5392 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2263...  Training loss: 12468.3926...  8.0712 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2264...  Training loss: 12636.4551...  5.9535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2265...  Training loss: 12421.0273...  6.4391 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2266...  Training loss: 12458.5918...  6.2324 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2267...  Training loss: 12336.6250...  5.7863 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2268...  Training loss: 12647.1475...  6.7622 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2269...  Training loss: 12434.8516...  6.2130 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2270...  Training loss: 12536.5273...  5.7692 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2271...  Training loss: 12344.9102...  6.2819 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2272...  Training loss: 12448.5410...  6.7815 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2273...  Training loss: 12528.3047...  8.2982 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2274...  Training loss: 12772.8809...  7.3886 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2275...  Training loss: 12619.4043...  6.3162 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2276...  Training loss: 12413.9688...  5.8108 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2277...  Training loss: 12540.9492...  8.2205 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2278...  Training loss: 12362.8574...  6.3262 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2279...  Training loss: 12609.2881...  9.8204 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2280...  Training loss: 12518.4180...  7.9749 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2281...  Training loss: 12620.7988...  9.7227 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2282...  Training loss: 12441.6787...  7.0707 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2283...  Training loss: 12686.1895...  8.0925 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2284...  Training loss: 12625.2744...  7.1001 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2285...  Training loss: 12665.2012...  6.7615 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2286...  Training loss: 12730.8809...  8.2501 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2287...  Training loss: 12617.2461...  7.0618 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2288...  Training loss: 12885.5166...  8.2481 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2289...  Training loss: 12551.2305...  6.4620 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2290...  Training loss: 12836.7656...  7.7463 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2291...  Training loss: 12672.9980...  6.5126 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2292...  Training loss: 12627.3662...  6.4321 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2293...  Training loss: 12473.9180...  8.0331 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2294...  Training loss: 12393.4766...  6.5323 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2295...  Training loss: 12723.8691...  8.0194 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2296...  Training loss: 12759.2344...  6.4001 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2297...  Training loss: 12694.7227...  6.5962 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2298...  Training loss: 12767.8418...  7.5695 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2299...  Training loss: 12741.5361...  6.5172 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2300...  Training loss: 12384.6084...  7.6987 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2301...  Training loss: 12238.2246...  6.4100 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2302...  Training loss: 12734.6641...  6.4023 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2303...  Training loss: 12581.8730...  7.6996 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2304...  Training loss: 12221.1914...  6.8886 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2305...  Training loss: 12660.7754...  8.0895 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2306...  Training loss: 12657.9717...  6.5207 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2307...  Training loss: 12508.3604...  7.6383 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2308...  Training loss: 12247.7129...  6.6297 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2309...  Training loss: 12208.2451...  6.4409 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2310...  Training loss: 12407.6318...  7.7101 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2311...  Training loss: 12848.8984...  6.3931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2312...  Training loss: 12608.4941...  8.3176 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2313...  Training loss: 12615.8203...  7.0054 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2314...  Training loss: 12528.4199...  7.1777 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2315...  Training loss: 12852.6328...  7.3604 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2316...  Training loss: 12687.9619...  6.9373 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2317...  Training loss: 12626.0137...  7.7190 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2318...  Training loss: 12623.8125...  7.6958 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2319...  Training loss: 13121.3135...  7.9154 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2320...  Training loss: 12778.1592...  6.5270 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2321...  Training loss: 12495.9883...  7.4983 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2322...  Training loss: 12941.4688...  7.2730 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2323...  Training loss: 12473.1348...  6.4878 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2324...  Training loss: 12838.0020...  7.7182 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2325...  Training loss: 12702.8184...  6.5060 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2326...  Training loss: 12916.3662...  7.6157 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2327...  Training loss: 12905.4697...  6.8134 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2328...  Training loss: 12695.4492...  6.4990 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2329...  Training loss: 12308.3721...  7.9064 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2330...  Training loss: 12470.5781...  6.5728 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2331...  Training loss: 12865.1924...  7.7091 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2332...  Training loss: 12568.2656...  6.4063 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2333...  Training loss: 12497.2637...  6.5392 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2334...  Training loss: 12522.3945...  7.7480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2335...  Training loss: 12668.7949...  6.5827 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2336...  Training loss: 12479.4717...  7.8023 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2337...  Training loss: 12329.2510...  6.6470 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2338...  Training loss: 12739.7285...  7.4195 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2339...  Training loss: 12888.1602...  7.4165 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2340...  Training loss: 12705.7480...  6.5518 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2341...  Training loss: 12583.6641...  7.8286 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2342...  Training loss: 12645.7598...  6.4917 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2343...  Training loss: 12623.2158...  7.6115 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2344...  Training loss: 12526.1934...  6.6160 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2345...  Training loss: 12971.2207...  6.4686 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2346...  Training loss: 13229.2363...  7.7915 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2347...  Training loss: 12778.8115...  6.6417 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2348...  Training loss: 12663.8477...  7.9690 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2349...  Training loss: 12681.8691...  6.4498 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2350...  Training loss: 12609.4717...  6.5408 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2351...  Training loss: 13108.7236...  7.8463 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2352...  Training loss: 12692.9609...  6.6006 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2353...  Training loss: 12686.1777...  7.7239 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2354...  Training loss: 12477.2266...  6.4834 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2355...  Training loss: 12616.5352...  7.1896 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2356...  Training loss: 13047.4648...  7.0910 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2357...  Training loss: 12548.9961...  6.5579 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2358...  Training loss: 12578.1406...  7.7491 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2359...  Training loss: 12589.3770...  6.1027 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2360...  Training loss: 12551.1211...  7.3446 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2361...  Training loss: 12694.9854...  7.8558 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2362...  Training loss: 12636.6562...  6.3604 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2363...  Training loss: 12682.0234...  8.6852 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2364...  Training loss: 12461.5723...  6.5455 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2365...  Training loss: 12933.8945...  7.7356 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2366...  Training loss: 12509.9688...  5.7881 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2367...  Training loss: 12530.3076...  5.4636 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2368...  Training loss: 12585.0312...  6.6636 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2369...  Training loss: 12374.2930...  5.5695 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2370...  Training loss: 12415.2461...  5.5938 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2371...  Training loss: 12638.2090...  6.5249 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2372...  Training loss: 12434.9697...  5.5316 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2373...  Training loss: 12219.1738...  5.5312 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2374...  Training loss: 12611.6309...  6.4681 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2375...  Training loss: 12558.7168...  5.5944 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2376...  Training loss: 12418.2227...  5.5402 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2377...  Training loss: 14328.0879...  5.5215 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2378...  Training loss: 12996.9238...  6.4416 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2379...  Training loss: 12693.4883...  5.6484 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2380...  Training loss: 12878.8125...  5.6234 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2381...  Training loss: 12482.1436...  5.7504 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2382...  Training loss: 12309.8223...  6.3639 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2383...  Training loss: 12562.7061...  5.9205 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2384...  Training loss: 12573.9355...  6.4125 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2385...  Training loss: 12716.3076...  7.1849 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2386...  Training loss: 12422.1211...  5.5629 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2387...  Training loss: 12449.9219...  5.5355 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2388...  Training loss: 12511.0645...  7.3143 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2389...  Training loss: 12632.7207...  6.3899 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2390...  Training loss: 12772.6436...  5.6307 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2391...  Training loss: 12387.1689...  6.5292 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2392...  Training loss: 12321.0801...  6.0821 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2393...  Training loss: 12817.1631...  6.4452 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2394...  Training loss: 12798.6787...  6.8924 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2395...  Training loss: 12518.2480...  5.6381 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2396...  Training loss: 12885.0186...  5.7313 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2397...  Training loss: 12619.8359...  7.4996 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2398...  Training loss: 12732.5254...  6.2982 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2399...  Training loss: 12575.4629...  5.5790 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2400...  Training loss: 12790.4531...  5.6152 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2401...  Training loss: 12684.4688...  7.3432 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2402...  Training loss: 12170.7871...  6.3067 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2403...  Training loss: 12487.2949...  5.5742 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2404...  Training loss: 12667.4941...  6.5693 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2405...  Training loss: 12636.2793...  6.1789 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2406...  Training loss: 12966.6699...  6.5878 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2407...  Training loss: 12584.5859...  7.5014 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2408...  Training loss: 12363.9941...  5.8447 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2409...  Training loss: 12566.7266...  5.5159 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2410...  Training loss: 12640.4463...  6.5469 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2411...  Training loss: 12445.2139...  6.2226 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2412...  Training loss: 12576.7236...  5.5538 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2413...  Training loss: 12325.5723...  6.1376 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2414...  Training loss: 12220.2549...  5.9974 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2415...  Training loss: 12249.4824...  5.5769 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2416...  Training loss: 12441.4365...  5.5609 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2417...  Training loss: 12293.5234...  6.4682 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2418...  Training loss: 12942.2070...  5.6295 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2419...  Training loss: 12437.2646...  5.5944 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2420...  Training loss: 12374.3877...  6.0939 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2421...  Training loss: 12608.5352...  6.1455 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2422...  Training loss: 12308.7734...  5.5738 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2423...  Training loss: 12426.7852...  5.5209 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2424...  Training loss: 12472.2197...  6.5683 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2425...  Training loss: 12443.5137...  5.5878 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2426...  Training loss: 12680.7471...  5.5606 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2427...  Training loss: 12282.9277...  6.6457 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2428...  Training loss: 12883.3057...  5.5684 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2429...  Training loss: 12563.2988...  5.5807 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2430...  Training loss: 12550.0400...  6.4900 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2431...  Training loss: 12508.8896...  5.6629 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2432...  Training loss: 12537.6191...  5.5134 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2433...  Training loss: 12765.9336...  5.8144 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2434...  Training loss: 12473.1562...  6.3857 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2435...  Training loss: 12251.5557...  5.5186 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2436...  Training loss: 12914.2070...  5.5746 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2437...  Training loss: 12645.3027...  6.6687 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2438...  Training loss: 13047.0234...  5.5452 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2439...  Training loss: 12831.6016...  5.5423 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2440...  Training loss: 12695.2832...  6.5814 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2441...  Training loss: 12583.9336...  5.5978 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2442...  Training loss: 12605.9805...  5.6674 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2443...  Training loss: 12766.0547...  6.5268 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2444...  Training loss: 12451.3750...  5.6605 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2445...  Training loss: 12584.8506...  5.8048 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2446...  Training loss: 12454.0625...  5.5660 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2447...  Training loss: 13072.9404...  6.6024 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2448...  Training loss: 12701.6484...  5.6085 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2449...  Training loss: 12915.2773...  5.6109 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2450...  Training loss: 12384.6211...  6.4948 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2451...  Training loss: 12626.9590...  5.5234 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2452...  Training loss: 12694.1943...  5.5708 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2453...  Training loss: 12542.8535...  6.4671 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2454...  Training loss: 12410.6055...  5.5995 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2455...  Training loss: 12297.7461...  5.5030 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2456...  Training loss: 12661.8613...  5.7189 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2457...  Training loss: 12247.7129...  6.6768 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2458...  Training loss: 12681.3809...  5.5413 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2459...  Training loss: 12385.0352...  5.6697 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2460...  Training loss: 12525.1963...  7.0348 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2461...  Training loss: 12400.2588...  5.5121 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2462...  Training loss: 12465.5928...  5.5737 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2463...  Training loss: 12306.1328...  6.5501 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2464...  Training loss: 12319.1758...  5.6370 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2465...  Training loss: 12197.0371...  5.5862 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2466...  Training loss: 12542.1641...  6.5888 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2467...  Training loss: 12330.8691...  5.6237 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2468...  Training loss: 12420.6719...  5.5704 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2469...  Training loss: 12246.7793...  5.8918 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2470...  Training loss: 12131.9590...  5.7961 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2471...  Training loss: 12312.0244...  5.5965 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2472...  Training loss: 12696.8057...  6.4646 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2473...  Training loss: 12444.6699...  5.6602 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2474...  Training loss: 12180.6582...  5.9101 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2475...  Training loss: 12387.1309...  5.6543 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2476...  Training loss: 12269.1064...  5.6140 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2477...  Training loss: 12488.6035...  6.5747 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2478...  Training loss: 12332.2031...  5.5751 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2479...  Training loss: 12558.6191...  5.6415 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2480...  Training loss: 12426.3643...  6.3961 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2481...  Training loss: 12554.7158...  5.6944 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2482...  Training loss: 12478.1035...  5.5685 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2483...  Training loss: 12497.4531...  5.7457 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2484...  Training loss: 12588.5488...  6.3278 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2485...  Training loss: 12395.9912...  5.5762 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2486...  Training loss: 12714.5303...  5.5387 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2487...  Training loss: 12476.1045...  6.5720 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2488...  Training loss: 12651.1953...  5.5382 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2489...  Training loss: 12565.7871...  5.6005 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2490...  Training loss: 12415.2852...  6.4410 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2491...  Training loss: 12296.5996...  5.6622 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2492...  Training loss: 12236.5527...  5.5079 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2493...  Training loss: 12610.2266...  5.5617 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2494...  Training loss: 12656.5645...  6.5275 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2495...  Training loss: 12564.5615...  5.5680 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2496...  Training loss: 12536.6006...  5.5591 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2497...  Training loss: 12455.4824...  6.5422 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2498...  Training loss: 12226.3750...  5.5735 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2499...  Training loss: 12083.2852...  5.5653 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2500...  Training loss: 12644.8857...  6.1227 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2501...  Training loss: 12421.9541...  6.0226 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2502...  Training loss: 12205.1035...  5.5253 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2503...  Training loss: 12584.6484...  5.5689 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2504...  Training loss: 12573.0215...  6.5293 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2505...  Training loss: 12351.9492...  5.6138 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2506...  Training loss: 12102.2314...  5.5185 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2507...  Training loss: 12061.3955...  6.5039 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2508...  Training loss: 12306.0381...  5.5898 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2509...  Training loss: 12764.6533...  5.5434 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2510...  Training loss: 12523.8027...  5.9798 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2511...  Training loss: 12471.5020...  6.1665 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2512...  Training loss: 12463.2090...  5.6067 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2513...  Training loss: 12727.9756...  5.5779 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2514...  Training loss: 12743.7324...  6.5551 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2515...  Training loss: 12478.4883...  5.5288 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2516...  Training loss: 12555.2539...  6.0498 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2517...  Training loss: 12923.8379...  7.1525 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2518...  Training loss: 12667.2432...  5.5626 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2519...  Training loss: 12395.8125...  5.4898 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2520...  Training loss: 12731.3984...  6.1653 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2521...  Training loss: 12370.5332...  6.0669 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2522...  Training loss: 12733.8320...  5.5104 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2523...  Training loss: 12504.9746...  5.5477 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2524...  Training loss: 12880.5947...  6.5572 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2525...  Training loss: 12791.4189...  5.5516 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2526...  Training loss: 12558.9492...  5.5624 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2527...  Training loss: 12299.1045...  6.5370 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2528...  Training loss: 12451.9648...  5.6468 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2529...  Training loss: 12575.1768...  5.4170 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2530...  Training loss: 12301.2246...  6.5579 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2531...  Training loss: 12363.3066...  5.7065 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2532...  Training loss: 12382.8594...  5.5284 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2533...  Training loss: 12448.1982...  6.4816 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2534...  Training loss: 12396.2168...  5.6472 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2535...  Training loss: 12146.5693...  5.5791 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2536...  Training loss: 12749.9316...  5.5792 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2537...  Training loss: 12806.5469...  6.5487 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2538...  Training loss: 12570.7812...  5.6079 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2539...  Training loss: 12424.1123...  5.4753 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2540...  Training loss: 12466.9941...  6.5235 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2541...  Training loss: 12508.6377...  5.5561 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2542...  Training loss: 12473.3652...  5.4684 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2543...  Training loss: 12793.7070...  6.6031 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2544...  Training loss: 13126.6455...  5.5031 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2545...  Training loss: 12707.2295...  5.6167 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2546...  Training loss: 12473.2637...  6.3611 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2547...  Training loss: 12353.1436...  6.4321 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2548...  Training loss: 12445.9688...  5.8398 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2549...  Training loss: 12814.0000...  7.4694 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2550...  Training loss: 12524.6260...  7.4985 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2551...  Training loss: 12558.2773...  6.3998 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2552...  Training loss: 12327.4316...  7.4342 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2553...  Training loss: 12505.1064...  5.9080 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2554...  Training loss: 12823.2148...  6.7001 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2555...  Training loss: 12395.8203...  5.9986 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2556...  Training loss: 12480.4238...  6.0234 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2557...  Training loss: 12403.9443...  7.6137 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2558...  Training loss: 12566.9258...  6.3163 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2559...  Training loss: 12552.0391...  6.4118 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2560...  Training loss: 12466.0293...  7.5431 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2561...  Training loss: 12465.3506...  6.4118 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2562...  Training loss: 12319.1562...  7.5468 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2563...  Training loss: 12760.0508...  6.3369 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2564...  Training loss: 12451.9795...  6.6295 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2565...  Training loss: 12383.5723...  7.5831 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2566...  Training loss: 12553.3896...  6.4677 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2567...  Training loss: 12271.4023...  7.5341 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2568...  Training loss: 12330.4863...  6.3442 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2569...  Training loss: 12478.4941...  6.2706 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2570...  Training loss: 12198.6299...  8.6916 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2571...  Training loss: 12171.6875...  6.9450 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2572...  Training loss: 12467.2109...  7.6231 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2573...  Training loss: 12456.1094...  6.8618 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2574...  Training loss: 12375.1074...  7.6521 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2575...  Training loss: 14212.2236...  6.2688 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2576...  Training loss: 12852.6807...  6.4120 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2577...  Training loss: 12558.4209...  7.5309 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2578...  Training loss: 12646.5801...  6.3909 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2579...  Training loss: 12335.3584...  7.8275 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2580...  Training loss: 12164.9238...  6.5305 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2581...  Training loss: 12376.7373...  6.4210 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2582...  Training loss: 12406.0918...  7.5339 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2583...  Training loss: 12636.0469...  6.3928 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2584...  Training loss: 12254.9238...  7.6781 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2585...  Training loss: 12279.7383...  6.4397 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2586...  Training loss: 12483.8008...  6.2912 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2587...  Training loss: 12388.7305...  7.4448 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2588...  Training loss: 12655.6250...  6.2540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2589...  Training loss: 12179.7949...  7.5488 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2590...  Training loss: 12149.9141...  6.3765 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2591...  Training loss: 12660.3398...  6.4699 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2592...  Training loss: 12617.9961...  7.6654 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2593...  Training loss: 12468.2363...  6.2971 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2594...  Training loss: 12676.0723...  7.7519 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2595...  Training loss: 12382.3262...  6.3687 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2596...  Training loss: 12577.8535...  6.3567 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2597...  Training loss: 12378.8770...  7.5178 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2598...  Training loss: 12583.1191...  6.2265 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2599...  Training loss: 12484.4238...  7.6323 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2600...  Training loss: 12062.5215...  6.2952 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2601...  Training loss: 12325.3223...  6.3082 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2602...  Training loss: 12643.8008...  7.6125 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2603...  Training loss: 12514.5723...  6.7107 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2604...  Training loss: 12745.5176...  7.6571 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2605...  Training loss: 12451.0771...  6.3949 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2606...  Training loss: 12304.9922...  6.9572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2607...  Training loss: 12467.2949...  7.5432 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2608...  Training loss: 12511.1191...  6.2269 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2609...  Training loss: 12403.4307...  7.7273 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2610...  Training loss: 12529.8223...  6.2698 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2611...  Training loss: 12215.6621...  7.2240 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2612...  Training loss: 12006.2139...  6.6100 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2613...  Training loss: 12062.0029...  6.2402 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2614...  Training loss: 12334.5078...  7.5249 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2615...  Training loss: 12142.1641...  6.2247 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2616...  Training loss: 12802.2988...  6.9553 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2617...  Training loss: 12413.6895...  7.0339 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2618...  Training loss: 12277.5781...  6.4026 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2619...  Training loss: 12525.4199...  7.4960 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2620...  Training loss: 12122.3496...  6.3882 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2621...  Training loss: 12404.6201...  6.7739 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2622...  Training loss: 12511.4141...  7.1298 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2623...  Training loss: 12475.2793...  6.2878 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2624...  Training loss: 12605.2246...  7.6678 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2625...  Training loss: 12295.0977...  6.3900 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2626...  Training loss: 12818.8926...  6.9174 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2627...  Training loss: 12518.2871...  6.9889 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2628...  Training loss: 12548.1055...  6.9161 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2629...  Training loss: 12403.1895...  7.7161 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2630...  Training loss: 12431.6514...  6.3752 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2631...  Training loss: 12545.9785...  7.8139 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2632...  Training loss: 12265.7324...  6.5756 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2633...  Training loss: 12246.8203...  6.3280 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2634...  Training loss: 12750.7520...  7.4939 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2635...  Training loss: 12471.0742...  6.2372 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2636...  Training loss: 12799.8447...  7.3258 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2637...  Training loss: 12644.8477...  6.5352 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2638...  Training loss: 12559.6270...  6.2467 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2639...  Training loss: 12460.0615...  7.4112 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2640...  Training loss: 12542.2656...  6.6570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2641...  Training loss: 12674.6680...  7.6699 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2642...  Training loss: 12271.8623...  6.3245 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2643...  Training loss: 12485.8203...  6.2934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2644...  Training loss: 12263.7529...  7.4674 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2645...  Training loss: 12898.0039...  6.2638 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2646...  Training loss: 12537.9082...  7.8388 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2647...  Training loss: 12748.9980...  6.3143 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2648...  Training loss: 12301.5391...  6.3867 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2649...  Training loss: 12462.6836...  7.6049 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2650...  Training loss: 12600.8105...  6.2773 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2651...  Training loss: 12430.5996...  7.4849 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2652...  Training loss: 12338.5664...  6.3221 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2653...  Training loss: 12233.0615...  6.3797 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2654...  Training loss: 12587.1934...  7.3519 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2655...  Training loss: 12130.2188...  6.2868 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2656...  Training loss: 12499.3223...  7.3067 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2657...  Training loss: 12307.6992...  6.5403 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2658...  Training loss: 12509.3926...  6.3243 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2659...  Training loss: 12097.7910...  7.4486 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2660...  Training loss: 12376.0781...  6.2945 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2661...  Training loss: 12181.1943...  7.1845 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2662...  Training loss: 12200.2178...  6.6396 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2663...  Training loss: 12198.5928...  6.2285 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2664...  Training loss: 12460.0977...  7.5870 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2665...  Training loss: 12134.4238...  6.2667 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2666...  Training loss: 12279.9697...  7.1924 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2667...  Training loss: 12032.5127...  6.7235 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2668...  Training loss: 12215.4170...  6.3722 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2669...  Training loss: 12301.6270...  7.6255 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2670...  Training loss: 12441.6338...  6.1866 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2671...  Training loss: 12394.9463...  6.5245 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2672...  Training loss: 12170.7344...  7.3206 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2673...  Training loss: 12234.1230...  6.3072 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2674...  Training loss: 12103.9570...  7.5799 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2675...  Training loss: 12331.6250...  6.3958 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2676...  Training loss: 12369.3311...  6.5174 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2677...  Training loss: 12406.7969...  7.4275 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2678...  Training loss: 12314.0000...  6.2905 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2679...  Training loss: 12364.6719...  7.5930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2680...  Training loss: 12435.3574...  6.2329 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2681...  Training loss: 12336.2559...  6.3125 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2682...  Training loss: 12476.3271...  7.6130 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2683...  Training loss: 12296.7734...  6.2524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2684...  Training loss: 12545.3652...  7.6473 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2685...  Training loss: 12328.6895...  6.3644 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2686...  Training loss: 12555.1650...  6.4015 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2687...  Training loss: 12464.8066...  7.5783 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2688...  Training loss: 12338.4541...  6.2894 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2689...  Training loss: 12180.9307...  7.6026 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2690...  Training loss: 12113.8340...  6.4176 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2691...  Training loss: 12569.4414...  6.2995 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2692...  Training loss: 12554.0762...  7.4955 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2693...  Training loss: 12435.9404...  6.3102 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2694...  Training loss: 12420.7539...  7.4477 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2695...  Training loss: 12403.7383...  6.2443 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2696...  Training loss: 12090.0195...  6.3105 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2697...  Training loss: 12019.3242...  7.6076 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2698...  Training loss: 12437.4717...  6.3046 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2699...  Training loss: 12371.1465...  7.6240 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2700...  Training loss: 12079.2158...  6.3309 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2701...  Training loss: 12438.0977...  6.1682 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2702...  Training loss: 12355.9629...  7.5231 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2703...  Training loss: 12213.9775...  6.3384 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2704...  Training loss: 11960.3574...  7.5210 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2705...  Training loss: 11928.2812...  6.2926 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2706...  Training loss: 12150.4570...  6.2243 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2707...  Training loss: 12523.3184...  7.4832 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2708...  Training loss: 12456.4102...  6.9819 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2709...  Training loss: 12404.0703...  7.9035 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2710...  Training loss: 12346.4219...  6.3690 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2711...  Training loss: 12610.1309...  6.1511 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2712...  Training loss: 12566.5566...  7.4800 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2713...  Training loss: 12378.0771...  6.1487 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2714...  Training loss: 12423.2578...  7.5329 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2715...  Training loss: 12772.9922...  6.3852 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2716...  Training loss: 12523.3438...  6.4452 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2717...  Training loss: 12321.8203...  7.5751 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2718...  Training loss: 12631.3184...  6.4124 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2719...  Training loss: 12241.0898...  7.7304 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2720...  Training loss: 12545.9033...  6.4268 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2721...  Training loss: 12438.2676...  6.3512 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2722...  Training loss: 12745.2500...  7.4109 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2723...  Training loss: 12701.6680...  6.3806 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2724...  Training loss: 12444.4609...  7.4839 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2725...  Training loss: 12048.9873...  6.3095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2726...  Training loss: 12239.9238...  6.3572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2727...  Training loss: 12451.9551...  7.5652 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2728...  Training loss: 12320.4766...  6.3086 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2729...  Training loss: 12245.5684...  7.6178 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2730...  Training loss: 12245.8721...  6.6998 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2731...  Training loss: 12351.9590...  6.2445 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2732...  Training loss: 12302.2344...  7.6024 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2733...  Training loss: 12078.6504...  6.2127 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2734...  Training loss: 12608.9180...  7.6311 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2735...  Training loss: 12579.4961...  6.1743 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2736...  Training loss: 12482.4180...  6.2191 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2737...  Training loss: 12255.4365...  7.3800 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2738...  Training loss: 12369.2832...  6.4121 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2739...  Training loss: 12309.7822...  7.5893 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2740...  Training loss: 12389.1660...  6.2334 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2741...  Training loss: 12671.9844...  7.0187 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2742...  Training loss: 13082.3916...  7.4941 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2743...  Training loss: 12509.4746...  6.6475 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2744...  Training loss: 12379.3320...  8.3535 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2745...  Training loss: 12343.1094...  7.7994 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2746...  Training loss: 12429.5439...  9.0232 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2747...  Training loss: 12819.8867...  7.5403 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2748...  Training loss: 12492.2949...  7.0287 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2749...  Training loss: 12492.7168...  8.5823 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2750...  Training loss: 12137.4082...  7.6831 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2751...  Training loss: 12435.2734...  8.1727 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2752...  Training loss: 12853.0957...  6.2806 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2753...  Training loss: 12263.7607...  7.6404 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2754...  Training loss: 12277.6221...  6.8852 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2755...  Training loss: 12254.2881...  8.5296 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2756...  Training loss: 12428.8711...  6.3193 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2757...  Training loss: 12526.4404...  6.3251 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2758...  Training loss: 12403.5312...  7.4184 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2759...  Training loss: 12416.4697...  6.3952 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2760...  Training loss: 12249.2695...  7.3404 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2761...  Training loss: 12613.4238...  6.4889 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2762...  Training loss: 12254.7461...  6.2996 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2763...  Training loss: 12350.3877...  7.5510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2764...  Training loss: 12506.0566...  6.2844 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2765...  Training loss: 12192.5391...  7.4534 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2766...  Training loss: 12204.0938...  6.3915 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2767...  Training loss: 12315.1738...  6.2660 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2768...  Training loss: 12200.0869...  7.5541 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2769...  Training loss: 11998.6074...  6.2639 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2770...  Training loss: 12401.8281...  7.5256 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2771...  Training loss: 12247.3994...  6.4765 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2772...  Training loss: 12311.8193...  6.3409 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2773...  Training loss: 14219.0439...  7.5476 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2774...  Training loss: 12730.3965...  6.2702 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2775...  Training loss: 12446.7598...  7.4518 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2776...  Training loss: 12625.0869...  6.5196 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2777...  Training loss: 12264.0205...  6.3447 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2778...  Training loss: 12082.4590...  7.5666 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2779...  Training loss: 12265.8369...  6.3173 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2780...  Training loss: 12366.6963...  7.3172 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2781...  Training loss: 12560.3271...  6.4193 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2782...  Training loss: 12211.0000...  6.3975 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2783...  Training loss: 12203.9795...  7.6525 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2784...  Training loss: 12337.3115...  6.3368 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2785...  Training loss: 12252.9180...  7.2409 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2786...  Training loss: 12492.7539...  6.6552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2787...  Training loss: 12165.1768...  6.3704 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2788...  Training loss: 12106.2725...  7.6354 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2789...  Training loss: 12558.2461...  6.2921 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2790...  Training loss: 12623.8672...  7.3697 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2791...  Training loss: 12271.9404...  6.6250 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2792...  Training loss: 12685.2715...  6.3601 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2793...  Training loss: 12254.8574...  8.8430 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2794...  Training loss: 12414.4980...  6.3603 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2795...  Training loss: 12355.5762...  7.5031 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2796...  Training loss: 12468.6982...  6.4674 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2797...  Training loss: 12394.8574...  8.1873 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2798...  Training loss: 12041.7070...  7.7589 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2799...  Training loss: 12162.5732...  6.2852 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2800...  Training loss: 12542.3408...  7.5364 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2801...  Training loss: 12450.3799...  6.2325 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2802...  Training loss: 12515.3330...  7.0818 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2803...  Training loss: 12255.5752...  6.7011 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2804...  Training loss: 12169.1484...  6.2435 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2805...  Training loss: 12418.0645...  8.5110 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2806...  Training loss: 12406.5176...  6.3357 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2807...  Training loss: 12146.6992...  7.4521 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2808...  Training loss: 12376.2168...  6.2722 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2809...  Training loss: 12055.7051...  6.2873 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2810...  Training loss: 11935.6328...  7.4619 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2811...  Training loss: 11975.9629...  10.4205 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2812...  Training loss: 12072.9795...  9.6795 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2813...  Training loss: 12102.5889...  6.5008 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2814...  Training loss: 12740.5605...  7.8671 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2815...  Training loss: 12207.5635...  6.4473 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2816...  Training loss: 12151.5625...  7.5327 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2817...  Training loss: 12414.0264...  6.9122 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2818...  Training loss: 12044.2568...  6.5413 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2819...  Training loss: 12228.4238...  7.8384 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2820...  Training loss: 12335.1006...  6.4617 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2821...  Training loss: 12344.8047...  7.8779 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2822...  Training loss: 12515.8867...  6.5725 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2823...  Training loss: 12072.9199...  6.6253 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2824...  Training loss: 12707.0820...  7.7032 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2825...  Training loss: 12282.3613...  6.4861 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2826...  Training loss: 12397.6152...  9.6299 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2827...  Training loss: 12303.8105...  9.6089 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2828...  Training loss: 12302.5449...  9.5819 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2829...  Training loss: 12525.2471...  8.3454 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2830...  Training loss: 12219.3369...  10.2136 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2831...  Training loss: 12087.4102...  6.9119 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2832...  Training loss: 12580.4414...  10.1986 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2833...  Training loss: 12254.7451...  5.8663 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2834...  Training loss: 12794.5146...  6.4217 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2835...  Training loss: 12625.5879...  7.9168 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2836...  Training loss: 12503.3281...  6.4970 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2837...  Training loss: 12295.5859...  7.0734 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2838...  Training loss: 12388.8115...  5.8001 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2839...  Training loss: 12567.9062...  6.6363 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2840...  Training loss: 12198.2861...  6.2743 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2841...  Training loss: 12403.0059...  5.7999 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2842...  Training loss: 12180.2246...  6.8877 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2843...  Training loss: 12667.2676...  5.8002 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2844...  Training loss: 12524.5469...  5.7351 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2845...  Training loss: 12686.1875...  6.7816 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2846...  Training loss: 12183.0527...  5.7774 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2847...  Training loss: 12389.4131...  5.7775 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2848...  Training loss: 12503.2725...  7.7071 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2849...  Training loss: 12295.2480...  5.8287 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2850...  Training loss: 12191.8027...  7.7892 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2851...  Training loss: 11979.4629...  7.8188 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2852...  Training loss: 12505.2373...  6.6251 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2853...  Training loss: 12045.9688...  7.3173 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2854...  Training loss: 12418.8828...  7.2662 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2855...  Training loss: 12191.9160...  8.1644 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2856...  Training loss: 12361.9717...  8.1558 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2857...  Training loss: 12145.1055...  8.0327 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2858...  Training loss: 12227.4863...  5.8964 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2859...  Training loss: 12096.5596...  6.8769 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2860...  Training loss: 12083.2256...  7.8187 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2861...  Training loss: 11983.9844...  5.7472 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2862...  Training loss: 12349.6943...  6.8970 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2863...  Training loss: 12082.0459...  5.9003 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2864...  Training loss: 12213.9062...  5.8210 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2865...  Training loss: 12021.7139...  6.8738 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2866...  Training loss: 11912.3340...  5.6854 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2867...  Training loss: 12136.7217...  7.5279 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2868...  Training loss: 12360.0918...  6.7375 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2869...  Training loss: 12321.1875...  5.7930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2870...  Training loss: 12020.6328...  8.2884 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2871...  Training loss: 12174.2500...  8.7785 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2872...  Training loss: 11996.2461...  6.7846 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2873...  Training loss: 12298.4512...  5.9744 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2874...  Training loss: 12219.9014...  5.7794 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2875...  Training loss: 12363.4990...  6.8553 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2876...  Training loss: 12196.5664...  5.7946 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2877...  Training loss: 12326.0332...  6.2136 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2878...  Training loss: 12399.5938...  9.2493 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2879...  Training loss: 12304.3564...  6.4953 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2880...  Training loss: 12351.0059...  7.9427 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2881...  Training loss: 12201.3066...  6.4211 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2882...  Training loss: 12571.0244...  7.4775 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2883...  Training loss: 12198.2988...  6.7370 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2884...  Training loss: 12514.1582...  6.6051 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2885...  Training loss: 12417.9062...  6.9607 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2886...  Training loss: 12245.0820...  6.5603 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2887...  Training loss: 12146.7227...  7.7409 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2888...  Training loss: 12039.6348...  6.6654 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2889...  Training loss: 12410.2012...  6.5026 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2890...  Training loss: 12257.3984...  7.7255 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2891...  Training loss: 12363.8320...  6.7528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2892...  Training loss: 12297.6025...  9.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2893...  Training loss: 12281.0098...  6.5982 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2894...  Training loss: 11985.2393...  7.0920 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2895...  Training loss: 11794.7695...  7.0479 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2896...  Training loss: 12290.2891...  6.3702 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2897...  Training loss: 12273.1641...  7.7628 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2898...  Training loss: 11998.3633...  6.4392 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2899...  Training loss: 12329.6133...  8.7881 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2900...  Training loss: 12227.7920...  8.7245 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2901...  Training loss: 12088.7119...  9.1587 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2902...  Training loss: 11825.4082...  7.1918 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2903...  Training loss: 11777.4805...  7.5653 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2904...  Training loss: 12096.6543...  7.3458 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2905...  Training loss: 12614.2305...  5.8448 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2906...  Training loss: 12332.8320...  6.7764 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2907...  Training loss: 12298.9727...  5.8507 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2908...  Training loss: 12251.1289...  7.0977 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2909...  Training loss: 12539.3154...  7.1405 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2910...  Training loss: 12366.8965...  5.7717 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2911...  Training loss: 12374.1006...  6.7727 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2912...  Training loss: 12377.4297...  7.3700 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2913...  Training loss: 12718.6562...  6.2910 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2914...  Training loss: 12541.8770...  6.8945 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2915...  Training loss: 12200.5234...  5.7496 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2916...  Training loss: 12558.0234...  6.3781 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2917...  Training loss: 12106.7520...  6.5860 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2918...  Training loss: 12536.8604...  7.0742 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2919...  Training loss: 12368.4688...  6.9889 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2920...  Training loss: 12564.5186...  5.7795 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2921...  Training loss: 12592.9287...  6.8650 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2922...  Training loss: 12270.9893...  7.5690 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2923...  Training loss: 12020.3711...  6.5124 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2924...  Training loss: 12123.2139...  9.9412 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2925...  Training loss: 12378.9609...  9.1363 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2926...  Training loss: 12151.5957...  9.1748 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2927...  Training loss: 12196.7422...  9.7164 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2928...  Training loss: 12192.1973...  8.4637 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2929...  Training loss: 12228.0957...  7.2256 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2930...  Training loss: 12140.5078...  8.4292 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2931...  Training loss: 11972.9922...  6.4950 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2932...  Training loss: 12521.7939...  7.8493 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2933...  Training loss: 12579.9473...  6.5075 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2934...  Training loss: 12401.2041...  6.5219 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2935...  Training loss: 12287.5996...  7.6710 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2936...  Training loss: 12337.7090...  6.6144 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2937...  Training loss: 12306.0840...  7.7562 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2938...  Training loss: 12255.1201...  6.7083 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2939...  Training loss: 12501.6875...  12.0410 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2940...  Training loss: 13010.5488...  10.6166 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2941...  Training loss: 12444.4219...  9.5410 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2942...  Training loss: 12319.5410...  7.8811 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2943...  Training loss: 12271.3340...  9.6809 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2944...  Training loss: 12271.0586...  7.0338 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2945...  Training loss: 12665.3965...  10.7543 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2946...  Training loss: 12271.7598...  8.3917 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2947...  Training loss: 12311.3867...  8.3152 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2948...  Training loss: 12180.0938...  6.6920 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2949...  Training loss: 12382.1543...  9.0387 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2950...  Training loss: 12612.1719...  7.6217 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2951...  Training loss: 12123.9941...  8.0675 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2952...  Training loss: 12056.0840...  7.5252 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2953...  Training loss: 12260.2432...  9.2329 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2954...  Training loss: 12423.3037...  7.7016 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2955...  Training loss: 12299.5684...  6.5196 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2956...  Training loss: 12251.8438...  7.6492 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2957...  Training loss: 12215.5869...  8.2734 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2958...  Training loss: 12073.4355...  9.0477 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2959...  Training loss: 12608.9717...  6.8200 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2960...  Training loss: 12150.1416...  9.2196 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2961...  Training loss: 12164.8379...  12.3832 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2962...  Training loss: 12245.3643...  15.6387 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2963...  Training loss: 12080.0391...  17.4237 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2964...  Training loss: 12088.7666...  16.5692 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2965...  Training loss: 12269.3750...  10.0924 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2966...  Training loss: 12174.2393...  9.9482 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2967...  Training loss: 11949.9941...  7.7770 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2968...  Training loss: 12310.9033...  9.8387 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2969...  Training loss: 12212.9844...  7.4689 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2970...  Training loss: 12045.8574...  8.7652 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2971...  Training loss: 14103.6904...  6.0387 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2972...  Training loss: 12608.6689...  6.9036 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2973...  Training loss: 12328.6758...  6.3187 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2974...  Training loss: 12581.4189...  5.7551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2975...  Training loss: 12259.0723...  6.9194 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2976...  Training loss: 11953.4043...  5.8167 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2977...  Training loss: 12173.0703...  6.0228 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2978...  Training loss: 12313.7559...  7.2331 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2979...  Training loss: 12423.5801...  6.1253 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2980...  Training loss: 12088.6240...  6.6451 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2981...  Training loss: 12110.4580...  6.1108 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2982...  Training loss: 12220.5967...  5.9957 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2983...  Training loss: 12165.7051...  7.7338 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2984...  Training loss: 12387.7773...  5.8076 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2985...  Training loss: 11985.3242...  5.6662 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2986...  Training loss: 11994.6484...  7.1846 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2987...  Training loss: 12331.6416...  6.3965 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2988...  Training loss: 12431.2412...  10.1691 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2989...  Training loss: 12181.8086...  7.7192 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2990...  Training loss: 12530.0713...  10.1343 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2991...  Training loss: 12278.5625...  8.0879 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2992...  Training loss: 12330.1680...  8.9930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2993...  Training loss: 12190.3672...  7.6984 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2994...  Training loss: 12450.6494...  9.2974 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2995...  Training loss: 12431.1562...  7.7325 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2996...  Training loss: 11949.2256...  9.1739 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2997...  Training loss: 12080.1426...  7.9298 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2998...  Training loss: 12349.2070...  9.1141 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2999...  Training loss: 12347.4619...  7.5482 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3000...  Training loss: 12536.0664...  8.7505 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3001...  Training loss: 12224.9395...  10.0332 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3002...  Training loss: 12025.7686...  12.2493 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3003...  Training loss: 12287.2090...  13.7087 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3004...  Training loss: 12351.9414...  14.3766 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3005...  Training loss: 12204.1680...  14.4104 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3006...  Training loss: 12241.3223...  13.8147 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3007...  Training loss: 12089.0449...  13.6695 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3008...  Training loss: 11909.0410...  13.0601 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3009...  Training loss: 11885.4023...  13.5835 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3010...  Training loss: 12108.9150...  11.5699 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3011...  Training loss: 11957.0879...  13.0200 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3012...  Training loss: 12541.9795...  13.5178 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3013...  Training loss: 12186.9893...  12.5948 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3014...  Training loss: 11979.0312...  13.5180 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3015...  Training loss: 12374.2109...  13.9188 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3016...  Training loss: 11955.4336...  12.7385 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3017...  Training loss: 12111.8408...  11.7318 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3018...  Training loss: 12221.6484...  13.7943 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3019...  Training loss: 12167.7305...  12.6095 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3020...  Training loss: 12365.2500...  13.2892 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3021...  Training loss: 11940.3916...  14.9881 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3022...  Training loss: 12595.2324...  13.8708 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3023...  Training loss: 12310.2568...  11.3073 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3024...  Training loss: 12284.9590...  14.7306 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3025...  Training loss: 12233.0498...  14.4587 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3026...  Training loss: 12266.4805...  10.8133 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3027...  Training loss: 12239.0459...  13.2646 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3028...  Training loss: 12094.5967...  13.2987 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3029...  Training loss: 11941.0625...  12.1933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3030...  Training loss: 12536.7910...  11.8920 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3031...  Training loss: 12260.7471...  9.4765 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3032...  Training loss: 12668.6055...  9.8496 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3033...  Training loss: 12432.0020...  10.9585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3034...  Training loss: 12353.0059...  8.8250 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3035...  Training loss: 12239.2188...  10.9897 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3036...  Training loss: 12258.5176...  11.0985 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3037...  Training loss: 12407.6211...  9.4161 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3038...  Training loss: 12066.6992...  7.8308 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3039...  Training loss: 12250.6035...  9.2507 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3040...  Training loss: 12076.1367...  7.6708 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3041...  Training loss: 12605.3535...  9.1417 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3042...  Training loss: 12407.5693...  7.8170 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3043...  Training loss: 12582.1172...  9.3547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3044...  Training loss: 12016.6836...  7.7519 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3045...  Training loss: 12254.8252...  9.6455 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3046...  Training loss: 12446.3066...  20.8295 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3047...  Training loss: 12267.5156...  17.8508 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3048...  Training loss: 12146.5449...  10.7016 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3049...  Training loss: 11900.2090...  14.0530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3050...  Training loss: 12305.9688...  8.9288 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3051...  Training loss: 11892.6426...  13.2391 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3052...  Training loss: 12334.6367...  12.1402 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3053...  Training loss: 11959.0918...  7.5011 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3054...  Training loss: 12186.3281...  9.1081 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3055...  Training loss: 12002.7305...  7.6305 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3056...  Training loss: 12156.3945...  9.3675 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3057...  Training loss: 12055.5801...  10.6431 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3058...  Training loss: 11998.7188...  13.4536 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3059...  Training loss: 11889.6738...  8.7136 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3060...  Training loss: 12244.9395...  6.8239 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3061...  Training loss: 11983.3848...  5.7955 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3062...  Training loss: 12034.9863...  6.8201 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3063...  Training loss: 11764.9277...  5.7015 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3064...  Training loss: 11846.4355...  7.6094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3065...  Training loss: 12038.2812...  8.3131 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3066...  Training loss: 12312.4658...  8.5201 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3067...  Training loss: 12151.9189...  9.9907 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3068...  Training loss: 11856.4395...  7.4400 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3069...  Training loss: 12042.0928...  7.6726 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3070...  Training loss: 11933.4609...  9.1786 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3071...  Training loss: 12189.2012...  10.8406 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3072...  Training loss: 12161.3457...  9.3566 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3073...  Training loss: 12214.0020...  9.1307 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3074...  Training loss: 12108.3076...  5.9892 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3075...  Training loss: 12323.5156...  7.0597 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3076...  Training loss: 12151.8945...  5.7945 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3077...  Training loss: 12146.5205...  6.2222 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3078...  Training loss: 12198.9727...  6.8874 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3079...  Training loss: 12087.9268...  5.7882 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3080...  Training loss: 12458.7207...  6.8959 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3081...  Training loss: 12118.0693...  5.8017 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3082...  Training loss: 12388.9502...  6.1890 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3083...  Training loss: 12210.7109...  7.1314 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3084...  Training loss: 12126.2930...  6.0420 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3085...  Training loss: 11906.0117...  5.7973 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3086...  Training loss: 11942.0020...  6.8152 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3087...  Training loss: 12334.2412...  5.7947 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3088...  Training loss: 12272.9043...  6.7328 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3089...  Training loss: 12237.3066...  5.9616 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3090...  Training loss: 12070.2480...  5.8235 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3091...  Training loss: 12147.1523...  6.8550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3092...  Training loss: 11983.9346...  5.8258 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3093...  Training loss: 11799.0312...  5.7592 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3094...  Training loss: 12171.5430...  6.9121 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3095...  Training loss: 12101.3379...  5.8406 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3096...  Training loss: 11886.8213...  6.0666 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3097...  Training loss: 12213.7188...  6.6846 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3098...  Training loss: 12217.3867...  5.6697 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3099...  Training loss: 12040.8643...  6.7983 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3100...  Training loss: 11793.8809...  5.8678 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3101...  Training loss: 11778.2959...  5.7604 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3102...  Training loss: 11955.8359...  6.9605 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3103...  Training loss: 12403.7295...  5.7465 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3104...  Training loss: 12265.8193...  5.7972 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3105...  Training loss: 12205.0723...  6.9761 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3106...  Training loss: 12142.5957...  5.8679 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3107...  Training loss: 12524.1191...  6.3062 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3108...  Training loss: 12388.7031...  6.4201 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3109...  Training loss: 12224.4160...  5.8236 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3110...  Training loss: 12183.3203...  6.9047 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3111...  Training loss: 12695.5244...  5.7947 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3112...  Training loss: 12426.1191...  5.8168 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3113...  Training loss: 12107.4854...  6.8286 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3114...  Training loss: 12538.1729...  5.8739 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3115...  Training loss: 12007.2168...  5.7593 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3116...  Training loss: 12434.3330...  6.8452 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3117...  Training loss: 12297.1387...  5.9186 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3118...  Training loss: 12519.8438...  6.6938 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3119...  Training loss: 12388.7451...  5.9700 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3120...  Training loss: 12184.9736...  5.8738 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3121...  Training loss: 11830.7930...  7.0613 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3122...  Training loss: 12092.2471...  5.8051 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3123...  Training loss: 12366.6309...  5.8120 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3124...  Training loss: 12138.8662...  6.9251 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3125...  Training loss: 11992.5625...  5.7814 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3126...  Training loss: 12109.7568...  6.2279 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3127...  Training loss: 12100.3340...  6.3695 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3128...  Training loss: 12037.2949...  5.7689 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3129...  Training loss: 11835.9922...  6.8974 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3130...  Training loss: 12394.0986...  5.8058 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3131...  Training loss: 12433.7373...  5.8324 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3132...  Training loss: 12249.2686...  6.7839 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3133...  Training loss: 12126.8672...  5.8379 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3134...  Training loss: 12101.5801...  5.9080 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3135...  Training loss: 12171.9609...  6.8986 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3136...  Training loss: 12211.0723...  5.7598 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3137...  Training loss: 12445.3975...  6.5638 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3138...  Training loss: 12750.6016...  6.1694 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3139...  Training loss: 12367.0879...  5.8540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3140...  Training loss: 12204.5088...  6.9334 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3141...  Training loss: 12151.3408...  5.7758 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3142...  Training loss: 12161.7783...  5.7468 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3143...  Training loss: 12588.4629...  6.8628 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3144...  Training loss: 12313.8799...  5.7448 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3145...  Training loss: 12239.1689...  5.7755 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3146...  Training loss: 11901.9902...  6.9503 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3147...  Training loss: 12260.6221...  5.7850 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3148...  Training loss: 12520.5820...  6.8794 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3149...  Training loss: 12048.8926...  5.7114 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3150...  Training loss: 12045.4199...  5.8759 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3151...  Training loss: 12118.3262...  6.8927 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3152...  Training loss: 12197.0078...  5.7736 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3153...  Training loss: 12321.4326...  5.8163 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3154...  Training loss: 12144.1074...  6.8506 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3155...  Training loss: 12122.8047...  5.7592 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3156...  Training loss: 12046.0332...  6.1939 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3157...  Training loss: 12356.6035...  6.5724 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3158...  Training loss: 12134.4561...  5.8564 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3159...  Training loss: 12060.5342...  7.0501 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3160...  Training loss: 12165.7871...  5.8084 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3161...  Training loss: 11963.9355...  5.7689 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3162...  Training loss: 11919.2012...  6.9583 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3163...  Training loss: 12157.7490...  5.7602 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3164...  Training loss: 12018.2510...  5.9047 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3165...  Training loss: 11797.6426...  6.9152 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3166...  Training loss: 12245.5000...  5.7361 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3167...  Training loss: 12120.4062...  6.7370 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3168...  Training loss: 12101.9365...  6.0176 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3169...  Training loss: 13956.6445...  5.7568 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3170...  Training loss: 12555.4824...  6.8058 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3171...  Training loss: 12197.8174...  5.6813 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3172...  Training loss: 12385.9395...  5.7799 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3173...  Training loss: 12121.6621...  6.8334 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3174...  Training loss: 11832.9590...  5.8064 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3175...  Training loss: 12094.8818...  5.7396 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3176...  Training loss: 12112.3740...  6.8123 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3177...  Training loss: 12317.9170...  5.8599 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3178...  Training loss: 12021.3301...  6.6831 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3179...  Training loss: 11969.6494...  6.1020 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3180...  Training loss: 12071.0791...  5.7877 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3181...  Training loss: 12129.3730...  6.7928 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3182...  Training loss: 12388.2852...  5.7674 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3183...  Training loss: 11992.3770...  5.7089 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3184...  Training loss: 11856.5371...  6.8057 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3185...  Training loss: 12335.2402...  5.7798 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3186...  Training loss: 12291.6543...  6.1026 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3187...  Training loss: 12098.2773...  6.7564 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3188...  Training loss: 12470.4941...  5.8410 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3189...  Training loss: 12178.4160...  6.9560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3190...  Training loss: 12343.6406...  5.7783 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3191...  Training loss: 12120.2969...  5.7327 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3192...  Training loss: 12386.1016...  6.9445 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3193...  Training loss: 12325.4971...  5.8305 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3194...  Training loss: 11878.2773...  5.7901 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3195...  Training loss: 12007.5020...  6.7803 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3196...  Training loss: 12292.4629...  5.8702 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3197...  Training loss: 12315.9541...  6.2301 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3198...  Training loss: 12535.1191...  6.4365 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3199...  Training loss: 12051.4326...  5.8596 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3200...  Training loss: 11924.8057...  6.8559 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3201...  Training loss: 12196.8115...  5.8453 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3202...  Training loss: 12292.0137...  5.7880 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3203...  Training loss: 12185.1787...  6.8835 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3204...  Training loss: 12147.2432...  5.7944 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3205...  Training loss: 11955.8145...  6.1851 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3206...  Training loss: 11769.8154...  6.4231 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3207...  Training loss: 11862.6455...  5.8655 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3208...  Training loss: 11970.7139...  6.9664 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3209...  Training loss: 11855.7412...  5.7911 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3210...  Training loss: 12562.0371...  5.7432 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3211...  Training loss: 12023.5264...  6.8256 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3212...  Training loss: 11890.5215...  5.7496 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3213...  Training loss: 12223.0488...  5.9014 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3214...  Training loss: 11869.7559...  6.8957 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3215...  Training loss: 11991.5137...  5.7665 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3216...  Training loss: 12041.2686...  6.7078 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3217...  Training loss: 12022.5859...  5.8353 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3218...  Training loss: 12331.3945...  5.7946 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3219...  Training loss: 11881.5791...  6.8689 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3220...  Training loss: 12489.0723...  5.8686 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3221...  Training loss: 12206.8340...  5.9381 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3222...  Training loss: 12172.8779...  6.9580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3223...  Training loss: 12091.0244...  5.8173 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3224...  Training loss: 12169.2842...  6.0438 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3225...  Training loss: 12216.7588...  7.0951 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3226...  Training loss: 12061.6660...  5.7882 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3227...  Training loss: 11846.4121...  6.8786 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3228...  Training loss: 12371.5195...  6.7917 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3229...  Training loss: 12168.7109...  6.9247 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3230...  Training loss: 12481.0752...  7.8643 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3231...  Training loss: 12280.1914...  6.5961 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3232...  Training loss: 12229.8945...  7.2657 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3233...  Training loss: 12137.6699...  6.9094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3234...  Training loss: 12118.8564...  7.8708 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3235...  Training loss: 12386.9521...  5.8968 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3236...  Training loss: 11957.7480...  5.9240 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3237...  Training loss: 12142.4170...  6.8638 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3238...  Training loss: 11953.3633...  6.5675 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3239...  Training loss: 12601.2148...  6.4059 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3240...  Training loss: 12298.0635...  6.9175 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3241...  Training loss: 12527.5215...  5.8693 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3242...  Training loss: 11952.8457...  6.8889 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3243...  Training loss: 12226.2568...  5.8424 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3244...  Training loss: 12302.7539...  5.7062 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3245...  Training loss: 12130.0254...  6.8570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3246...  Training loss: 11959.9131...  5.8381 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3247...  Training loss: 11807.3418...  6.5338 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3248...  Training loss: 12134.3145...  7.5088 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3249...  Training loss: 11927.2646...  5.7422 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3250...  Training loss: 12261.5215...  6.9766 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3251...  Training loss: 11868.3926...  5.7948 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3252...  Training loss: 12063.0195...  5.8142 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3253...  Training loss: 11785.7969...  6.9752 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3254...  Training loss: 12121.0352...  5.7971 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3255...  Training loss: 11926.8379...  5.9318 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3256...  Training loss: 11975.9629...  6.8222 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3257...  Training loss: 11773.7773...  5.8494 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3258...  Training loss: 12171.5781...  5.7203 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3259...  Training loss: 11952.5293...  6.8220 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3260...  Training loss: 12007.9824...  5.7615 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3261...  Training loss: 11834.0029...  5.7470 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3262...  Training loss: 11770.1904...  6.8345 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3263...  Training loss: 12009.9805...  5.8036 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3264...  Training loss: 12256.8789...  5.7393 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3265...  Training loss: 12092.0557...  6.2883 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3266...  Training loss: 11814.8086...  6.3752 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3267...  Training loss: 12032.0996...  5.8040 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3268...  Training loss: 11837.2842...  5.7838 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3269...  Training loss: 12052.1572...  7.2529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3270...  Training loss: 12049.7793...  6.2320 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3271...  Training loss: 12105.5518...  5.7649 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3272...  Training loss: 12000.3516...  6.1438 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3273...  Training loss: 12049.9814...  6.3577 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3274...  Training loss: 12091.6377...  6.7002 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3275...  Training loss: 12105.7617...  7.0992 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3276...  Training loss: 12078.7588...  6.3836 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3277...  Training loss: 12005.2266...  5.8998 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3278...  Training loss: 12244.2012...  5.6917 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3279...  Training loss: 12085.4434...  6.3292 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3280...  Training loss: 12265.7744...  6.3142 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3281...  Training loss: 12139.9648...  5.7389 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3282...  Training loss: 12040.9072...  5.8896 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3283...  Training loss: 11870.5176...  6.7220 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3284...  Training loss: 11800.8359...  5.7129 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3285...  Training loss: 12238.3320...  5.7837 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3286...  Training loss: 12135.8574...  6.7342 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3287...  Training loss: 12076.7861...  5.9943 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3288...  Training loss: 11978.4053...  5.8013 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3289...  Training loss: 12034.1602...  5.7803 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3290...  Training loss: 11817.9980...  6.7068 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3291...  Training loss: 11675.0049...  5.7513 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3292...  Training loss: 12106.0127...  5.8573 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3293...  Training loss: 11989.5186...  6.8486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3294...  Training loss: 11784.6406...  5.7677 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3295...  Training loss: 12072.1191...  5.7008 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3296...  Training loss: 12104.5234...  6.8083 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3297...  Training loss: 11972.3740...  5.7364 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3298...  Training loss: 11660.1641...  5.7600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3299...  Training loss: 11533.6084...  6.8880 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3300...  Training loss: 11874.5693...  5.7688 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3301...  Training loss: 12215.1992...  5.7249 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3302...  Training loss: 12023.6973...  6.7304 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3303...  Training loss: 11987.9434...  5.8864 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3304...  Training loss: 12033.5713...  5.8159 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3305...  Training loss: 12302.9473...  6.2554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3306...  Training loss: 12202.8848...  6.3741 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3307...  Training loss: 12118.3027...  5.6603 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3308...  Training loss: 12135.6328...  5.7781 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3309...  Training loss: 12529.7559...  6.8480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3310...  Training loss: 12281.2959...  5.7891 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3311...  Training loss: 12016.9609...  5.7956 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3312...  Training loss: 12359.9395...  6.8061 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3313...  Training loss: 11906.6348...  5.7381 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3314...  Training loss: 12280.7734...  5.7783 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3315...  Training loss: 12214.2969...  6.7374 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3316...  Training loss: 12391.8711...  5.8939 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3317...  Training loss: 12337.0020...  5.7278 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3318...  Training loss: 12094.9004...  6.6004 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3319...  Training loss: 11793.2627...  5.9608 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3320...  Training loss: 11936.3486...  5.7316 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3321...  Training loss: 12201.0605...  5.7455 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3322...  Training loss: 12034.3447...  6.7416 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3323...  Training loss: 11980.4395...  5.7725 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3324...  Training loss: 12108.9912...  5.8229 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3325...  Training loss: 12114.7100...  6.7288 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3326...  Training loss: 12017.9551...  5.8091 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3327...  Training loss: 11763.0137...  5.7536 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3328...  Training loss: 12273.9609...  6.8164 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3329...  Training loss: 12362.0801...  5.8232 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3330...  Training loss: 12200.8271...  5.7186 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3331...  Training loss: 11992.5938...  5.7682 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3332...  Training loss: 12143.7646...  6.7579 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3333...  Training loss: 12090.6660...  5.7490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3334...  Training loss: 12151.8525...  5.7438 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3335...  Training loss: 12406.3154...  6.7149 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3336...  Training loss: 12687.1455...  5.8690 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3337...  Training loss: 12338.3682...  5.7212 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3338...  Training loss: 12162.3965...  6.6019 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3339...  Training loss: 12019.8652...  6.1199 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3340...  Training loss: 12153.1914...  5.7668 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3341...  Training loss: 12514.4453...  5.7529 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3342...  Training loss: 12192.7520...  6.7078 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3343...  Training loss: 12128.2656...  5.9078 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3344...  Training loss: 11886.3574...  5.8327 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3345...  Training loss: 12135.6777...  6.9238 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3346...  Training loss: 12416.0957...  5.7831 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3347...  Training loss: 11969.7939...  5.6622 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3348...  Training loss: 11870.7266...  6.5618 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3349...  Training loss: 11954.8301...  5.9446 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3350...  Training loss: 12138.7988...  5.7491 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3351...  Training loss: 12267.2217...  5.7967 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3352...  Training loss: 12097.9844...  6.7508 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3353...  Training loss: 12120.9873...  5.7564 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3354...  Training loss: 12006.7764...  5.6500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3355...  Training loss: 12371.7500...  6.8083 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3356...  Training loss: 12000.4150...  5.8183 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3357...  Training loss: 11972.3223...  5.6855 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3358...  Training loss: 12046.5264...  6.8009 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3359...  Training loss: 11928.5781...  5.7272 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3360...  Training loss: 11951.1299...  5.7844 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3361...  Training loss: 12098.3350...  6.7261 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3362...  Training loss: 11931.1279...  5.9553 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3363...  Training loss: 11663.8076...  5.7389 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3364...  Training loss: 12069.1074...  6.4763 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3365...  Training loss: 12075.8418...  5.9985 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3366...  Training loss: 12016.7656...  5.8238 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3367...  Training loss: 13972.3623...  5.9210 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3368...  Training loss: 12409.8730...  6.8328 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3369...  Training loss: 12229.2891...  5.7793 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3370...  Training loss: 12307.2705...  6.2678 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3371...  Training loss: 12064.6680...  6.4823 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3372...  Training loss: 11772.6162...  5.7930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3373...  Training loss: 11991.9307...  5.8903 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3374...  Training loss: 12186.7969...  7.0981 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3375...  Training loss: 12241.0469...  5.9708 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3376...  Training loss: 11913.6104...  5.7307 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3377...  Training loss: 12065.5312...  5.7125 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3378...  Training loss: 12059.8164...  6.8744 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3379...  Training loss: 12032.4902...  5.7776 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3380...  Training loss: 12301.8359...  5.7647 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3381...  Training loss: 11857.2578...  6.7652 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3382...  Training loss: 11858.3223...  5.7620 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3383...  Training loss: 12318.5820...  5.7487 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3384...  Training loss: 12258.3975...  6.1886 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3385...  Training loss: 12016.6953...  6.4825 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3386...  Training loss: 12455.8350...  5.7877 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3387...  Training loss: 12095.1016...  5.7094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3388...  Training loss: 12256.8838...  6.7729 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3389...  Training loss: 12062.7109...  5.7371 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3390...  Training loss: 12270.9531...  5.8213 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3391...  Training loss: 12093.1455...  6.6423 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3392...  Training loss: 11745.7266...  5.9937 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3393...  Training loss: 11885.0293...  5.7157 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3394...  Training loss: 12190.2627...  5.7190 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3395...  Training loss: 12054.1445...  6.7730 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3396...  Training loss: 12223.0430...  5.7419 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3397...  Training loss: 11930.6738...  5.8647 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3398...  Training loss: 11935.1016...  6.8151 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3399...  Training loss: 12065.8789...  5.7523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3400...  Training loss: 12134.5791...  5.7467 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3401...  Training loss: 11890.9238...  6.7773 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3402...  Training loss: 12163.7227...  5.8488 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3403...  Training loss: 11806.5352...  5.7113 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3404...  Training loss: 11539.8809...  6.7373 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3405...  Training loss: 11555.5928...  5.7796 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3406...  Training loss: 11828.6709...  5.6888 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3407...  Training loss: 11833.1641...  6.4028 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3408...  Training loss: 12517.0693...  6.1380 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3409...  Training loss: 11868.5576...  5.7803 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3410...  Training loss: 11775.9844...  5.8435 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3411...  Training loss: 12170.8320...  6.7594 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3412...  Training loss: 11803.9492...  5.8113 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3413...  Training loss: 11995.4932...  5.7429 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3414...  Training loss: 12062.7070...  6.7214 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3415...  Training loss: 12038.2451...  5.9479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3416...  Training loss: 12186.5283...  5.7113 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3417...  Training loss: 11759.6211...  5.7662 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3418...  Training loss: 12390.6055...  6.7521 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3419...  Training loss: 12082.5410...  5.7990 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3420...  Training loss: 12134.9727...  5.7712 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3421...  Training loss: 11991.6924...  6.7368 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3422...  Training loss: 12002.0938...  5.7369 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3423...  Training loss: 12088.6602...  5.7718 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3424...  Training loss: 11928.1738...  6.8348 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3425...  Training loss: 11722.9746...  5.8642 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3426...  Training loss: 12244.6895...  5.7738 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3427...  Training loss: 12060.5176...  5.7852 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3428...  Training loss: 12414.5205...  6.7399 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3429...  Training loss: 12261.3877...  5.7349 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3430...  Training loss: 12140.0820...  5.7709 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3431...  Training loss: 12074.1670...  6.3996 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3432...  Training loss: 12025.1299...  6.1847 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3433...  Training loss: 12311.9980...  5.7575 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3434...  Training loss: 11904.3047...  5.7699 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3435...  Training loss: 12095.8047...  6.7396 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3436...  Training loss: 11807.2842...  5.7050 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3437...  Training loss: 12420.9736...  5.8435 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3438...  Training loss: 12051.9775...  6.8446 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3439...  Training loss: 12368.4951...  5.7315 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3440...  Training loss: 11922.1836...  5.7457 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3441...  Training loss: 12117.2480...  6.6572 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3442...  Training loss: 12126.7529...  5.8279 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3443...  Training loss: 12160.2715...  5.7834 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3444...  Training loss: 11922.7930...  6.4837 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3445...  Training loss: 11756.6299...  6.2758 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3446...  Training loss: 12055.1299...  5.7245 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3447...  Training loss: 11704.4258...  5.6950 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3448...  Training loss: 12121.4463...  6.8046 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3449...  Training loss: 11842.0117...  5.8199 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3450...  Training loss: 12019.4590...  5.7616 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3451...  Training loss: 11756.0371...  6.8277 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3452...  Training loss: 11973.4863...  5.7909 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3453...  Training loss: 11840.5088...  5.7577 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3454...  Training loss: 11875.7480...  6.7337 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3455...  Training loss: 11702.1367...  5.8777 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3456...  Training loss: 11993.5137...  5.7824 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3457...  Training loss: 11791.1504...  6.2995 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3458...  Training loss: 11885.5430...  6.1466 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3459...  Training loss: 11731.7080...  5.8404 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3460...  Training loss: 11727.9893...  5.8053 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3461...  Training loss: 11846.6846...  6.7058 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3462...  Training loss: 12173.5498...  5.7663 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3463...  Training loss: 12031.0215...  5.7228 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3464...  Training loss: 11779.6279...  5.7910 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3465...  Training loss: 11871.4668...  6.7955 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3466...  Training loss: 11756.7217...  5.8317 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3467...  Training loss: 12006.4863...  5.7952 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3468...  Training loss: 12024.1592...  6.8265 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3469...  Training loss: 11981.5098...  5.7861 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3470...  Training loss: 11944.8682...  5.6780 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3471...  Training loss: 12048.0361...  6.8519 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3472...  Training loss: 11911.7920...  5.7203 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3473...  Training loss: 12008.2461...  5.7210 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3474...  Training loss: 12079.8652...  6.7098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3475...  Training loss: 11930.5977...  5.7623 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3476...  Training loss: 12172.2041...  5.8428 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3477...  Training loss: 11950.4590...  6.1102 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3478...  Training loss: 12164.7129...  6.4850 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3479...  Training loss: 12058.5469...  5.7838 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3480...  Training loss: 12135.8828...  5.6698 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3481...  Training loss: 11822.0293...  6.8055 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3482...  Training loss: 11681.7344...  5.7411 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3483...  Training loss: 12114.4043...  5.7659 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3484...  Training loss: 12052.8105...  6.7160 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3485...  Training loss: 12026.8906...  5.8377 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3486...  Training loss: 11979.0518...  6.1369 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3487...  Training loss: 12049.4824...  7.3048 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3488...  Training loss: 11729.1074...  6.7150 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3489...  Training loss: 11616.7324...  5.7957 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3490...  Training loss: 11997.5840...  5.7245 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3491...  Training loss: 11964.7480...  7.7406 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3492...  Training loss: 11714.6680...  6.5769 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3493...  Training loss: 12033.5078...  5.7748 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3494...  Training loss: 11952.9492...  6.7641 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3495...  Training loss: 11824.0020...  6.3964 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3496...  Training loss: 11533.7373...  6.7117 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3497...  Training loss: 11611.6182...  7.0539 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3498...  Training loss: 11730.0254...  5.7420 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3499...  Training loss: 12187.5820...  5.9091 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3500...  Training loss: 12074.2871...  7.8178 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3501...  Training loss: 12057.8633...  6.5211 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3502...  Training loss: 11946.1182...  5.9097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3503...  Training loss: 12277.9180...  6.6524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3504...  Training loss: 12102.5332...  6.7053 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3505...  Training loss: 12139.7480...  6.8902 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3506...  Training loss: 12125.3652...  5.9998 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3507...  Training loss: 12379.8418...  5.9094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3508...  Training loss: 12208.3066...  6.6203 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3509...  Training loss: 11883.5254...  7.3315 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3510...  Training loss: 12322.6172...  6.6903 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3511...  Training loss: 11870.4521...  6.4537 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3512...  Training loss: 12226.6035...  5.7761 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3513...  Training loss: 12107.5576...  6.0094 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3514...  Training loss: 12276.5547...  7.0798 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3515...  Training loss: 12304.9043...  5.9292 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3516...  Training loss: 12008.5684...  5.8580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3517...  Training loss: 11705.5605...  6.5944 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3518...  Training loss: 11932.4375...  6.0338 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3519...  Training loss: 12103.8701...  5.9781 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3520...  Training loss: 11920.6553...  6.0588 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3521...  Training loss: 11782.1689...  6.6200 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3522...  Training loss: 11984.7490...  5.8112 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3523...  Training loss: 12046.6934...  6.4219 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3524...  Training loss: 11887.1494...  6.6335 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3525...  Training loss: 11682.6787...  5.7499 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3526...  Training loss: 12258.5771...  5.8051 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3527...  Training loss: 12270.6465...  6.6355 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3528...  Training loss: 12209.6855...  6.0012 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3529...  Training loss: 11928.5283...  5.8163 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3530...  Training loss: 11957.0459...  5.8450 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3531...  Training loss: 11978.3633...  6.8781 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3532...  Training loss: 12039.3711...  5.9772 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3533...  Training loss: 12332.7871...  6.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3534...  Training loss: 12567.8760...  7.0351 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3535...  Training loss: 12233.3076...  5.8478 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3536...  Training loss: 12086.9561...  5.7825 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3537...  Training loss: 12011.8096...  6.8789 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3538...  Training loss: 11931.6172...  5.8396 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3539...  Training loss: 12357.1387...  5.7710 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3540...  Training loss: 12064.7871...  6.7989 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3541...  Training loss: 12084.5771...  5.8840 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3542...  Training loss: 11785.7266...  5.7553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3543...  Training loss: 12056.8535...  6.6686 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3544...  Training loss: 12329.2920...  5.9936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3545...  Training loss: 11944.0576...  5.7871 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3546...  Training loss: 11824.3535...  5.7175 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3547...  Training loss: 11938.5615...  6.7327 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3548...  Training loss: 12057.6123...  5.7735 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3549...  Training loss: 12073.9766...  5.7755 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3550...  Training loss: 12035.8730...  7.0113 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3551...  Training loss: 11980.9346...  5.6599 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3552...  Training loss: 11776.9629...  5.7766 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3553...  Training loss: 12278.4258...  6.8165 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3554...  Training loss: 11896.4033...  5.7323 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3555...  Training loss: 11890.5283...  5.8212 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3556...  Training loss: 11962.5938...  6.6989 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3557...  Training loss: 11675.5605...  5.8993 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3558...  Training loss: 11857.8398...  5.8360 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3559...  Training loss: 11955.3438...  5.7975 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3560...  Training loss: 11939.8320...  6.6604 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3561...  Training loss: 11596.5527...  5.7650 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3562...  Training loss: 11924.1816...  5.7575 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3563...  Training loss: 11949.3125...  6.7406 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3564...  Training loss: 11901.2559...  5.8394 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3565...  Training loss: 13842.5762...  5.7654 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3566...  Training loss: 12318.0859...  5.7096 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3567...  Training loss: 12140.1426...  6.7796 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3568...  Training loss: 12266.7559...  5.7719 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3569...  Training loss: 11932.1230...  5.8332 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3570...  Training loss: 11662.3867...  6.7487 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3571...  Training loss: 11926.4316...  5.6830 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3572...  Training loss: 12080.3584...  5.8125 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3573...  Training loss: 12155.3711...  6.3426 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3574...  Training loss: 11816.6973...  6.2717 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3575...  Training loss: 11834.3652...  5.8200 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3576...  Training loss: 12021.0713...  5.7362 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3577...  Training loss: 11879.1172...  6.8428 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3578...  Training loss: 12088.0225...  5.7597 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3579...  Training loss: 11712.1133...  6.1630 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3580...  Training loss: 11821.0654...  6.5419 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3581...  Training loss: 12129.7861...  5.7609 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3582...  Training loss: 12244.0273...  5.8151 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3583...  Training loss: 11883.7539...  6.7187 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3584...  Training loss: 12222.7695...  5.8090 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3585...  Training loss: 12032.9258...  5.8768 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3586...  Training loss: 12140.1895...  6.6583 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3587...  Training loss: 11985.4883...  5.8698 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3588...  Training loss: 12261.7930...  5.6795 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3589...  Training loss: 12043.7090...  6.0551 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3590...  Training loss: 11599.5312...  6.4744 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3591...  Training loss: 11783.8457...  5.8649 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3592...  Training loss: 12165.4697...  5.8042 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3593...  Training loss: 12134.1084...  6.7315 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3594...  Training loss: 12228.3633...  5.7367 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3595...  Training loss: 11919.7910...  5.7615 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3596...  Training loss: 11834.5781...  5.7573 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3597...  Training loss: 11948.1143...  6.7943 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3598...  Training loss: 12035.2432...  5.7777 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3599...  Training loss: 11859.1162...  5.7862 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3600...  Training loss: 11983.2227...  6.7707 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3601...  Training loss: 11733.4121...  5.7747 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3602...  Training loss: 11511.6348...  5.8037 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3603...  Training loss: 11565.5713...  6.7995 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3604...  Training loss: 11792.4404...  5.8489 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3605...  Training loss: 11804.3594...  5.8373 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3606...  Training loss: 12321.8770...  6.5643 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3607...  Training loss: 11856.7109...  5.9659 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3608...  Training loss: 11752.7676...  5.8250 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3609...  Training loss: 12053.4707...  5.8690 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3610...  Training loss: 11612.3018...  6.8372 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3611...  Training loss: 11823.4824...  5.7795 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3612...  Training loss: 11917.0371...  5.7761 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3613...  Training loss: 11866.7432...  6.7871 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3614...  Training loss: 12021.1484...  5.8099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3615...  Training loss: 11586.8535...  5.7523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3616...  Training loss: 12226.3721...  6.8655 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3617...  Training loss: 11958.3721...  5.7250 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3618...  Training loss: 12048.6592...  5.7207 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3619...  Training loss: 11877.2402...  6.7468 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3620...  Training loss: 11999.5439...  5.8737 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3621...  Training loss: 12054.6113...  6.0168 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3622...  Training loss: 11854.0400...  7.4438 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3623...  Training loss: 11619.3203...  6.1693 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3624...  Training loss: 12169.0820...  6.1441 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3625...  Training loss: 11972.8613...  6.4055 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3626...  Training loss: 12277.8828...  6.6350 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3627...  Training loss: 12063.9023...  5.7552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3628...  Training loss: 12028.4180...  5.7478 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3629...  Training loss: 11964.9785...  6.8051 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3630...  Training loss: 12076.3398...  5.7912 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3631...  Training loss: 12179.1230...  5.8197 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3632...  Training loss: 11857.7920...  6.4962 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3633...  Training loss: 11971.7246...  6.1360 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3634...  Training loss: 11800.4189...  6.0858 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3635...  Training loss: 12313.1631...  6.4998 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3636...  Training loss: 12056.4219...  6.0698 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3637...  Training loss: 12258.2334...  5.7196 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3638...  Training loss: 11771.3008...  5.8106 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3639...  Training loss: 12069.6865...  6.8278 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3640...  Training loss: 12169.1406...  5.7195 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3641...  Training loss: 12001.2188...  5.8276 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3642...  Training loss: 11835.3496...  6.7729 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3643...  Training loss: 11567.2617...  5.9129 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3644...  Training loss: 11975.0830...  5.7373 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3645...  Training loss: 11668.4180...  6.0059 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3646...  Training loss: 12044.7656...  6.5713 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3647...  Training loss: 11745.6699...  5.8179 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3648...  Training loss: 11968.1406...  5.8002 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3649...  Training loss: 11693.5410...  6.8298 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3650...  Training loss: 11908.4902...  5.8338 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3651...  Training loss: 11846.8984...  5.7937 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3652...  Training loss: 11649.6992...  6.8479 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3653...  Training loss: 11653.5088...  5.8111 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3654...  Training loss: 12002.2695...  6.0252 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3655...  Training loss: 11660.0527...  6.6180 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3656...  Training loss: 11773.6865...  5.8408 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3657...  Training loss: 11587.1689...  5.7448 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3658...  Training loss: 11635.1270...  6.7139 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3659...  Training loss: 11785.5840...  5.7055 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3660...  Training loss: 12035.8848...  5.7941 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3661...  Training loss: 11923.3965...  6.8833 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3662...  Training loss: 11623.3291...  5.8081 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3663...  Training loss: 11835.5078...  5.7697 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3664...  Training loss: 11651.7012...  6.5391 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3665...  Training loss: 12001.0098...  5.9305 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3666...  Training loss: 11906.0879...  5.7905 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3667...  Training loss: 11874.5156...  6.5373 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3668...  Training loss: 11876.5137...  6.0398 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3669...  Training loss: 12064.5342...  5.6539 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3670...  Training loss: 11876.4766...  6.0434 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3671...  Training loss: 11905.8906...  6.7002 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3672...  Training loss: 12006.4053...  5.9213 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3673...  Training loss: 11816.5830...  6.3157 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3674...  Training loss: 12181.4238...  5.6933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3675...  Training loss: 11828.1367...  6.9322 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3676...  Training loss: 12061.0752...  5.8083 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3677...  Training loss: 12053.4736...  5.7915 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3678...  Training loss: 12019.0391...  6.8716 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3679...  Training loss: 11755.1514...  5.7330 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3680...  Training loss: 11636.2285...  5.7599 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3681...  Training loss: 11989.4512...  6.7137 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3682...  Training loss: 12116.9531...  5.9465 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3683...  Training loss: 11917.4883...  5.8059 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3684...  Training loss: 11850.0488...  6.0607 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3685...  Training loss: 11894.5156...  6.5352 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3686...  Training loss: 11699.7344...  5.6947 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3687...  Training loss: 11466.0996...  5.7768 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3688...  Training loss: 11975.7852...  6.8225 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3689...  Training loss: 11818.1543...  5.8789 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3690...  Training loss: 11544.7969...  5.8454 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3691...  Training loss: 11928.9365...  5.9166 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3692...  Training loss: 11941.1299...  6.6499 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3693...  Training loss: 11668.6602...  5.6758 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3694...  Training loss: 11439.8945...  5.7602 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3695...  Training loss: 11434.2422...  6.7495 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3696...  Training loss: 11658.7598...  5.8085 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3697...  Training loss: 12093.4629...  5.8077 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3698...  Training loss: 11910.0781...  6.0056 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3699...  Training loss: 11923.8477...  6.4948 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3700...  Training loss: 11907.5645...  5.8512 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3701...  Training loss: 12158.3555...  5.8054 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3702...  Training loss: 12021.6055...  6.8321 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3703...  Training loss: 12011.7764...  5.7238 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3704...  Training loss: 11980.1562...  5.7194 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3705...  Training loss: 12325.2158...  6.8706 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3706...  Training loss: 12111.0762...  5.8615 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3707...  Training loss: 11784.8867...  5.8858 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3708...  Training loss: 12216.4863...  6.6112 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3709...  Training loss: 11711.3301...  5.9753 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3710...  Training loss: 12070.4121...  5.8222 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3711...  Training loss: 12066.3887...  5.7659 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3712...  Training loss: 12245.3965...  6.6979 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3713...  Training loss: 12225.0332...  5.8221 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3714...  Training loss: 11843.5049...  5.7840 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3715...  Training loss: 11634.4355...  5.7500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3716...  Training loss: 11839.4463...  6.8665 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3717...  Training loss: 12096.9570...  5.8557 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3718...  Training loss: 11869.3594...  5.8063 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3719...  Training loss: 11768.8828...  6.8825 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3720...  Training loss: 11951.5527...  5.8097 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3721...  Training loss: 11892.3193...  5.7360 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3722...  Training loss: 11852.4902...  6.7617 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3723...  Training loss: 11571.7148...  5.8733 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3724...  Training loss: 12100.8203...  5.8192 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3725...  Training loss: 12134.2510...  5.9543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3726...  Training loss: 12149.4932...  6.6694 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3727...  Training loss: 11924.9658...  5.7896 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3728...  Training loss: 11888.5586...  5.7032 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3729...  Training loss: 11960.8887...  6.8248 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3730...  Training loss: 12034.5234...  5.7523 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3731...  Training loss: 12150.5059...  5.8109 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3732...  Training loss: 12500.9746...  6.7988 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3733...  Training loss: 12046.0957...  5.7823 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3734...  Training loss: 11947.3906...  5.8936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3735...  Training loss: 11943.1689...  6.1468 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3736...  Training loss: 11870.3594...  6.3709 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3737...  Training loss: 12298.5137...  5.7861 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3738...  Training loss: 11982.0898...  5.6209 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3739...  Training loss: 11981.1406...  6.7965 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3740...  Training loss: 11770.4551...  5.8142 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3741...  Training loss: 12015.8574...  5.8109 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3742...  Training loss: 12258.3877...  6.8398 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3743...  Training loss: 11732.2910...  5.7639 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3744...  Training loss: 11694.1572...  5.7902 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3745...  Training loss: 11865.6318...  6.8282 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3746...  Training loss: 12006.7578...  5.8298 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3747...  Training loss: 11955.4258...  5.9032 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3748...  Training loss: 11890.2588...  6.4809 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3749...  Training loss: 11832.3105...  6.2166 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3750...  Training loss: 11699.0244...  5.6985 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3751...  Training loss: 12195.8203...  5.8531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3752...  Training loss: 11936.1836...  6.8039 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3753...  Training loss: 11760.2588...  5.8072 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3754...  Training loss: 11901.3857...  5.8119 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3755...  Training loss: 11787.7275...  6.5550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3756...  Training loss: 11669.2656...  5.9073 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3757...  Training loss: 12028.0078...  5.7794 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3758...  Training loss: 11895.4688...  5.7582 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3759...  Training loss: 11555.2266...  6.7543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3760...  Training loss: 11965.8945...  5.8816 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3761...  Training loss: 11868.2998...  5.8024 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3762...  Training loss: 11875.8027...  6.5696 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3763...  Training loss: 13877.6104...  5.9854 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3764...  Training loss: 12234.2617...  5.8748 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3765...  Training loss: 12017.1094...  6.1773 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3766...  Training loss: 12183.7090...  6.6204 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3767...  Training loss: 11874.5078...  5.7732 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3768...  Training loss: 11704.4580...  5.7557 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3769...  Training loss: 11971.4531...  6.8393 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3770...  Training loss: 12076.0488...  5.8233 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3771...  Training loss: 12073.1387...  5.8615 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3772...  Training loss: 11723.6699...  6.7357 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3773...  Training loss: 11852.4570...  5.7300 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3774...  Training loss: 11885.2227...  5.8209 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3775...  Training loss: 11906.5693...  6.8385 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3776...  Training loss: 12044.0898...  5.8468 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3777...  Training loss: 11838.0547...  5.7357 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3778...  Training loss: 11691.8535...  6.2237 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3779...  Training loss: 12065.7051...  6.3947 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3780...  Training loss: 12123.6582...  5.7595 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3781...  Training loss: 11945.5322...  5.9759 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3782...  Training loss: 12253.5117...  6.6270 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3783...  Training loss: 11931.4893...  5.7714 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3784...  Training loss: 12084.2744...  5.7643 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3785...  Training loss: 11923.8320...  6.9792 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3786...  Training loss: 12152.5771...  5.8472 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3787...  Training loss: 11954.0977...  5.6829 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3788...  Training loss: 11546.8545...  7.1072 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3789...  Training loss: 11697.7109...  5.9317 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3790...  Training loss: 12048.0723...  5.7358 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3791...  Training loss: 11977.0605...  5.7766 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3792...  Training loss: 12127.7568...  6.7208 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3793...  Training loss: 11778.6279...  5.7726 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3794...  Training loss: 11664.4492...  5.7558 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3795...  Training loss: 11858.1191...  6.7567 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3796...  Training loss: 12036.8047...  5.9379 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3797...  Training loss: 11843.2168...  5.7626 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3798...  Training loss: 11933.0801...  6.0248 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3799...  Training loss: 11798.4414...  6.6856 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3800...  Training loss: 11505.8008...  5.8734 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3801...  Training loss: 11517.0723...  5.8179 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3802...  Training loss: 11774.9639...  6.6148 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3803...  Training loss: 11586.5078...  5.9997 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3804...  Training loss: 12314.7363...  5.8234 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3805...  Training loss: 11774.6221...  5.8088 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3806...  Training loss: 11640.6328...  6.0597 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3807...  Training loss: 11906.9951...  6.6244 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3808...  Training loss: 11649.8125...  5.7512 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3809...  Training loss: 11700.2070...  5.8556 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3810...  Training loss: 11758.3066...  6.8016 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3811...  Training loss: 11751.7754...  5.7465 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3812...  Training loss: 12049.3496...  5.7197 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3813...  Training loss: 11569.6660...  6.7047 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3814...  Training loss: 12199.7832...  5.7752 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3815...  Training loss: 11920.8223...  5.8033 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3816...  Training loss: 11933.7744...  6.3171 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3817...  Training loss: 11828.6660...  6.1514 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3818...  Training loss: 11846.9941...  5.8500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3819...  Training loss: 11975.1504...  5.7030 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3820...  Training loss: 11737.6436...  6.8428 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3821...  Training loss: 11612.1904...  5.9677 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3822...  Training loss: 12186.3145...  5.7413 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3823...  Training loss: 11932.3125...  6.8670 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3824...  Training loss: 12270.6133...  6.1407 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3825...  Training loss: 12062.7188...  6.0541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3826...  Training loss: 11986.7607...  8.0429 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3827...  Training loss: 11846.8848...  6.9366 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3828...  Training loss: 11950.1855...  6.7305 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3829...  Training loss: 12035.4111...  6.9091 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3830...  Training loss: 11851.4854...  6.9321 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3831...  Training loss: 11916.0957...  6.9494 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3832...  Training loss: 11837.3643...  7.1406 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3833...  Training loss: 12373.8535...  5.7674 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3834...  Training loss: 12028.1074...  5.7339 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3835...  Training loss: 12130.9434...  7.0951 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3836...  Training loss: 11720.0840...  7.0733 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3837...  Training loss: 12024.4551...  5.7251 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3838...  Training loss: 12056.8984...  5.8319 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3839...  Training loss: 11810.9941...  6.7820 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3840...  Training loss: 11759.2793...  5.7085 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3841...  Training loss: 11581.6738...  5.7669 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3842...  Training loss: 11901.5273...  6.7345 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3843...  Training loss: 11623.1035...  5.9142 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3844...  Training loss: 11994.4043...  5.7952 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3845...  Training loss: 11620.4404...  6.6424 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3846...  Training loss: 11890.4092...  5.8479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3847...  Training loss: 11625.2441...  5.7370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3848...  Training loss: 11793.3750...  5.9189 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3849...  Training loss: 11760.1709...  6.6736 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3850...  Training loss: 11683.4082...  5.7258 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3851...  Training loss: 11434.1836...  5.8118 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3852...  Training loss: 11852.7490...  6.9143 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3853...  Training loss: 11659.2256...  5.8403 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3854...  Training loss: 11772.3115...  5.8269 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3855...  Training loss: 11670.5166...  6.7814 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3856...  Training loss: 11510.2783...  6.6113 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3857...  Training loss: 11692.4922...  5.9686 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3858...  Training loss: 11993.3457...  6.4751 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3859...  Training loss: 11822.5605...  6.0016 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3860...  Training loss: 11518.4072...  5.8848 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3861...  Training loss: 11762.4883...  6.8793 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3862...  Training loss: 11629.1074...  7.6981 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3863...  Training loss: 11936.9160...  5.8190 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3864...  Training loss: 11746.3926...  5.7494 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3865...  Training loss: 11892.7461...  6.8179 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3866...  Training loss: 11833.4502...  5.7985 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3867...  Training loss: 11927.1279...  5.7444 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3868...  Training loss: 11786.3613...  6.3627 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3869...  Training loss: 11788.6582...  6.2089 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3870...  Training loss: 12007.0654...  5.8293 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3871...  Training loss: 11775.9541...  5.8270 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3872...  Training loss: 12000.6953...  6.8653 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3873...  Training loss: 11755.8428...  5.8145 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3874...  Training loss: 11968.2998...  5.7923 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3875...  Training loss: 11952.0635...  6.6938 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3876...  Training loss: 11898.8828...  5.7961 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3877...  Training loss: 11720.7461...  5.8053 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3878...  Training loss: 11581.2744...  6.5306 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3879...  Training loss: 11979.5664...  6.1707 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3880...  Training loss: 12033.3867...  5.8103 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3881...  Training loss: 11779.4092...  5.7610 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3882...  Training loss: 11856.9004...  6.8469 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3883...  Training loss: 11821.3066...  5.8465 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3884...  Training loss: 11578.5391...  5.7709 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3885...  Training loss: 11415.3867...  6.9916 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3886...  Training loss: 11938.6660...  5.7468 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3887...  Training loss: 11845.2881...  5.8421 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3888...  Training loss: 11518.1152...  6.7065 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3889...  Training loss: 11883.3672...  5.8108 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3890...  Training loss: 11860.8262...  5.7975 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3891...  Training loss: 11542.1045...  6.1204 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3892...  Training loss: 11491.1191...  6.4430 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3893...  Training loss: 11493.9277...  5.6983 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3894...  Training loss: 11702.3018...  5.7919 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3895...  Training loss: 12073.5957...  6.9313 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3896...  Training loss: 11822.0127...  5.7585 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3897...  Training loss: 11862.1602...  5.8229 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3898...  Training loss: 11846.8105...  6.7246 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3899...  Training loss: 12083.8730...  5.8724 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3900...  Training loss: 12026.1699...  5.7568 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3901...  Training loss: 11975.7549...  6.7529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3902...  Training loss: 11972.4219...  6.0248 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3903...  Training loss: 12318.1973...  5.7177 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3904...  Training loss: 12135.3135...  6.0295 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3905...  Training loss: 11743.2910...  6.5568 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3906...  Training loss: 12225.3398...  5.7852 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3907...  Training loss: 11714.7998...  5.7838 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3908...  Training loss: 12074.5508...  6.8156 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3909...  Training loss: 11958.1846...  5.7851 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3910...  Training loss: 12150.8301...  5.6789 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3911...  Training loss: 12183.3984...  6.7092 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3912...  Training loss: 11818.8867...  5.8766 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3913...  Training loss: 11600.6592...  5.7691 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3914...  Training loss: 11785.7891...  6.7138 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3915...  Training loss: 12001.3945...  5.8855 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3916...  Training loss: 11773.2188...  5.7335 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3917...  Training loss: 11666.5117...  5.8209 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3918...  Training loss: 11778.6680...  6.6592 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3919...  Training loss: 11838.7305...  5.8945 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3920...  Training loss: 11686.3174...  5.7653 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3921...  Training loss: 11503.5859...  5.7359 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3922...  Training loss: 11999.7490...  6.9090 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3923...  Training loss: 12060.3037...  5.7801 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3924...  Training loss: 11969.6084...  5.8530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3925...  Training loss: 11717.2246...  6.8700 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3926...  Training loss: 11780.7275...  5.7277 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3927...  Training loss: 11819.9043...  5.7334 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3928...  Training loss: 11820.8760...  6.7875 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3929...  Training loss: 12041.0352...  5.7968 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3930...  Training loss: 12394.0645...  5.7551 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3931...  Training loss: 12005.1113...  6.8036 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3932...  Training loss: 11937.1836...  5.9305 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3933...  Training loss: 11811.3877...  5.6930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3934...  Training loss: 11899.6250...  5.9934 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3935...  Training loss: 12257.2578...  6.5594 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3936...  Training loss: 11978.9268...  5.8403 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3937...  Training loss: 11973.1152...  5.7410 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3938...  Training loss: 11587.7832...  6.4634 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3939...  Training loss: 11970.7920...  6.0599 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3940...  Training loss: 12294.3105...  5.8012 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3941...  Training loss: 11760.9082...  5.7825 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3942...  Training loss: 11671.1699...  6.8497 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3943...  Training loss: 11707.2295...  5.7748 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3944...  Training loss: 11964.5742...  5.7303 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3945...  Training loss: 11886.2500...  6.7614 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3946...  Training loss: 11751.6973...  5.9704 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3947...  Training loss: 11922.9570...  5.7899 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3948...  Training loss: 11677.5361...  5.8542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3949...  Training loss: 12085.6357...  5.9441 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3950...  Training loss: 11780.0381...  6.5415 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3951...  Training loss: 11739.9980...  5.8367 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3952...  Training loss: 11844.0840...  5.7588 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3953...  Training loss: 11621.2129...  6.7406 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3954...  Training loss: 11716.6348...  6.0370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3955...  Training loss: 11896.5938...  5.7677 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3956...  Training loss: 11806.6260...  5.7784 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3957...  Training loss: 11555.1826...  6.7847 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3958...  Training loss: 11863.9727...  5.7189 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3959...  Training loss: 11781.0400...  5.7911 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3960...  Training loss: 11795.3750...  6.8284 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    saver.restore(sess, 'checkpoints/i3800_l512.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i3960_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3960_l512.ckpt\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i3960_l512.ckpt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i3960_l512.ckpt\n",
      "Farres, and angoner simple as a confidence, the disappointment of\n",
      "the carriage, and started the carriage. He's any of a settles\n",
      "about a serene. Thoughts the stream of men hospending of me, and I am\n",
      "the laborer of the soul of two more and tininess and to them,\" he asked\n",
      "to the look of talk to his husband. \"Yash as I've seen, and I'm not\n",
      "stupid, and I can't chench, that you see how she was to speak of him\n",
      "as you want to go,\" said Stepan Arkadyevitch's familiar.\n",
      "\n",
      "\"Ah!\" at his sake, when she saw he had terrible of that what she\n",
      "had ashemed or had so to be there to drawe it as to have so day the\n",
      "portrait. She did not know that her soft, and a man was too say, as it\n",
      "interest about his brother's fetth and as an existence of confections and\n",
      "state, and he heard her husband, the sight of the painful terms of\n",
      "the position, while were still thought of them. Shishing was true, and\n",
      "that she could not believe how humble were too, and was that his brother\n",
      "had asleep in the conscionsory of this prevented and a conscience to\n",
      "succoust and walk, but there was only of the subsecules, but had no sister\n",
      "of them often himself about her that they would count to all,\n",
      "and would not have been seeing the steps to sought a smile, and stepped\n",
      "at the court of her man with his breath, and always settled herself to\n",
      "the stung and at once being differently, than all when he was as is\n",
      "any acquaintance when the solution of the children through this\n",
      "answer.\n",
      "\n",
      "\"What did you do a complete service.\"\n",
      "\n",
      "\"What's the solition, it's not the day to do it to the sun work\n",
      "about,\" said Stepan Arkadyevitch.\n",
      "\n",
      "\"I hate that,\" she said and would not like all that is a long thing\n",
      "at that man after her fashion, and all the money of the same thing to\n",
      "the principle in the party freedom, and terrible only only always be\n",
      "at her, but with all the peasant of the mother, though he could not\n",
      "believe it. He went into her eyes to be still mateer to the\n",
      "sense, and without threw which had set him by a peasant heart,\n",
      "and the strange he had a\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i200_l512.ckpt\n",
      "Farded theme seelly and she can he with thim, this han the sord the passes. Ther the possing him as im his word or her on threed and\n",
      "there all told than the sorted. I were he had an that he sence at have that what a mare, who had beer he said him of the proation and shild steren the sale a chimes in the\n",
      "parser, that was and his his have some the persacing himsall worked on his arone her his assion to do that shime he she\n",
      "strees them hus to the chather he camen her that he was\n",
      "seant wat the const if of himsels all, and at the\n",
      "cheest of a to sear,\" she said. And thought were this shound the brother and any heant was the surtily the sall and hustered ather and with\n",
      "at her the samoned was of the\n",
      "huss of the\n",
      "carted his took they see had and was still, and the consertatiov, and her with her, and him. I'll and when with his with himsers was the provon all at the was had the had and thought. The dout that he could be the carread traily he husbent on hid sore of the\n",
      "childer, and he was some that th\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i600_l512.ckpt\n",
      "Farviach, and showthing the corressant,\n",
      "and her first had to the sudder to thring. There when the disconserted of the children were the sone in what the pairs were so some went off his heart of his\n",
      "fares, she were beather, and who caming, she were\n",
      "all heast on her she had both\n",
      "served that in she was stood in the sort, and this were a corner at her to be and the\n",
      "dow the position were strained to have happened the day,\n",
      "became her about that with the carration the\n",
      "callent were\n",
      "countrating him with having, and\n",
      "she can't to be an time, and with a shall sore and hured stranged and had that how the dread hand of home\n",
      "has both his house with her. She can\n",
      "tryes through its, and him that he saw sholling her happ hands and the dispecture.\n",
      "\n",
      "\"What hose to take of his face to the string as his sead, and so meant of them on the\n",
      "cracketer and have brought on\n",
      "a meant in all that, and the mind to them. The delight and to a same short in a\n",
      "forther servess, he had a charmaning the done of his face he was allo\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i1200_l512.ckpt\n",
      "Farovs,\n",
      "washing to the minds. The considation of the convincy of the\n",
      "children say the sority. \"I can't see you, and tire one and step the\n",
      "same feeling. At an one was so to back her in a still and anyone to a conself to\n",
      "say her with this way,\" said Vronsky, as though a proposer over the\n",
      "provided and shooting in an interesting she was trained over.\n",
      "\n",
      "\"Anna was the doing?\" she said, \"then has asked a man and to the\n",
      "position, as though he would be, and what's that it's a promot of the\n",
      "sense, to see help in the did not constract, and this were the son and\n",
      "to her hairs, and three hands as though at him, but the peasant were had no divorce.\n",
      "\n",
      "\"You know you see a man told that is it?\"\n",
      "\n",
      "\"Yes and you will be in secret in his hund and always been a meant were\n",
      "string in the son, this went. He saw it he's talking in taking\n",
      "about the delight and as it,\" he said to the precoined and trouser the\n",
      "mement, and he was so then to all them, she sould he had nothing to see her\n",
      "were the priest saw that he had seeme\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i1200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
